import lzma
import numpy as np
import pickle
import torch
import ttnn
from pathlib import Path
aten = torch.ops.aten
@torch.fx.wrap
def conv(
    input_tensor,
    weight_tensor,
    bias_tensor,
    batch_size,
    in_channels,
    out_channels,
    in_spatial_shape,
    kernel_spatial_shape,
    stride,
    padding,
    dilation,
    groups,
    device,
    transposed,
    output_padding=None,
):
    if len(in_spatial_shape) == 1:
        # TODO(tt-metal#16258): conv1d API doesn't support transposed yet
        assert not transposed, "conv1d doesn't support transposed yet"
        return ttnn.Conv1d(
            input_tensor=input_tensor,
            weight_tensor=weight_tensor,
            bias_tensor=bias_tensor,
            batch_size=batch_size,
            in_channels=in_channels,
            out_channels=out_channels,
            input_length=in_spatial_shape[0],
            kernel_size=kernel_spatial_shape[0],
            stride=stride[0],
            padding=padding[0],
            dilation=dilation[0],
            groups=groups,
            device=device,
        )
    if len(in_spatial_shape) == 2:
        in_h, in_w = in_spatial_shape
        if transposed:
            return ttnn.conv_transpose2d(
                input_tensor=input_tensor,
                weight_tensor=weight_tensor,
                bias_tensor=bias_tensor,
                batch_size=batch_size,
                in_channels=in_channels,
                out_channels=out_channels,
                input_height=in_h,
                input_width=in_w,
                kernel_size=kernel_spatial_shape,
                stride=stride,
                padding=padding,
                output_padding=output_padding,
                dilation=dilation,
                groups=groups,
                device=device,
            )
        else:
            assert output_padding is None, "conv2d has no output padding"
            return ttnn.conv2d(
                input_tensor=input_tensor,
                weight_tensor=weight_tensor,
                bias_tensor=bias_tensor,
                batch_size=batch_size,
                in_channels=in_channels,
                out_channels=out_channels,
                input_height=in_h,
                input_width=in_w,
                kernel_size=kernel_spatial_shape,
                stride=stride,
                padding=padding,
                dilation=dilation,
                groups=groups,
                device=device,
            )
    assert False, "unsupported conv shape"

@torch.fx.wrap
def pack_to_tuple(*args):
    return tuple(args)

@torch.fx.wrap
def repeat(t, sizes):
    return ttnn.repeat(t, ttnn.Shape(sizes))

@torch.fx.wrap
def stack(tensors, dim, output_shape):
    # Handle negative dims by wrapping around
    dim = (dim + len(output_shape)) % len(output_shape)

    # Create shape for unsqueezed tensors - same as output but with size 1
    # in the stack dimension
    unsqueezed_shape = output_shape.copy()
    unsqueezed_shape[dim] = 1

    # Reshape each input tensor to add the new dimension
    unsqueezed_tensors = []
    for tensor in tensors:
        unsqueezed_tensors.append(ttnn.reshape(tensor, unsqueezed_shape))

    # Concatenate all reshaped tensors along the stack dimension
    return ttnn.concat(unsqueezed_tensors, dim)

@torch.fx.wrap
def move_to_host(device_tensor, layout):
    host_tensor = ttnn.from_device(device_tensor)
    return ttnn.to_layout(host_tensor, layout)

@torch.fx.wrap
def clone(t):
    return ttnn.clone(t, memory_config=t.memory_config(), dtype=t.dtype)


ref = globals()["stack"]
globals()["stack_wrapper"] = ref
del globals()["stack"]


ref = globals()["clone"]
globals()["clone_wrapper"] = ref
del globals()["clone"]


ref = globals()["conv"]
globals()["conv_wrapper"] = ref
del globals()["conv"]


ref = globals()["pack_to_tuple"]
globals()["pack_to_tuple_wrapper"] = ref
del globals()["pack_to_tuple"]


ref = globals()["move_to_host"]
globals()["move_to_host_wrapper"] = ref
del globals()["move_to_host"]


ref = globals()["repeat"]
globals()["repeat_wrapper"] = ref
del globals()["repeat"]

def comp_pcc(golden, calculated, pcc=0.99):
    golden = torch.Tensor(golden)
    calculated = torch.Tensor(calculated)

    if golden.dtype != calculated.dtype:
        calculated = calculated.type(golden.dtype)

    if torch.all(torch.isnan(golden)) and torch.all(torch.isnan(calculated)):
        # logger.warning("Both tensors are 'nan'")
        return True, 1.0

    if torch.all(torch.isnan(golden)) or torch.all(torch.isnan(calculated)):
        # logger.error("One tensor is all nan, the other is not.")
        return False, 0.0

    # Test if either is completely zero
    if torch.any(golden.bool()) != torch.any(calculated.bool()):
        # logger.error("One tensor is all zero")
        return False, 0.0

    # For now, mask all infs and nans so that we check the rest... TODO
    golden = golden.clone()
    golden[
        torch.logical_or(
            torch.isnan(golden),
            torch.logical_or(torch.isinf(golden), torch.isneginf(golden)),
        )
    ] = 0
    calculated = calculated.clone()
    calculated[
        torch.logical_or(
            torch.isnan(calculated),
            torch.logical_or(torch.isinf(calculated), torch.isneginf(calculated)),
        )
    ] = 0

    if torch.equal(golden, calculated):
        return True, 1.0

    if golden.dtype == torch.bfloat16:
        golden = golden.type(torch.float32)
        calculated = calculated.type(torch.float32)
    cal_pcc = np.min(
        np.ma.corrcoef(
            np.ma.masked_invalid(torch.squeeze(golden).detach().numpy()).flatten(),
            np.ma.masked_invalid(torch.squeeze(calculated).detach().numpy()).flatten(),
        )
    )

    if isinstance(cal_pcc, np.ma.core.MaskedConstant):
        return True, 1.0

    return cal_pcc >= pcc, cal_pcc

def construct_pcc_assert_message(message, expected_pytorch_result, actual_pytorch_result):
    messages = []
    messages.append(message)
    # messages.append("Expected")
    # messages.append(str(expected_pytorch_result))
    # messages.append("Actual")
    # messages.append(str(actual_pytorch_result))
    messages = [str(m) for m in messages]
    return "\n".join(messages)

def assert_with_pcc(expected_pytorch_result, actual_pytorch_result, pcc=0.999):
    assert list(expected_pytorch_result.shape) == list(
        actual_pytorch_result.shape
    ), f"list(expected_pytorch_result.shape)={list(expected_pytorch_result.shape)} vs list(actual_pytorch_result.shape)={list(actual_pytorch_result.shape)}"
    pcc_passed, pcc_message = comp_pcc(expected_pytorch_result, actual_pytorch_result, pcc)
    assert pcc_passed, construct_pcc_assert_message(pcc_message, expected_pytorch_result, actual_pytorch_result)
    return pcc_passed, pcc_message


def test_accuracy(expected, actual):
    if isinstance(actual, ttnn.Tensor):
        actual = ttnn.to_torch(actual)
    assert_with_pcc(expected, actual, pcc = 0.90)

def forward(arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1, arg33_1, arg34_1, arg35_1, arg36_1, arg37_1, arg38_1, arg39_1, arg40_1, arg41_1, arg42_1, arg43_1, arg44_1, arg45_1, arg46_1, arg47_1, arg48_1, arg49_1, arg50_1, arg51_1, arg52_1, arg53_1, arg54_1, arg55_1, arg56_1, arg57_1, arg58_1, arg59_1, arg60_1, arg61_1, arg62_1, arg63_1, arg64_1, arg65_1, arg66_1, arg67_1, arg68_1, arg69_1, arg70_1, arg71_1, arg72_1, arg73_1, arg74_1, arg75_1, arg76_1, arg77_1, arg78_1, arg79_1, arg80_1, arg81_1, arg82_1, arg83_1, arg84_1, arg85_1, arg86_1, arg87_1, arg88_1, arg89_1, arg90_1, arg91_1, arg92_1, arg93_1, arg94_1, arg95_1, arg96_1, arg97_1, arg98_1, arg99_1, arg100_1, arg101_1, arg102_1, arg103_1, arg104_1, arg105_1, arg106_1, arg107_1, arg108_1, arg109_1, arg110_1, arg111_1, arg112_1, arg113_1, arg114_1, arg115_1, arg116_1, arg117_1, arg118_1, arg119_1, arg120_1, arg121_1, arg122_1, arg123_1, arg124_1, arg125_1, arg126_1, arg127_1, arg128_1, arg129_1, arg130_1, arg131_1, arg132_1, arg133_1, arg134_1, arg135_1, arg136_1, arg137_1, arg138_1, arg139_1, arg140_1, arg141_1, arg142_1, arg143_1, arg144_1, arg145_1, arg146_1, arg147_1, arg148_1, arg149_1, arg150_1, arg151_1, arg152_1, arg153_1, arg154_1, arg155_1, arg156_1, arg157_1, arg158_1, arg159_1, arg160_1, arg161_1, arg162_1, arg163_1, arg164_1, arg165_1, arg166_1, arg167_1, arg168_1, arg169_1, arg170_1, arg171_1, arg172_1, arg173_1, arg174_1, arg175_1, arg176_1, arg177_1, arg178_1, arg179_1, arg180_1, arg181_1, arg182_1, arg183_1, arg184_1, arg185_1, arg186_1, arg187_1, arg188_1, arg189_1, arg190_1, arg191_1, arg192_1, arg193_1, arg194_1, arg195_1, arg196_1, arg197_1, arg198_1, arg199_1, arg200_1, arg201_1, arg202_1, arg203_1, arg204_1, arg205_1, arg206_1, arg207_1, arg208_1, arg209_1, arg210_1, arg211_1, arg212_1, arg213_1, arg214_1, arg215_1, arg216_1, arg217_1, arg218_1, arg219_1, arg220_1, arg221_1, arg222_1, arg223_1, arg224_1, arg225_1, arg226_1, arg227_1, arg228_1, arg229_1, arg230_1, arg231_1, arg232_1, arg233_1, arg234_1, arg235_1, arg236_1, arg237_1, arg238_1, arg239_1, arg240_1, arg241_1, arg242_1, arg243_1, arg244_1, arg245_1, arg246_1, arg247_1, arg248_1, arg249_1, arg250_1, arg251_1, arg252_1, arg253_1, arg254_1, arg255_1, arg256_1, arg257_1, arg258_1, arg259_1, arg260_1, arg261_1, arg262_1, arg263_1, arg264_1, arg265_1, arg266_1, arg267_1, arg268_1, arg269_1, arg270_1, arg271_1, arg272_1, arg273_1, arg274_1, arg275_1, arg276_1, arg277_1, arg278_1, arg279_1, arg280_1, arg281_1, arg282_1, arg283_1, arg284_1, arg285_1, arg286_1, arg287_1, arg288_1, arg289_1, arg290_1, arg291_1, arg292_1, arg293_1, arg294_1, arg295_1, arg296_1, arg297_1, arg298_1, arg299_1, arg300_1, arg301_1, arg302_1, arg303_1, arg304_1, arg305_1, arg306_1, arg307_1, arg308_1, arg309_1, arg310_1, arg311_1, arg312_1, arg313_1, arg314_1, arg315_1, arg316_1, arg317_1, arg318_1, arg319_1, arg320_1, arg321_1, arg322_1, arg323_1, arg324_1, arg325_1, arg326_1, arg327_1, arg328_1, arg329_1, arg330_1, arg331_1, arg332_1, arg333_1, arg334_1, arg335_1, arg336_1, arg337_1, arg338_1, arg339_1, arg340_1, arg341_1, arg342_1, arg343_1, arg344_1, arg345_1, arg346_1, arg347_1, arg348_1, arg349_1, arg350_1, arg351_1, arg352_1, arg353_1, arg354_1, arg355_1, arg356_1, arg357_1, arg358_1, arg359_1, arg360_1, arg361_1, arg362_1, arg363_1, arg364_1, arg365_1, arg366_1, arg367_1, arg368_1, arg369_1, arg370_1, arg371_1, arg372_1, arg373_1, arg374_1, arg375_1, arg376_1, arg377_1, arg378_1, arg379_1, arg380_1, arg381_1, arg382_1, arg383_1, arg384_1, arg385_1, arg386_1, arg387_1, arg388_1, arg389_1, arg390_1, arg391_1, arg392_1, arg393_1, arg394_1, arg395_1, arg396_1, arg397_1, arg398_1, arg399_1, arg400_1, arg401_1, arg402_1, arg403_1, arg404_1, arg405_1, arg406_1, arg407_1, arg408_1, arg409_1, arg410_1, arg411_1, arg412_1, arg413_1, arg414_1, arg415_1, arg416_1, arg417_1, arg418_1, arg419_1, arg420_1, arg421_1, arg422_1, arg423_1, arg424_1, arg425_1, arg426_1, arg427_1, arg428_1, arg429_1, arg430_1, arg431_1, arg432_1, arg433_1, arg434_1, arg435_1, arg436_1, arg437_1, arg438_1, arg439_1, arg440_1, arg441_1, arg442_1, arg443_1, arg444_1, arg445_1, arg446_1, arg447_1, arg448_1, arg449_1, arg450_1, arg451_1, arg452_1, arg453_1, arg454_1, arg455_1, arg456_1, arg457_1, arg458_1, arg459_1, arg460_1, arg461_1, arg462_1, arg463_1, arg464_1, arg465_1, arg466_1, arg467_1, arg468_1, arg469_1, arg470_1, arg471_1, arg472_1, arg473_1, arg474_1, arg475_1, arg476_1, arg477_1, arg478_1, arg479_1, arg480_1, arg481_1, arg482_1):
  device = ttnn.open_device(device_id=0, l1_small_size=16384)
  select = aten.select.int(arg482_1, 0, 0, )
  _tensor_constant0 = torch.tensor([0.5000, 0.5000, 0.5000])
  lift_fresh_copy = aten.lift_fresh_copy.default(_tensor_constant0, )
  _tensor_constant1 = torch.tensor([0.5000, 0.5000, 0.5000])
  lift_fresh_copy_1 = aten.lift_fresh_copy.default(_tensor_constant1, )
  slice_1 = aten.slice.Tensor(lift_fresh_copy, 0, 0, 9223372036854775807, )
  unsqueeze = aten.unsqueeze.default(slice_1, 1, )
  unsqueeze_1 = aten.unsqueeze.default(unsqueeze, 2, )
  sub = aten.sub.Tensor(select, unsqueeze_1, )
  slice_2 = aten.slice.Tensor(lift_fresh_copy_1, 0, 0, 9223372036854775807, )
  unsqueeze_2 = aten.unsqueeze.default(slice_2, 1, )
  unsqueeze_3 = aten.unsqueeze.default(unsqueeze_2, 2, )
  div = aten.div.Tensor(sub, unsqueeze_3, )
  unsqueeze_4 = aten.unsqueeze.default(div, 0, )
  arange = aten.arange.default(320, dtype = torch.float32, device = "cpu", pin_memory = False)
  arange_1 = aten.arange.default(320, dtype = torch.float32, device = "cpu", pin_memory = False)
  add = aten.add.Tensor(arange, 0.5, )
  mul = aten.mul.Tensor(add, 1.0, )
  sub_1 = aten.sub.Tensor(mul, 0.5, )
  clamp = aten.clamp.default(sub_1, 0.0, )
  add_1 = aten.add.Tensor(arange_1, 0.5, )
  mul_1 = aten.mul.Tensor(add_1, 1.0, )
  sub_2 = aten.sub.Tensor(mul_1, 0.5, )
  clamp_1 = aten.clamp.default(sub_2, 0.0, )
  _to_copy = aten._to_copy.default(clamp, dtype = torch.int64)
  ceil = aten.ceil.default(clamp, )
  clamp_2 = aten.clamp.default(ceil, None, 319, )
  _to_copy_1 = aten._to_copy.default(clamp_2, dtype = torch.int64)
  _to_copy_2 = aten._to_copy.default(clamp_1, dtype = torch.int64)
  ceil_1 = aten.ceil.default(clamp_1, )
  clamp_3 = aten.clamp.default(ceil_1, None, 319, )
  _to_copy_3 = aten._to_copy.default(clamp_3, dtype = torch.int64)
  unsqueeze_5 = aten.unsqueeze.default(clamp, 1, )
  unsqueeze_6 = aten.unsqueeze.default(_to_copy, 1, )
  unsqueeze_7 = aten.unsqueeze.default(_to_copy_1, 1, )
  _unsafe_index = aten._unsafe_index.Tensor(unsqueeze_4, [None, None, unsqueeze_6, _to_copy_2], )
  _unsafe_index_1 = aten._unsafe_index.Tensor(unsqueeze_4, [None, None, unsqueeze_7, _to_copy_2], )
  _unsafe_index_2 = aten._unsafe_index.Tensor(unsqueeze_4, [None, None, unsqueeze_6, _to_copy_3], )
  _unsafe_index_3 = aten._unsafe_index.Tensor(unsqueeze_4, [None, None, unsqueeze_7, _to_copy_3], )
  sub_3 = aten.sub.Tensor(unsqueeze_5, unsqueeze_6, )
  rsub = aten.rsub.Scalar(sub_3, 1.0, )
  sub_4 = aten.sub.Tensor(clamp_1, _to_copy_2, )
  rsub_1 = aten.rsub.Scalar(sub_4, 1.0, )
  mul_2 = aten.mul.Tensor(_unsafe_index, rsub, )
  mul_3 = aten.mul.Tensor(_unsafe_index_1, sub_3, )
  add_2 = aten.add.Tensor(mul_2, mul_3, )
  mul_4 = aten.mul.Tensor(_unsafe_index_2, rsub, )
  mul_5 = aten.mul.Tensor(_unsafe_index_3, sub_3, )
  add_3 = aten.add.Tensor(mul_4, mul_5, )
  mul_6 = aten.mul.Tensor(add_2, rsub_1, )
  mul_7 = aten.mul.Tensor(add_3, sub_4, )
  add_4 = aten.add.Tensor(mul_6, mul_7, )
  select_1 = aten.select.int(add_4, 0, 0, )
  new_full = aten.new_full.default(select_1, [1, 3, 320, 320], 0, pin_memory = False)
  convolution = aten.convolution.default(new_full, arg0_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training = aten._native_batch_norm_legit_no_training.default(convolution, arg1_1, arg2_1, arg272_1, arg273_1, 0.03, 0.001, )
  getitem = _native_batch_norm_legit_no_training[0]
  hardswish = aten.hardswish.default(getitem, )
  convolution_1 = aten.convolution.default(hardswish, arg3_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 16, )
  _native_batch_norm_legit_no_training_1 = aten._native_batch_norm_legit_no_training.default(convolution_1, arg4_1, arg5_1, arg275_1, arg276_1, 0.03, 0.001, )
  getitem_3 = _native_batch_norm_legit_no_training_1[0]
  relu = aten.relu.default(getitem_3, )
  convolution_2 = aten.convolution.default(relu, arg6_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_2 = aten._native_batch_norm_legit_no_training.default(convolution_2, arg7_1, arg8_1, arg278_1, arg279_1, 0.03, 0.001, )
  getitem_6 = _native_batch_norm_legit_no_training_2[0]
  add_5 = aten.add.Tensor(getitem_6, hardswish, )
  convolution_3 = aten.convolution.default(add_5, arg9_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_3 = aten._native_batch_norm_legit_no_training.default(convolution_3, arg10_1, arg11_1, arg281_1, arg282_1, 0.03, 0.001, )
  getitem_9 = _native_batch_norm_legit_no_training_3[0]
  relu_1 = aten.relu.default(getitem_9, )
  convolution_4 = aten.convolution.default(relu_1, arg12_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 64, )
  _native_batch_norm_legit_no_training_4 = aten._native_batch_norm_legit_no_training.default(convolution_4, arg13_1, arg14_1, arg284_1, arg285_1, 0.03, 0.001, )
  getitem_12 = _native_batch_norm_legit_no_training_4[0]
  relu_2 = aten.relu.default(getitem_12, )
  convolution_5 = aten.convolution.default(relu_2, arg15_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_5 = aten._native_batch_norm_legit_no_training.default(convolution_5, arg16_1, arg17_1, arg287_1, arg288_1, 0.03, 0.001, )
  getitem_15 = _native_batch_norm_legit_no_training_5[0]
  convolution_6 = aten.convolution.default(getitem_15, arg18_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_6 = aten._native_batch_norm_legit_no_training.default(convolution_6, arg19_1, arg20_1, arg290_1, arg291_1, 0.03, 0.001, )
  getitem_18 = _native_batch_norm_legit_no_training_6[0]
  relu_3 = aten.relu.default(getitem_18, )
  convolution_7 = aten.convolution.default(relu_3, arg21_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 72, )
  _native_batch_norm_legit_no_training_7 = aten._native_batch_norm_legit_no_training.default(convolution_7, arg22_1, arg23_1, arg293_1, arg294_1, 0.03, 0.001, )
  getitem_21 = _native_batch_norm_legit_no_training_7[0]
  relu_4 = aten.relu.default(getitem_21, )
  convolution_8 = aten.convolution.default(relu_4, arg24_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_8 = aten._native_batch_norm_legit_no_training.default(convolution_8, arg25_1, arg26_1, arg296_1, arg297_1, 0.03, 0.001, )
  getitem_24 = _native_batch_norm_legit_no_training_8[0]
  add_6 = aten.add.Tensor(getitem_24, getitem_15, )
  convolution_9 = aten.convolution.default(add_6, arg27_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_9 = aten._native_batch_norm_legit_no_training.default(convolution_9, arg28_1, arg29_1, arg299_1, arg300_1, 0.03, 0.001, )
  getitem_27 = _native_batch_norm_legit_no_training_9[0]
  relu_5 = aten.relu.default(getitem_27, )
  convolution_10 = aten.convolution.default(relu_5, arg30_1, None, [2, 2], [2, 2], [1, 1], False, [0, 0], 72, )
  _native_batch_norm_legit_no_training_10 = aten._native_batch_norm_legit_no_training.default(convolution_10, arg31_1, arg32_1, arg302_1, arg303_1, 0.03, 0.001, )
  getitem_30 = _native_batch_norm_legit_no_training_10[0]
  relu_6 = aten.relu.default(getitem_30, )
  mean = aten.mean.dim(relu_6, [-1, -2], True, )
  convolution_11 = aten.convolution.default(mean, arg33_1, arg34_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_7 = aten.relu.default(convolution_11, )
  convolution_12 = aten.convolution.default(relu_7, arg35_1, arg36_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid = aten.hardsigmoid.default(convolution_12, )
  mul_8 = aten.mul.Tensor(hardsigmoid, relu_6, )
  convolution_13 = aten.convolution.default(mul_8, arg37_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_11 = aten._native_batch_norm_legit_no_training.default(convolution_13, arg38_1, arg39_1, arg305_1, arg306_1, 0.03, 0.001, )
  getitem_33 = _native_batch_norm_legit_no_training_11[0]
  convolution_14 = aten.convolution.default(getitem_33, arg40_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_12 = aten._native_batch_norm_legit_no_training.default(convolution_14, arg41_1, arg42_1, arg308_1, arg309_1, 0.03, 0.001, )
  getitem_36 = _native_batch_norm_legit_no_training_12[0]
  relu_8 = aten.relu.default(getitem_36, )
  convolution_15 = aten.convolution.default(relu_8, arg43_1, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120, )
  _native_batch_norm_legit_no_training_13 = aten._native_batch_norm_legit_no_training.default(convolution_15, arg44_1, arg45_1, arg311_1, arg312_1, 0.03, 0.001, )
  getitem_39 = _native_batch_norm_legit_no_training_13[0]
  relu_9 = aten.relu.default(getitem_39, )
  mean_1 = aten.mean.dim(relu_9, [-1, -2], True, )
  convolution_16 = aten.convolution.default(mean_1, arg46_1, arg47_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_10 = aten.relu.default(convolution_16, )
  convolution_17 = aten.convolution.default(relu_10, arg48_1, arg49_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_1 = aten.hardsigmoid.default(convolution_17, )
  mul_9 = aten.mul.Tensor(hardsigmoid_1, relu_9, )
  convolution_18 = aten.convolution.default(mul_9, arg50_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_14 = aten._native_batch_norm_legit_no_training.default(convolution_18, arg51_1, arg52_1, arg314_1, arg315_1, 0.03, 0.001, )
  getitem_42 = _native_batch_norm_legit_no_training_14[0]
  add_7 = aten.add.Tensor(getitem_42, getitem_33, )
  convolution_19 = aten.convolution.default(add_7, arg53_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_15 = aten._native_batch_norm_legit_no_training.default(convolution_19, arg54_1, arg55_1, arg317_1, arg318_1, 0.03, 0.001, )
  getitem_45 = _native_batch_norm_legit_no_training_15[0]
  relu_11 = aten.relu.default(getitem_45, )
  convolution_20 = aten.convolution.default(relu_11, arg56_1, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120, )
  _native_batch_norm_legit_no_training_16 = aten._native_batch_norm_legit_no_training.default(convolution_20, arg57_1, arg58_1, arg320_1, arg321_1, 0.03, 0.001, )
  getitem_48 = _native_batch_norm_legit_no_training_16[0]
  relu_12 = aten.relu.default(getitem_48, )
  mean_2 = aten.mean.dim(relu_12, [-1, -2], True, )
  convolution_21 = aten.convolution.default(mean_2, arg59_1, arg60_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_13 = aten.relu.default(convolution_21, )
  convolution_22 = aten.convolution.default(relu_13, arg61_1, arg62_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_2 = aten.hardsigmoid.default(convolution_22, )
  mul_10 = aten.mul.Tensor(hardsigmoid_2, relu_12, )
  convolution_23 = aten.convolution.default(mul_10, arg63_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_17 = aten._native_batch_norm_legit_no_training.default(convolution_23, arg64_1, arg65_1, arg323_1, arg324_1, 0.03, 0.001, )
  getitem_51 = _native_batch_norm_legit_no_training_17[0]
  add_8 = aten.add.Tensor(getitem_51, add_7, )
  convolution_24 = aten.convolution.default(add_8, arg66_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_18 = aten._native_batch_norm_legit_no_training.default(convolution_24, arg67_1, arg68_1, arg326_1, arg327_1, 0.03, 0.001, )
  getitem_54 = _native_batch_norm_legit_no_training_18[0]
  hardswish_1 = aten.hardswish.default(getitem_54, )
  convolution_25 = aten.convolution.default(hardswish_1, arg69_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240, )
  _native_batch_norm_legit_no_training_19 = aten._native_batch_norm_legit_no_training.default(convolution_25, arg70_1, arg71_1, arg329_1, arg330_1, 0.03, 0.001, )
  getitem_57 = _native_batch_norm_legit_no_training_19[0]
  hardswish_2 = aten.hardswish.default(getitem_57, )
  convolution_26 = aten.convolution.default(hardswish_2, arg72_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_20 = aten._native_batch_norm_legit_no_training.default(convolution_26, arg73_1, arg74_1, arg332_1, arg333_1, 0.03, 0.001, )
  getitem_60 = _native_batch_norm_legit_no_training_20[0]
  convolution_27 = aten.convolution.default(getitem_60, arg75_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_21 = aten._native_batch_norm_legit_no_training.default(convolution_27, arg76_1, arg77_1, arg335_1, arg336_1, 0.03, 0.001, )
  getitem_63 = _native_batch_norm_legit_no_training_21[0]
  hardswish_3 = aten.hardswish.default(getitem_63, )
  convolution_28 = aten.convolution.default(hardswish_3, arg78_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 200, )
  _native_batch_norm_legit_no_training_22 = aten._native_batch_norm_legit_no_training.default(convolution_28, arg79_1, arg80_1, arg338_1, arg339_1, 0.03, 0.001, )
  getitem_66 = _native_batch_norm_legit_no_training_22[0]
  hardswish_4 = aten.hardswish.default(getitem_66, )
  convolution_29 = aten.convolution.default(hardswish_4, arg81_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_23 = aten._native_batch_norm_legit_no_training.default(convolution_29, arg82_1, arg83_1, arg341_1, arg342_1, 0.03, 0.001, )
  getitem_69 = _native_batch_norm_legit_no_training_23[0]
  add_9 = aten.add.Tensor(getitem_69, getitem_60, )
  convolution_30 = aten.convolution.default(add_9, arg84_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_24 = aten._native_batch_norm_legit_no_training.default(convolution_30, arg85_1, arg86_1, arg344_1, arg345_1, 0.03, 0.001, )
  getitem_72 = _native_batch_norm_legit_no_training_24[0]
  hardswish_5 = aten.hardswish.default(getitem_72, )
  convolution_31 = aten.convolution.default(hardswish_5, arg87_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 184, )
  _native_batch_norm_legit_no_training_25 = aten._native_batch_norm_legit_no_training.default(convolution_31, arg88_1, arg89_1, arg347_1, arg348_1, 0.03, 0.001, )
  getitem_75 = _native_batch_norm_legit_no_training_25[0]
  hardswish_6 = aten.hardswish.default(getitem_75, )
  convolution_32 = aten.convolution.default(hardswish_6, arg90_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_26 = aten._native_batch_norm_legit_no_training.default(convolution_32, arg91_1, arg92_1, arg350_1, arg351_1, 0.03, 0.001, )
  getitem_78 = _native_batch_norm_legit_no_training_26[0]
  add_10 = aten.add.Tensor(getitem_78, add_9, )
  convolution_33 = aten.convolution.default(add_10, arg93_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_27 = aten._native_batch_norm_legit_no_training.default(convolution_33, arg94_1, arg95_1, arg353_1, arg354_1, 0.03, 0.001, )
  getitem_81 = _native_batch_norm_legit_no_training_27[0]
  hardswish_7 = aten.hardswish.default(getitem_81, )
  convolution_34 = aten.convolution.default(hardswish_7, arg96_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 184, )
  _native_batch_norm_legit_no_training_28 = aten._native_batch_norm_legit_no_training.default(convolution_34, arg97_1, arg98_1, arg356_1, arg357_1, 0.03, 0.001, )
  getitem_84 = _native_batch_norm_legit_no_training_28[0]
  hardswish_8 = aten.hardswish.default(getitem_84, )
  convolution_35 = aten.convolution.default(hardswish_8, arg99_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_29 = aten._native_batch_norm_legit_no_training.default(convolution_35, arg100_1, arg101_1, arg359_1, arg360_1, 0.03, 0.001, )
  getitem_87 = _native_batch_norm_legit_no_training_29[0]
  add_11 = aten.add.Tensor(getitem_87, add_10, )
  convolution_36 = aten.convolution.default(add_11, arg102_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_30 = aten._native_batch_norm_legit_no_training.default(convolution_36, arg103_1, arg104_1, arg362_1, arg363_1, 0.03, 0.001, )
  getitem_90 = _native_batch_norm_legit_no_training_30[0]
  hardswish_9 = aten.hardswish.default(getitem_90, )
  convolution_37 = aten.convolution.default(hardswish_9, arg105_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480, )
  _native_batch_norm_legit_no_training_31 = aten._native_batch_norm_legit_no_training.default(convolution_37, arg106_1, arg107_1, arg365_1, arg366_1, 0.03, 0.001, )
  getitem_93 = _native_batch_norm_legit_no_training_31[0]
  hardswish_10 = aten.hardswish.default(getitem_93, )
  mean_3 = aten.mean.dim(hardswish_10, [-1, -2], True, )
  convolution_38 = aten.convolution.default(mean_3, arg108_1, arg109_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_14 = aten.relu.default(convolution_38, )
  convolution_39 = aten.convolution.default(relu_14, arg110_1, arg111_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_3 = aten.hardsigmoid.default(convolution_39, )
  mul_11 = aten.mul.Tensor(hardsigmoid_3, hardswish_10, )
  convolution_40 = aten.convolution.default(mul_11, arg112_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_32 = aten._native_batch_norm_legit_no_training.default(convolution_40, arg113_1, arg114_1, arg368_1, arg369_1, 0.03, 0.001, )
  getitem_96 = _native_batch_norm_legit_no_training_32[0]
  convolution_41 = aten.convolution.default(getitem_96, arg115_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_33 = aten._native_batch_norm_legit_no_training.default(convolution_41, arg116_1, arg117_1, arg371_1, arg372_1, 0.03, 0.001, )
  getitem_99 = _native_batch_norm_legit_no_training_33[0]
  hardswish_11 = aten.hardswish.default(getitem_99, )
  convolution_42 = aten.convolution.default(hardswish_11, arg118_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 672, )
  _native_batch_norm_legit_no_training_34 = aten._native_batch_norm_legit_no_training.default(convolution_42, arg119_1, arg120_1, arg374_1, arg375_1, 0.03, 0.001, )
  getitem_102 = _native_batch_norm_legit_no_training_34[0]
  hardswish_12 = aten.hardswish.default(getitem_102, )
  mean_4 = aten.mean.dim(hardswish_12, [-1, -2], True, )
  convolution_43 = aten.convolution.default(mean_4, arg121_1, arg122_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_15 = aten.relu.default(convolution_43, )
  convolution_44 = aten.convolution.default(relu_15, arg123_1, arg124_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_4 = aten.hardsigmoid.default(convolution_44, )
  mul_12 = aten.mul.Tensor(hardsigmoid_4, hardswish_12, )
  convolution_45 = aten.convolution.default(mul_12, arg125_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_35 = aten._native_batch_norm_legit_no_training.default(convolution_45, arg126_1, arg127_1, arg377_1, arg378_1, 0.03, 0.001, )
  getitem_105 = _native_batch_norm_legit_no_training_35[0]
  add_12 = aten.add.Tensor(getitem_105, getitem_96, )
  convolution_46 = aten.convolution.default(add_12, arg128_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_36 = aten._native_batch_norm_legit_no_training.default(convolution_46, arg129_1, arg130_1, arg380_1, arg381_1, 0.03, 0.001, )
  getitem_108 = _native_batch_norm_legit_no_training_36[0]
  hardswish_13 = aten.hardswish.default(getitem_108, )
  convolution_47 = aten.convolution.default(hardswish_13, arg131_1, None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672, )
  _native_batch_norm_legit_no_training_37 = aten._native_batch_norm_legit_no_training.default(convolution_47, arg132_1, arg133_1, arg383_1, arg384_1, 0.03, 0.001, )
  getitem_111 = _native_batch_norm_legit_no_training_37[0]
  hardswish_14 = aten.hardswish.default(getitem_111, )
  mean_5 = aten.mean.dim(hardswish_14, [-1, -2], True, )
  convolution_48 = aten.convolution.default(mean_5, arg134_1, arg135_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_16 = aten.relu.default(convolution_48, )
  convolution_49 = aten.convolution.default(relu_16, arg136_1, arg137_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_5 = aten.hardsigmoid.default(convolution_49, )
  mul_13 = aten.mul.Tensor(hardsigmoid_5, hardswish_14, )
  convolution_50 = aten.convolution.default(mul_13, arg138_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_38 = aten._native_batch_norm_legit_no_training.default(convolution_50, arg139_1, arg140_1, arg386_1, arg387_1, 0.03, 0.001, )
  getitem_114 = _native_batch_norm_legit_no_training_38[0]
  convolution_51 = aten.convolution.default(getitem_114, arg141_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_39 = aten._native_batch_norm_legit_no_training.default(convolution_51, arg142_1, arg143_1, arg389_1, arg390_1, 0.03, 0.001, )
  getitem_117 = _native_batch_norm_legit_no_training_39[0]
  hardswish_15 = aten.hardswish.default(getitem_117, )
  convolution_52 = aten.convolution.default(hardswish_15, arg144_1, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480, )
  _native_batch_norm_legit_no_training_40 = aten._native_batch_norm_legit_no_training.default(convolution_52, arg145_1, arg146_1, arg392_1, arg393_1, 0.03, 0.001, )
  getitem_120 = _native_batch_norm_legit_no_training_40[0]
  hardswish_16 = aten.hardswish.default(getitem_120, )
  mean_6 = aten.mean.dim(hardswish_16, [-1, -2], True, )
  convolution_53 = aten.convolution.default(mean_6, arg147_1, arg148_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_17 = aten.relu.default(convolution_53, )
  convolution_54 = aten.convolution.default(relu_17, arg149_1, arg150_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_6 = aten.hardsigmoid.default(convolution_54, )
  mul_14 = aten.mul.Tensor(hardsigmoid_6, hardswish_16, )
  convolution_55 = aten.convolution.default(mul_14, arg151_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_41 = aten._native_batch_norm_legit_no_training.default(convolution_55, arg152_1, arg153_1, arg395_1, arg396_1, 0.03, 0.001, )
  getitem_123 = _native_batch_norm_legit_no_training_41[0]
  add_13 = aten.add.Tensor(getitem_123, getitem_114, )
  convolution_56 = aten.convolution.default(add_13, arg154_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_42 = aten._native_batch_norm_legit_no_training.default(convolution_56, arg155_1, arg156_1, arg398_1, arg399_1, 0.03, 0.001, )
  getitem_126 = _native_batch_norm_legit_no_training_42[0]
  hardswish_17 = aten.hardswish.default(getitem_126, )
  convolution_57 = aten.convolution.default(hardswish_17, arg157_1, None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480, )
  _native_batch_norm_legit_no_training_43 = aten._native_batch_norm_legit_no_training.default(convolution_57, arg158_1, arg159_1, arg401_1, arg402_1, 0.03, 0.001, )
  getitem_129 = _native_batch_norm_legit_no_training_43[0]
  hardswish_18 = aten.hardswish.default(getitem_129, )
  mean_7 = aten.mean.dim(hardswish_18, [-1, -2], True, )
  convolution_58 = aten.convolution.default(mean_7, arg160_1, arg161_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  relu_18 = aten.relu.default(convolution_58, )
  convolution_59 = aten.convolution.default(relu_18, arg162_1, arg163_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  hardsigmoid_7 = aten.hardsigmoid.default(convolution_59, )
  mul_15 = aten.mul.Tensor(hardsigmoid_7, hardswish_18, )
  convolution_60 = aten.convolution.default(mul_15, arg164_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_44 = aten._native_batch_norm_legit_no_training.default(convolution_60, arg165_1, arg166_1, arg404_1, arg405_1, 0.03, 0.001, )
  getitem_132 = _native_batch_norm_legit_no_training_44[0]
  add_14 = aten.add.Tensor(getitem_132, add_13, )
  convolution_61 = aten.convolution.default(add_14, arg167_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_45 = aten._native_batch_norm_legit_no_training.default(convolution_61, arg168_1, arg169_1, arg407_1, arg408_1, 0.03, 0.001, )
  getitem_135 = _native_batch_norm_legit_no_training_45[0]
  hardswish_19 = aten.hardswish.default(getitem_135, )
  convolution_62 = aten.convolution.default(hardswish_19, arg170_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_46 = aten._native_batch_norm_legit_no_training.default(convolution_62, arg171_1, arg172_1, arg410_1, arg411_1, 0.03, 0.001, )
  getitem_138 = _native_batch_norm_legit_no_training_46[0]
  hardtanh = aten.hardtanh.default(getitem_138, 0.0, 6.0, )
  convolution_63 = aten.convolution.default(hardtanh, arg173_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 256, )
  _native_batch_norm_legit_no_training_47 = aten._native_batch_norm_legit_no_training.default(convolution_63, arg174_1, arg175_1, arg413_1, arg414_1, 0.03, 0.001, )
  getitem_141 = _native_batch_norm_legit_no_training_47[0]
  hardtanh_1 = aten.hardtanh.default(getitem_141, 0.0, 6.0, )
  convolution_64 = aten.convolution.default(hardtanh_1, arg176_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_48 = aten._native_batch_norm_legit_no_training.default(convolution_64, arg177_1, arg178_1, arg416_1, arg417_1, 0.03, 0.001, )
  getitem_144 = _native_batch_norm_legit_no_training_48[0]
  hardtanh_2 = aten.hardtanh.default(getitem_144, 0.0, 6.0, )
  convolution_65 = aten.convolution.default(hardtanh_2, arg179_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_49 = aten._native_batch_norm_legit_no_training.default(convolution_65, arg180_1, arg181_1, arg419_1, arg420_1, 0.03, 0.001, )
  getitem_147 = _native_batch_norm_legit_no_training_49[0]
  hardtanh_3 = aten.hardtanh.default(getitem_147, 0.0, 6.0, )
  convolution_66 = aten.convolution.default(hardtanh_3, arg182_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 128, )
  _native_batch_norm_legit_no_training_50 = aten._native_batch_norm_legit_no_training.default(convolution_66, arg183_1, arg184_1, arg422_1, arg423_1, 0.03, 0.001, )
  getitem_150 = _native_batch_norm_legit_no_training_50[0]
  hardtanh_4 = aten.hardtanh.default(getitem_150, 0.0, 6.0, )
  convolution_67 = aten.convolution.default(hardtanh_4, arg185_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_51 = aten._native_batch_norm_legit_no_training.default(convolution_67, arg186_1, arg187_1, arg425_1, arg426_1, 0.03, 0.001, )
  getitem_153 = _native_batch_norm_legit_no_training_51[0]
  hardtanh_5 = aten.hardtanh.default(getitem_153, 0.0, 6.0, )
  convolution_68 = aten.convolution.default(hardtanh_5, arg188_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_52 = aten._native_batch_norm_legit_no_training.default(convolution_68, arg189_1, arg190_1, arg428_1, arg429_1, 0.03, 0.001, )
  getitem_156 = _native_batch_norm_legit_no_training_52[0]
  hardtanh_6 = aten.hardtanh.default(getitem_156, 0.0, 6.0, )
  convolution_69 = aten.convolution.default(hardtanh_6, arg191_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 128, )
  _native_batch_norm_legit_no_training_53 = aten._native_batch_norm_legit_no_training.default(convolution_69, arg192_1, arg193_1, arg431_1, arg432_1, 0.03, 0.001, )
  getitem_159 = _native_batch_norm_legit_no_training_53[0]
  hardtanh_7 = aten.hardtanh.default(getitem_159, 0.0, 6.0, )
  convolution_70 = aten.convolution.default(hardtanh_7, arg194_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_54 = aten._native_batch_norm_legit_no_training.default(convolution_70, arg195_1, arg196_1, arg434_1, arg435_1, 0.03, 0.001, )
  getitem_162 = _native_batch_norm_legit_no_training_54[0]
  hardtanh_8 = aten.hardtanh.default(getitem_162, 0.0, 6.0, )
  convolution_71 = aten.convolution.default(hardtanh_8, arg197_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_55 = aten._native_batch_norm_legit_no_training.default(convolution_71, arg198_1, arg199_1, arg437_1, arg438_1, 0.03, 0.001, )
  getitem_165 = _native_batch_norm_legit_no_training_55[0]
  hardtanh_9 = aten.hardtanh.default(getitem_165, 0.0, 6.0, )
  convolution_72 = aten.convolution.default(hardtanh_9, arg200_1, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 64, )
  _native_batch_norm_legit_no_training_56 = aten._native_batch_norm_legit_no_training.default(convolution_72, arg201_1, arg202_1, arg440_1, arg441_1, 0.03, 0.001, )
  getitem_168 = _native_batch_norm_legit_no_training_56[0]
  hardtanh_10 = aten.hardtanh.default(getitem_168, 0.0, 6.0, )
  convolution_73 = aten.convolution.default(hardtanh_10, arg203_1, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  _native_batch_norm_legit_no_training_57 = aten._native_batch_norm_legit_no_training.default(convolution_73, arg204_1, arg205_1, arg443_1, arg444_1, 0.03, 0.001, )
  getitem_171 = _native_batch_norm_legit_no_training_57[0]
  hardtanh_11 = aten.hardtanh.default(getitem_171, 0.0, 6.0, )
  convolution_74 = aten.convolution.default(hardswish_13, arg206_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 672, )
  _native_batch_norm_legit_no_training_58 = aten._native_batch_norm_legit_no_training.default(convolution_74, arg207_1, arg208_1, arg446_1, arg447_1, 0.03, 0.001, )
  getitem_174 = _native_batch_norm_legit_no_training_58[0]
  hardtanh_12 = aten.hardtanh.default(getitem_174, 0.0, 6.0, )
  convolution_75 = aten.convolution.default(hardtanh_12, arg209_1, arg210_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view = aten.view.default(convolution_75, [1, -1, 4, 20, 20], )
  permute = aten.permute.default(view, [0, 3, 4, 1, 2], )
  clone_1 = aten.clone.default(permute, memory_format = torch.contiguous_format)
  _unsafe_view = aten._unsafe_view.default(clone_1, [1, 2400, 4], )
  convolution_76 = aten.convolution.default(hardswish_19, arg211_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480, )
  _native_batch_norm_legit_no_training_59 = aten._native_batch_norm_legit_no_training.default(convolution_76, arg212_1, arg213_1, arg449_1, arg450_1, 0.03, 0.001, )
  getitem_177 = _native_batch_norm_legit_no_training_59[0]
  hardtanh_13 = aten.hardtanh.default(getitem_177, 0.0, 6.0, )
  convolution_77 = aten.convolution.default(hardtanh_13, arg214_1, arg215_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_1 = aten.view.default(convolution_77, [1, -1, 4, 10, 10], )
  permute_1 = aten.permute.default(view_1, [0, 3, 4, 1, 2], )
  clone_2 = aten.clone.default(permute_1, memory_format = torch.contiguous_format)
  _unsafe_view_1 = aten._unsafe_view.default(clone_2, [1, 600, 4], )
  convolution_78 = aten.convolution.default(hardtanh_2, arg216_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 512, )
  _native_batch_norm_legit_no_training_60 = aten._native_batch_norm_legit_no_training.default(convolution_78, arg217_1, arg218_1, arg452_1, arg453_1, 0.03, 0.001, )
  getitem_180 = _native_batch_norm_legit_no_training_60[0]
  hardtanh_14 = aten.hardtanh.default(getitem_180, 0.0, 6.0, )
  convolution_79 = aten.convolution.default(hardtanh_14, arg219_1, arg220_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_2 = aten.view.default(convolution_79, [1, -1, 4, 5, 5], )
  permute_2 = aten.permute.default(view_2, [0, 3, 4, 1, 2], )
  clone_3 = aten.clone.default(permute_2, memory_format = torch.contiguous_format)
  _unsafe_view_2 = aten._unsafe_view.default(clone_3, [1, 150, 4], )
  convolution_80 = aten.convolution.default(hardtanh_5, arg221_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 256, )
  _native_batch_norm_legit_no_training_61 = aten._native_batch_norm_legit_no_training.default(convolution_80, arg222_1, arg223_1, arg455_1, arg456_1, 0.03, 0.001, )
  getitem_183 = _native_batch_norm_legit_no_training_61[0]
  hardtanh_15 = aten.hardtanh.default(getitem_183, 0.0, 6.0, )
  convolution_81 = aten.convolution.default(hardtanh_15, arg224_1, arg225_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_3 = aten.view.default(convolution_81, [1, -1, 4, 3, 3], )
  permute_3 = aten.permute.default(view_3, [0, 3, 4, 1, 2], )
  clone_4 = aten.clone.default(permute_3, memory_format = torch.contiguous_format)
  _unsafe_view_3 = aten._unsafe_view.default(clone_4, [1, 54, 4], )
  convolution_82 = aten.convolution.default(hardtanh_8, arg226_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 256, )
  _native_batch_norm_legit_no_training_62 = aten._native_batch_norm_legit_no_training.default(convolution_82, arg227_1, arg228_1, arg458_1, arg459_1, 0.03, 0.001, )
  getitem_186 = _native_batch_norm_legit_no_training_62[0]
  hardtanh_16 = aten.hardtanh.default(getitem_186, 0.0, 6.0, )
  convolution_83 = aten.convolution.default(hardtanh_16, arg229_1, arg230_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_4 = aten.view.default(convolution_83, [1, -1, 4, 2, 2], )
  permute_4 = aten.permute.default(view_4, [0, 3, 4, 1, 2], )
  clone_5 = aten.clone.default(permute_4, memory_format = torch.contiguous_format)
  _unsafe_view_4 = aten._unsafe_view.default(clone_5, [1, 24, 4], )
  convolution_84 = aten.convolution.default(hardtanh_11, arg231_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 128, )
  _native_batch_norm_legit_no_training_63 = aten._native_batch_norm_legit_no_training.default(convolution_84, arg232_1, arg233_1, arg461_1, arg462_1, 0.03, 0.001, )
  getitem_189 = _native_batch_norm_legit_no_training_63[0]
  hardtanh_17 = aten.hardtanh.default(getitem_189, 0.0, 6.0, )
  convolution_85 = aten.convolution.default(hardtanh_17, arg234_1, arg235_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_5 = aten.view.default(convolution_85, [1, -1, 4, 1, 1], )
  permute_5 = aten.permute.default(view_5, [0, 3, 4, 1, 2], )
  view_6 = aten.view.default(permute_5, [1, -1, 4], )
  cat = aten.cat.default([_unsafe_view, _unsafe_view_1, _unsafe_view_2, _unsafe_view_3, _unsafe_view_4, view_6], 1, )
  convolution_86 = aten.convolution.default(hardswish_13, arg236_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 672, )
  _native_batch_norm_legit_no_training_64 = aten._native_batch_norm_legit_no_training.default(convolution_86, arg237_1, arg238_1, arg464_1, arg465_1, 0.03, 0.001, )
  getitem_192 = _native_batch_norm_legit_no_training_64[0]
  hardtanh_18 = aten.hardtanh.default(getitem_192, 0.0, 6.0, )
  convolution_87 = aten.convolution.default(hardtanh_18, arg239_1, arg240_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_7 = aten.view.default(convolution_87, [1, -1, 91, 20, 20], )
  permute_6 = aten.permute.default(view_7, [0, 3, 4, 1, 2], )
  clone_6 = aten.clone.default(permute_6, memory_format = torch.contiguous_format)
  _unsafe_view_5 = aten._unsafe_view.default(clone_6, [1, 2400, 91], )
  convolution_88 = aten.convolution.default(hardswish_19, arg241_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480, )
  _native_batch_norm_legit_no_training_65 = aten._native_batch_norm_legit_no_training.default(convolution_88, arg242_1, arg243_1, arg467_1, arg468_1, 0.03, 0.001, )
  getitem_195 = _native_batch_norm_legit_no_training_65[0]
  hardtanh_19 = aten.hardtanh.default(getitem_195, 0.0, 6.0, )
  convolution_89 = aten.convolution.default(hardtanh_19, arg244_1, arg245_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_8 = aten.view.default(convolution_89, [1, -1, 91, 10, 10], )
  permute_7 = aten.permute.default(view_8, [0, 3, 4, 1, 2], )
  clone_7 = aten.clone.default(permute_7, memory_format = torch.contiguous_format)
  _unsafe_view_6 = aten._unsafe_view.default(clone_7, [1, 600, 91], )
  convolution_90 = aten.convolution.default(hardtanh_2, arg246_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 512, )
  _native_batch_norm_legit_no_training_66 = aten._native_batch_norm_legit_no_training.default(convolution_90, arg247_1, arg248_1, arg470_1, arg471_1, 0.03, 0.001, )
  getitem_198 = _native_batch_norm_legit_no_training_66[0]
  hardtanh_20 = aten.hardtanh.default(getitem_198, 0.0, 6.0, )
  convolution_91 = aten.convolution.default(hardtanh_20, arg249_1, arg250_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_9 = aten.view.default(convolution_91, [1, -1, 91, 5, 5], )
  permute_8 = aten.permute.default(view_9, [0, 3, 4, 1, 2], )
  clone_8 = aten.clone.default(permute_8, memory_format = torch.contiguous_format)
  _unsafe_view_7 = aten._unsafe_view.default(clone_8, [1, 150, 91], )
  convolution_92 = aten.convolution.default(hardtanh_5, arg251_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 256, )
  _native_batch_norm_legit_no_training_67 = aten._native_batch_norm_legit_no_training.default(convolution_92, arg252_1, arg253_1, arg473_1, arg474_1, 0.03, 0.001, )
  getitem_201 = _native_batch_norm_legit_no_training_67[0]
  hardtanh_21 = aten.hardtanh.default(getitem_201, 0.0, 6.0, )
  convolution_93 = aten.convolution.default(hardtanh_21, arg254_1, arg255_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_10 = aten.view.default(convolution_93, [1, -1, 91, 3, 3], )
  permute_9 = aten.permute.default(view_10, [0, 3, 4, 1, 2], )
  clone_9 = aten.clone.default(permute_9, memory_format = torch.contiguous_format)
  _unsafe_view_8 = aten._unsafe_view.default(clone_9, [1, 54, 91], )
  convolution_94 = aten.convolution.default(hardtanh_8, arg256_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 256, )
  _native_batch_norm_legit_no_training_68 = aten._native_batch_norm_legit_no_training.default(convolution_94, arg257_1, arg258_1, arg476_1, arg477_1, 0.03, 0.001, )
  getitem_204 = _native_batch_norm_legit_no_training_68[0]
  hardtanh_22 = aten.hardtanh.default(getitem_204, 0.0, 6.0, )
  convolution_95 = aten.convolution.default(hardtanh_22, arg259_1, arg260_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_11 = aten.view.default(convolution_95, [1, -1, 91, 2, 2], )
  permute_10 = aten.permute.default(view_11, [0, 3, 4, 1, 2], )
  clone_10 = aten.clone.default(permute_10, memory_format = torch.contiguous_format)
  _unsafe_view_9 = aten._unsafe_view.default(clone_10, [1, 24, 91], )
  convolution_96 = aten.convolution.default(hardtanh_11, arg261_1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 128, )
  _native_batch_norm_legit_no_training_69 = aten._native_batch_norm_legit_no_training.default(convolution_96, arg262_1, arg263_1, arg479_1, arg480_1, 0.03, 0.001, )
  getitem_207 = _native_batch_norm_legit_no_training_69[0]
  hardtanh_23 = aten.hardtanh.default(getitem_207, 0.0, 6.0, )
  convolution_97 = aten.convolution.default(hardtanh_23, arg264_1, arg265_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, )
  view_12 = aten.view.default(convolution_97, [1, -1, 91, 1, 1], )
  permute_11 = aten.permute.default(view_12, [0, 3, 4, 1, 2], )
  view_13 = aten.view.default(permute_11, [1, -1, 91], )
  cat_1 = aten.cat.default([_unsafe_view_5, _unsafe_view_6, _unsafe_view_7, _unsafe_view_8, _unsafe_view_9, view_13], 1, )
  arange_2 = aten.arange.start(0, 20, device = "cpu", pin_memory = False)
  add_15 = aten.add.Tensor(arange_2, 0.5, )
  div_1 = aten.div.Tensor(add_15, 20, )
  arange_3 = aten.arange.start(0, 20, device = "cpu", pin_memory = False)
  add_16 = aten.add.Tensor(arange_3, 0.5, )
  div_2 = aten.div.Tensor(add_16, 20, )
  view_14 = aten.view.default(div_2, [-1, 1], )
  expand = aten.expand.default(view_14, [20, 20], )
  view_15 = aten.view.default(div_1, [1, -1], )
  expand_1 = aten.expand.default(view_15, [20, 20], )
  clone_11 = aten.clone.default(expand_1, memory_format = torch.contiguous_format)
  _unsafe_view_10 = aten._unsafe_view.default(clone_11, [400], )
  clone_12 = aten.clone.default(expand, memory_format = torch.contiguous_format)
  _unsafe_view_11 = aten._unsafe_view.default(clone_12, [400], )
  stack = aten.stack.default([_unsafe_view_10, _unsafe_view_11, _unsafe_view_10, _unsafe_view_11, _unsafe_view_10, _unsafe_view_11, _unsafe_view_10, _unsafe_view_11, _unsafe_view_10, _unsafe_view_11, _unsafe_view_10, _unsafe_view_11], -1, )
  view_16 = aten.view.default(stack, [-1, 2], )
  clamp_4 = aten.clamp.default(arg266_1, 0, 1, )
  repeat = aten.repeat.default(clamp_4, [400, 1], )
  cat_2 = aten.cat.default([view_16, repeat], 1, )
  arange_4 = aten.arange.start(0, 10, device = "cpu", pin_memory = False)
  add_17 = aten.add.Tensor(arange_4, 0.5, )
  div_3 = aten.div.Tensor(add_17, 10, )
  arange_5 = aten.arange.start(0, 10, device = "cpu", pin_memory = False)
  add_18 = aten.add.Tensor(arange_5, 0.5, )
  div_4 = aten.div.Tensor(add_18, 10, )
  view_17 = aten.view.default(div_4, [-1, 1], )
  expand_2 = aten.expand.default(view_17, [10, 10], )
  view_18 = aten.view.default(div_3, [1, -1], )
  expand_3 = aten.expand.default(view_18, [10, 10], )
  clone_13 = aten.clone.default(expand_3, memory_format = torch.contiguous_format)
  _unsafe_view_12 = aten._unsafe_view.default(clone_13, [100], )
  clone_14 = aten.clone.default(expand_2, memory_format = torch.contiguous_format)
  _unsafe_view_13 = aten._unsafe_view.default(clone_14, [100], )
  stack_1 = aten.stack.default([_unsafe_view_12, _unsafe_view_13, _unsafe_view_12, _unsafe_view_13, _unsafe_view_12, _unsafe_view_13, _unsafe_view_12, _unsafe_view_13, _unsafe_view_12, _unsafe_view_13, _unsafe_view_12, _unsafe_view_13], -1, )
  view_19 = aten.view.default(stack_1, [-1, 2], )
  clamp_5 = aten.clamp.default(arg267_1, 0, 1, )
  repeat_1 = aten.repeat.default(clamp_5, [100, 1], )
  cat_3 = aten.cat.default([view_19, repeat_1], 1, )
  arange_6 = aten.arange.start(0, 5, device = "cpu", pin_memory = False)
  add_19 = aten.add.Tensor(arange_6, 0.5, )
  div_5 = aten.div.Tensor(add_19, 5, )
  arange_7 = aten.arange.start(0, 5, device = "cpu", pin_memory = False)
  add_20 = aten.add.Tensor(arange_7, 0.5, )
  div_6 = aten.div.Tensor(add_20, 5, )
  view_20 = aten.view.default(div_6, [-1, 1], )
  expand_4 = aten.expand.default(view_20, [5, 5], )
  view_21 = aten.view.default(div_5, [1, -1], )
  expand_5 = aten.expand.default(view_21, [5, 5], )
  clone_15 = aten.clone.default(expand_5, memory_format = torch.contiguous_format)
  _unsafe_view_14 = aten._unsafe_view.default(clone_15, [25], )
  clone_16 = aten.clone.default(expand_4, memory_format = torch.contiguous_format)
  _unsafe_view_15 = aten._unsafe_view.default(clone_16, [25], )
  stack_2 = aten.stack.default([_unsafe_view_14, _unsafe_view_15, _unsafe_view_14, _unsafe_view_15, _unsafe_view_14, _unsafe_view_15, _unsafe_view_14, _unsafe_view_15, _unsafe_view_14, _unsafe_view_15, _unsafe_view_14, _unsafe_view_15], -1, )
  view_22 = aten.view.default(stack_2, [-1, 2], )
  clamp_6 = aten.clamp.default(arg268_1, 0, 1, )
  repeat_2 = aten.repeat.default(clamp_6, [25, 1], )
  cat_4 = aten.cat.default([view_22, repeat_2], 1, )
  arange_8 = aten.arange.start(0, 3, device = "cpu", pin_memory = False)
  add_21 = aten.add.Tensor(arange_8, 0.5, )
  div_7 = aten.div.Tensor(add_21, 3, )
  arange_9 = aten.arange.start(0, 3, device = "cpu", pin_memory = False)
  add_22 = aten.add.Tensor(arange_9, 0.5, )
  div_8 = aten.div.Tensor(add_22, 3, )
  view_23 = aten.view.default(div_8, [-1, 1], )
  expand_6 = aten.expand.default(view_23, [3, 3], )
  view_24 = aten.view.default(div_7, [1, -1], )
  expand_7 = aten.expand.default(view_24, [3, 3], )
  clone_17 = aten.clone.default(expand_7, memory_format = torch.contiguous_format)
  _unsafe_view_16 = aten._unsafe_view.default(clone_17, [9], )
  clone_18 = aten.clone.default(expand_6, memory_format = torch.contiguous_format)
  _unsafe_view_17 = aten._unsafe_view.default(clone_18, [9], )
  stack_3 = aten.stack.default([_unsafe_view_16, _unsafe_view_17, _unsafe_view_16, _unsafe_view_17, _unsafe_view_16, _unsafe_view_17, _unsafe_view_16, _unsafe_view_17, _unsafe_view_16, _unsafe_view_17, _unsafe_view_16, _unsafe_view_17], -1, )
  view_25 = aten.view.default(stack_3, [-1, 2], )
  clamp_7 = aten.clamp.default(arg269_1, 0, 1, )
  repeat_3 = aten.repeat.default(clamp_7, [9, 1], )
  cat_5 = aten.cat.default([view_25, repeat_3], 1, )
  arange_10 = aten.arange.start(0, 2, device = "cpu", pin_memory = False)
  add_23 = aten.add.Tensor(arange_10, 0.5, )
  div_9 = aten.div.Tensor(add_23, 2, )
  arange_11 = aten.arange.start(0, 2, device = "cpu", pin_memory = False)
  add_24 = aten.add.Tensor(arange_11, 0.5, )
  div_10 = aten.div.Tensor(add_24, 2, )
  view_26 = aten.view.default(div_10, [-1, 1], )
  expand_8 = aten.expand.default(view_26, [2, 2], )
  view_27 = aten.view.default(div_9, [1, -1], )
  expand_9 = aten.expand.default(view_27, [2, 2], )
  clone_19 = aten.clone.default(expand_9, memory_format = torch.contiguous_format)
  _unsafe_view_18 = aten._unsafe_view.default(clone_19, [4], )
  clone_20 = aten.clone.default(expand_8, memory_format = torch.contiguous_format)
  _unsafe_view_19 = aten._unsafe_view.default(clone_20, [4], )
  stack_4 = aten.stack.default([_unsafe_view_18, _unsafe_view_19, _unsafe_view_18, _unsafe_view_19, _unsafe_view_18, _unsafe_view_19, _unsafe_view_18, _unsafe_view_19, _unsafe_view_18, _unsafe_view_19, _unsafe_view_18, _unsafe_view_19], -1, )
  view_28 = aten.view.default(stack_4, [-1, 2], )
  clamp_8 = aten.clamp.default(arg270_1, 0, 1, )
  repeat_4 = aten.repeat.default(clamp_8, [4, 1], )
  cat_6 = aten.cat.default([view_28, repeat_4], 1, )
  arange_12 = aten.arange.start(0, 1, device = "cpu", pin_memory = False)
  add_25 = aten.add.Tensor(arange_12, 0.5, )
  div_11 = aten.div.Tensor(add_25, 1, )
  arange_13 = aten.arange.start(0, 1, device = "cpu", pin_memory = False)
  add_26 = aten.add.Tensor(arange_13, 0.5, )
  div_12 = aten.div.Tensor(add_26, 1, )
  view_29 = aten.view.default(div_12, [-1, 1], )
  expand_10 = aten.expand.default(view_29, [1, 1], )
  view_30 = aten.view.default(div_11, [1, -1], )
  expand_11 = aten.expand.default(view_30, [1, 1], )
  view_31 = aten.view.default(expand_11, [-1], )
  view_32 = aten.view.default(expand_10, [-1], )
  stack_5 = aten.stack.default([view_31, view_32, view_31, view_32, view_31, view_32, view_31, view_32, view_31, view_32, view_31, view_32], -1, )
  view_33 = aten.view.default(stack_5, [-1, 2], )
  clamp_9 = aten.clamp.default(arg271_1, 0, 1, )
  repeat_5 = aten.repeat.default(clamp_9, [1, 1], )
  cat_7 = aten.cat.default([view_33, repeat_5], 1, )
  cat_8 = aten.cat.default([cat_2, cat_3, cat_4, cat_5, cat_6, cat_7], )
  _tensor_constant2 = torch.tensor([320, 320])
  lift_fresh_copy_2 = aten.lift_fresh_copy.default(_tensor_constant2, )
  slice_3 = aten.slice.Tensor(cat_8, 0, 0, 9223372036854775807, )
  slice_4 = aten.slice.Tensor(slice_3, 1, 0, 2, )
  slice_5 = aten.slice.Tensor(cat_8, 0, 0, 9223372036854775807, )
  slice_6 = aten.slice.Tensor(slice_5, 1, 2, 9223372036854775807, )
  mul_16 = aten.mul.Tensor(slice_6, 0.5, )
  sub_5 = aten.sub.Tensor(slice_4, mul_16, )
  mul_17 = aten.mul.Tensor(sub_5, lift_fresh_copy_2, )
  slice_7 = aten.slice.Tensor(cat_8, 0, 0, 9223372036854775807, )
  slice_8 = aten.slice.Tensor(slice_7, 1, 0, 2, )
  slice_9 = aten.slice.Tensor(cat_8, 0, 0, 9223372036854775807, )
  slice_10 = aten.slice.Tensor(slice_9, 1, 2, 9223372036854775807, )
  mul_18 = aten.mul.Tensor(slice_10, 0.5, )
  add_27 = aten.add.Tensor(slice_8, mul_18, )
  mul_19 = aten.mul.Tensor(add_27, lift_fresh_copy_2, )
  cat_9 = aten.cat.default([mul_17, mul_19], -1, )
  # return (cat, cat_1, cat_9, new_full)
  ttnn_full = ttnn.full((1, 3, 320, 320), fill_value = 0, device = device, layout = ttnn.TILE_LAYOUT)
  test_accuracy(new_full, ttnn_full)
  ttnn_permute_12 = ttnn.permute(ttnn_full, (0, 2, 3, 1), )
  ttnn_from_device = ttnn.from_device(ttnn_permute_12, )
  ttnn_to_layout_32 = ttnn.to_layout(ttnn_from_device, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape = ttnn.reshape(ttnn_to_layout_32, (1, 1, 102400, 3), )
  ttnn_from_device_1 = ttnn.from_device(ttnn_reshape, )
  ttnn_to_layout_33 = ttnn.to_layout(ttnn_from_device_1, ttnn.TILE_LAYOUT, )
  ttnn_to_device = ttnn.to_device(ttnn_to_layout_33, device = device)
  ttnn_from_torch = ttnn.from_torch(arg0_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv = conv_wrapper(ttnn_to_device, ttnn_from_torch, None, 1, 3, 16, [320, 320], [3, 3], [2, 2], [1, 1], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved = ttnn.sharded_to_interleaved(ttnn_prefix_conv, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_2 = ttnn.from_device(ttnn_sharded_to_interleaved, )
  ttnn_to_layout_34 = ttnn.to_layout(ttnn_from_device_2, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_1 = ttnn.reshape(ttnn_to_layout_34, (1, 160, 160, 16), )
  ttnn_from_device_3 = ttnn.from_device(ttnn_reshape_1, )
  ttnn_to_layout_35 = ttnn.to_layout(ttnn_from_device_3, ttnn.TILE_LAYOUT, )
  ttnn_to_device_1 = ttnn.to_device(ttnn_to_layout_35, device = device)
  ttnn_permute_13 = ttnn.permute(ttnn_to_device_1, (0, 3, 1, 2), )
  ttnn_from_torch_1 = ttnn.from_torch(arg273_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add = ttnn.add(ttnn_from_torch_1, 0.001, )
  ttnn_rsqrt = ttnn.rsqrt(ttnn_add, )
  ttnn_from_device_4 = ttnn.from_device(ttnn_rsqrt, )
  ttnn_to_layout_36 = ttnn.to_layout(ttnn_from_device_4, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_2 = ttnn.reshape(ttnn_to_layout_36, (1, 16, 1, 1), )
  ttnn_from_torch_2 = ttnn.from_torch(arg272_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_3 = ttnn.reshape(ttnn_from_torch_2, (1, 16, 1, 1), )
  ttnn_from_device_5 = ttnn.from_device(ttnn_reshape_3, )
  ttnn_to_layout_37 = ttnn.to_layout(ttnn_from_device_5, ttnn.TILE_LAYOUT, )
  ttnn_to_device_2 = ttnn.to_device(ttnn_to_layout_37, device = device)
  ttnn_subtract = ttnn.subtract(ttnn_permute_13, ttnn_to_device_2, )
  ttnn_from_device_6 = ttnn.from_device(ttnn_reshape_2, )
  ttnn_to_layout_38 = ttnn.to_layout(ttnn_from_device_6, ttnn.TILE_LAYOUT, )
  ttnn_to_device_3 = ttnn.to_device(ttnn_to_layout_38, device = device)
  ttnn_multiply = ttnn.multiply(ttnn_subtract, ttnn_to_device_3, )
  ttnn_from_torch_3 = ttnn.from_torch(arg1_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_4 = ttnn.reshape(ttnn_from_torch_3, (1, 16, 1, 1), )
  ttnn_from_device_7 = ttnn.from_device(ttnn_reshape_4, )
  ttnn_to_layout_39 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_4 = ttnn.to_device(ttnn_to_layout_39, device = device)
  ttnn_multiply_1 = ttnn.multiply(ttnn_multiply, ttnn_to_device_4, )
  ttnn_from_torch_4 = ttnn.from_torch(arg2_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_5 = ttnn.reshape(ttnn_from_torch_4, (1, 16, 1, 1), )
  ttnn_from_device_8 = ttnn.from_device(ttnn_reshape_5, )
  ttnn_to_layout_40 = ttnn.to_layout(ttnn_from_device_8, ttnn.TILE_LAYOUT, )
  ttnn_to_device_5 = ttnn.to_device(ttnn_to_layout_40, device = device)
  ttnn_add_1 = ttnn.add(ttnn_multiply_1, ttnn_to_device_5, )
  ttnn_prefix_pack_to_tuple = pack_to_tuple_wrapper(ttnn_add_1, )
  ttnn_prefix_getitem = ttnn_prefix_pack_to_tuple[0]
  ttnn_hardswish = ttnn.hardswish(ttnn_prefix_getitem, )
  test_accuracy(hardswish, ttnn_hardswish)
  ttnn_permute_14 = ttnn.permute(ttnn_hardswish, (0, 2, 3, 1), )
  ttnn_from_device_9 = ttnn.from_device(ttnn_permute_14, )
  ttnn_to_layout_41 = ttnn.to_layout(ttnn_from_device_9, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_6 = ttnn.reshape(ttnn_to_layout_41, (1, 1, 25600, 16), )
  ttnn_from_device_10 = ttnn.from_device(ttnn_reshape_6, )
  ttnn_to_layout_42 = ttnn.to_layout(ttnn_from_device_10, ttnn.TILE_LAYOUT, )
  ttnn_to_device_6 = ttnn.to_device(ttnn_to_layout_42, device = device)
  ttnn_from_torch_5 = ttnn.from_torch(arg3_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_1 = conv_wrapper(ttnn_to_device_6, ttnn_from_torch_5, None, 1, 16, 16, [160, 160], [3, 3], [1, 1], [1, 1], [1, 1], 16, device, False, None, )
  ttnn_sharded_to_interleaved_1 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_1, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_11 = ttnn.from_device(ttnn_sharded_to_interleaved_1, )
  ttnn_to_layout_43 = ttnn.to_layout(ttnn_from_device_11, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_7 = ttnn.reshape(ttnn_to_layout_43, (1, 160, 160, 16), )
  ttnn_from_device_12 = ttnn.from_device(ttnn_reshape_7, )
  ttnn_to_layout_44 = ttnn.to_layout(ttnn_from_device_12, ttnn.TILE_LAYOUT, )
  ttnn_to_device_7 = ttnn.to_device(ttnn_to_layout_44, device = device)
  ttnn_permute_15 = ttnn.permute(ttnn_to_device_7, (0, 3, 1, 2), )
  ttnn_from_torch_6 = ttnn.from_torch(arg276_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_2 = ttnn.add(ttnn_from_torch_6, 0.001, )
  ttnn_rsqrt_1 = ttnn.rsqrt(ttnn_add_2, )
  ttnn_from_device_13 = ttnn.from_device(ttnn_rsqrt_1, )
  ttnn_to_layout_45 = ttnn.to_layout(ttnn_from_device_13, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_8 = ttnn.reshape(ttnn_to_layout_45, (1, 16, 1, 1), )
  ttnn_from_torch_7 = ttnn.from_torch(arg275_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_9 = ttnn.reshape(ttnn_from_torch_7, (1, 16, 1, 1), )
  ttnn_from_device_14 = ttnn.from_device(ttnn_reshape_9, )
  ttnn_to_layout_46 = ttnn.to_layout(ttnn_from_device_14, ttnn.TILE_LAYOUT, )
  ttnn_to_device_8 = ttnn.to_device(ttnn_to_layout_46, device = device)
  ttnn_subtract_1 = ttnn.subtract(ttnn_permute_15, ttnn_to_device_8, )
  ttnn_from_device_15 = ttnn.from_device(ttnn_reshape_8, )
  ttnn_to_layout_47 = ttnn.to_layout(ttnn_from_device_15, ttnn.TILE_LAYOUT, )
  ttnn_to_device_9 = ttnn.to_device(ttnn_to_layout_47, device = device)
  ttnn_multiply_2 = ttnn.multiply(ttnn_subtract_1, ttnn_to_device_9, )
  ttnn_from_torch_8 = ttnn.from_torch(arg4_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_10 = ttnn.reshape(ttnn_from_torch_8, (1, 16, 1, 1), )
  ttnn_from_device_16 = ttnn.from_device(ttnn_reshape_10, )
  ttnn_to_layout_48 = ttnn.to_layout(ttnn_from_device_16, ttnn.TILE_LAYOUT, )
  ttnn_to_device_10 = ttnn.to_device(ttnn_to_layout_48, device = device)
  ttnn_multiply_3 = ttnn.multiply(ttnn_multiply_2, ttnn_to_device_10, )
  ttnn_from_torch_9 = ttnn.from_torch(arg5_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_11 = ttnn.reshape(ttnn_from_torch_9, (1, 16, 1, 1), )
  ttnn_from_device_17 = ttnn.from_device(ttnn_reshape_11, )
  ttnn_to_layout_49 = ttnn.to_layout(ttnn_from_device_17, ttnn.TILE_LAYOUT, )
  ttnn_to_device_11 = ttnn.to_device(ttnn_to_layout_49, device = device)
  ttnn_add_3 = ttnn.add(ttnn_multiply_3, ttnn_to_device_11, )
  ttnn_prefix_pack_to_tuple_1 = pack_to_tuple_wrapper(ttnn_add_3, )
  ttnn_prefix_getitem_1 = ttnn_prefix_pack_to_tuple_1[0]
  ttnn_relu = ttnn.relu(ttnn_prefix_getitem_1, )
  test_accuracy(relu, ttnn_relu)
  ttnn_permute_16 = ttnn.permute(ttnn_relu, (0, 2, 3, 1), )
  ttnn_from_device_18 = ttnn.from_device(ttnn_permute_16, )
  ttnn_to_layout_50 = ttnn.to_layout(ttnn_from_device_18, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_12 = ttnn.reshape(ttnn_to_layout_50, (1, 1, 25600, 16), )
  ttnn_from_device_19 = ttnn.from_device(ttnn_reshape_12, )
  ttnn_to_layout_51 = ttnn.to_layout(ttnn_from_device_19, ttnn.TILE_LAYOUT, )
  ttnn_to_device_12 = ttnn.to_device(ttnn_to_layout_51, device = device)
  ttnn_from_torch_10 = ttnn.from_torch(arg6_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_2 = conv_wrapper(ttnn_to_device_12, ttnn_from_torch_10, None, 1, 16, 16, [160, 160], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_2 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_2, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_20 = ttnn.from_device(ttnn_sharded_to_interleaved_2, )
  ttnn_to_layout_52 = ttnn.to_layout(ttnn_from_device_20, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_13 = ttnn.reshape(ttnn_to_layout_52, (1, 160, 160, 16), )
  ttnn_from_device_21 = ttnn.from_device(ttnn_reshape_13, )
  ttnn_to_layout_53 = ttnn.to_layout(ttnn_from_device_21, ttnn.TILE_LAYOUT, )
  ttnn_to_device_13 = ttnn.to_device(ttnn_to_layout_53, device = device)
  ttnn_permute_17 = ttnn.permute(ttnn_to_device_13, (0, 3, 1, 2), )
  test_accuracy(convolution_2, ttnn_permute_17)
  ttnn_from_torch_11 = ttnn.from_torch(arg279_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_4 = ttnn.add(ttnn_from_torch_11, 0.001, )
  ttnn_rsqrt_2 = ttnn.rsqrt(ttnn_add_4, )
  ttnn_from_device_22 = ttnn.from_device(ttnn_rsqrt_2, )
  ttnn_to_layout_54 = ttnn.to_layout(ttnn_from_device_22, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_14 = ttnn.reshape(ttnn_to_layout_54, (1, 16, 1, 1), )
  ttnn_from_torch_12 = ttnn.from_torch(arg278_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_15 = ttnn.reshape(ttnn_from_torch_12, (1, 16, 1, 1), )
  ttnn_from_device_23 = ttnn.from_device(ttnn_reshape_15, )
  ttnn_to_layout_55 = ttnn.to_layout(ttnn_from_device_23, ttnn.TILE_LAYOUT, )
  ttnn_to_device_14 = ttnn.to_device(ttnn_to_layout_55, device = device)
  ttnn_subtract_2 = ttnn.subtract(ttnn_permute_17, ttnn_to_device_14, )
  ttnn_from_device_24 = ttnn.from_device(ttnn_reshape_14, )
  ttnn_to_layout_56 = ttnn.to_layout(ttnn_from_device_24, ttnn.TILE_LAYOUT, )
  ttnn_to_device_15 = ttnn.to_device(ttnn_to_layout_56, device = device)
  ttnn_multiply_4 = ttnn.multiply(ttnn_subtract_2, ttnn_to_device_15, )
  ttnn_from_torch_13 = ttnn.from_torch(arg7_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_16 = ttnn.reshape(ttnn_from_torch_13, (1, 16, 1, 1), )
  ttnn_from_device_25 = ttnn.from_device(ttnn_reshape_16, )
  ttnn_to_layout_57 = ttnn.to_layout(ttnn_from_device_25, ttnn.TILE_LAYOUT, )
  ttnn_to_device_16 = ttnn.to_device(ttnn_to_layout_57, device = device)
  ttnn_multiply_5 = ttnn.multiply(ttnn_multiply_4, ttnn_to_device_16, )
  ttnn_from_torch_14 = ttnn.from_torch(arg8_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_17 = ttnn.reshape(ttnn_from_torch_14, (1, 16, 1, 1), )
  ttnn_from_device_26 = ttnn.from_device(ttnn_reshape_17, )
  ttnn_to_layout_58 = ttnn.to_layout(ttnn_from_device_26, ttnn.TILE_LAYOUT, )
  ttnn_to_device_17 = ttnn.to_device(ttnn_to_layout_58, device = device)
  ttnn_add_5 = ttnn.add(ttnn_multiply_5, ttnn_to_device_17, )
  ttnn_prefix_pack_to_tuple_2 = pack_to_tuple_wrapper(ttnn_add_5, )
  ttnn_prefix_getitem_2 = ttnn_prefix_pack_to_tuple_2[0]
  test_accuracy(getitem_6, ttnn_prefix_getitem_2)
  ttnn_add_6 = ttnn.add(ttnn_prefix_getitem_2, ttnn_hardswish, )
  test_accuracy(add_5, ttnn_add_6)
  ttnn_permute_18 = ttnn.permute(ttnn_add_6, (0, 2, 3, 1), )
  ttnn_from_device_27 = ttnn.from_device(ttnn_permute_18, )
  ttnn_to_layout_59 = ttnn.to_layout(ttnn_from_device_27, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_18 = ttnn.reshape(ttnn_to_layout_59, (1, 1, 25600, 16), )
  ttnn_from_device_28 = ttnn.from_device(ttnn_reshape_18, )
  ttnn_to_layout_60 = ttnn.to_layout(ttnn_from_device_28, ttnn.TILE_LAYOUT, )
  ttnn_to_device_18 = ttnn.to_device(ttnn_to_layout_60, device = device)
  ttnn_from_torch_15 = ttnn.from_torch(arg9_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_3 = conv_wrapper(ttnn_to_device_18, ttnn_from_torch_15, None, 1, 16, 64, [160, 160], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_3 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_3, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_29 = ttnn.from_device(ttnn_sharded_to_interleaved_3, )
  ttnn_to_layout_61 = ttnn.to_layout(ttnn_from_device_29, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_19 = ttnn.reshape(ttnn_to_layout_61, (1, 160, 160, 64), )
  ttnn_from_device_30 = ttnn.from_device(ttnn_reshape_19, )
  ttnn_to_layout_62 = ttnn.to_layout(ttnn_from_device_30, ttnn.TILE_LAYOUT, )
  ttnn_to_device_19 = ttnn.to_device(ttnn_to_layout_62, device = device)
  ttnn_permute_19 = ttnn.permute(ttnn_to_device_19, (0, 3, 1, 2), )
  test_accuracy(convolution_3, ttnn_permute_19)
  ttnn_from_torch_16 = ttnn.from_torch(arg282_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_7 = ttnn.add(ttnn_from_torch_16, 0.001, )
  ttnn_rsqrt_3 = ttnn.rsqrt(ttnn_add_7, )
  ttnn_from_device_31 = ttnn.from_device(ttnn_rsqrt_3, )
  ttnn_to_layout_63 = ttnn.to_layout(ttnn_from_device_31, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_20 = ttnn.reshape(ttnn_to_layout_63, (1, 64, 1, 1), )
  ttnn_from_torch_17 = ttnn.from_torch(arg281_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_21 = ttnn.reshape(ttnn_from_torch_17, (1, 64, 1, 1), )
  ttnn_from_device_32 = ttnn.from_device(ttnn_reshape_21, )
  ttnn_to_layout_64 = ttnn.to_layout(ttnn_from_device_32, ttnn.TILE_LAYOUT, )
  ttnn_to_device_20 = ttnn.to_device(ttnn_to_layout_64, device = device)
  ttnn_subtract_3 = ttnn.subtract(ttnn_permute_19, ttnn_to_device_20, )
  ttnn_from_device_33 = ttnn.from_device(ttnn_reshape_20, )
  ttnn_to_layout_65 = ttnn.to_layout(ttnn_from_device_33, ttnn.TILE_LAYOUT, )
  ttnn_to_device_21 = ttnn.to_device(ttnn_to_layout_65, device = device)
  ttnn_multiply_6 = ttnn.multiply(ttnn_subtract_3, ttnn_to_device_21, )
  ttnn_from_torch_18 = ttnn.from_torch(arg10_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_22 = ttnn.reshape(ttnn_from_torch_18, (1, 64, 1, 1), )
  ttnn_from_device_34 = ttnn.from_device(ttnn_reshape_22, )
  ttnn_to_layout_66 = ttnn.to_layout(ttnn_from_device_34, ttnn.TILE_LAYOUT, )
  ttnn_to_device_22 = ttnn.to_device(ttnn_to_layout_66, device = device)
  ttnn_multiply_7 = ttnn.multiply(ttnn_multiply_6, ttnn_to_device_22, )
  ttnn_from_torch_19 = ttnn.from_torch(arg11_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_23 = ttnn.reshape(ttnn_from_torch_19, (1, 64, 1, 1), )
  ttnn_from_device_35 = ttnn.from_device(ttnn_reshape_23, )
  ttnn_to_layout_67 = ttnn.to_layout(ttnn_from_device_35, ttnn.TILE_LAYOUT, )
  ttnn_to_device_23 = ttnn.to_device(ttnn_to_layout_67, device = device)
  ttnn_add_8 = ttnn.add(ttnn_multiply_7, ttnn_to_device_23, )
  ttnn_prefix_pack_to_tuple_3 = pack_to_tuple_wrapper(ttnn_add_8, )
  ttnn_prefix_getitem_3 = ttnn_prefix_pack_to_tuple_3[0]
  test_accuracy(getitem_9, ttnn_prefix_getitem_3)
  ttnn_relu_1 = ttnn.relu(ttnn_prefix_getitem_3, )
  test_accuracy(relu_1, ttnn_relu_1)
  ttnn_permute_20 = ttnn.permute(ttnn_relu_1, (0, 2, 3, 1), )
  ttnn_from_device_36 = ttnn.from_device(ttnn_permute_20, )
  ttnn_to_layout_68 = ttnn.to_layout(ttnn_from_device_36, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_24 = ttnn.reshape(ttnn_to_layout_68, (1, 1, 25600, 64), )
  ttnn_from_device_37 = ttnn.from_device(ttnn_reshape_24, )
  ttnn_to_layout_69 = ttnn.to_layout(ttnn_from_device_37, ttnn.TILE_LAYOUT, )
  ttnn_to_device_24 = ttnn.to_device(ttnn_to_layout_69, device = device)
  ttnn_from_torch_20 = ttnn.from_torch(arg12_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_4 = conv_wrapper(ttnn_to_device_24, ttnn_from_torch_20, None, 1, 64, 64, [160, 160], [3, 3], [2, 2], [1, 1], [1, 1], 64, device, False, None, )
  ttnn_sharded_to_interleaved_4 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_4, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_38 = ttnn.from_device(ttnn_sharded_to_interleaved_4, )
  ttnn_to_layout_70 = ttnn.to_layout(ttnn_from_device_38, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_25 = ttnn.reshape(ttnn_to_layout_70, (1, 80, 80, 64), )
  ttnn_from_device_39 = ttnn.from_device(ttnn_reshape_25, )
  ttnn_to_layout_71 = ttnn.to_layout(ttnn_from_device_39, ttnn.TILE_LAYOUT, )
  ttnn_to_device_25 = ttnn.to_device(ttnn_to_layout_71, device = device)
  ttnn_permute_21 = ttnn.permute(ttnn_to_device_25, (0, 3, 1, 2), )
  test_accuracy(convolution_4, ttnn_permute_21)
  ttnn_from_torch_21 = ttnn.from_torch(arg285_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_9 = ttnn.add(ttnn_from_torch_21, 0.001, )
  ttnn_rsqrt_4 = ttnn.rsqrt(ttnn_add_9, )
  ttnn_from_device_40 = ttnn.from_device(ttnn_rsqrt_4, )
  ttnn_to_layout_72 = ttnn.to_layout(ttnn_from_device_40, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_26 = ttnn.reshape(ttnn_to_layout_72, (1, 64, 1, 1), )
  ttnn_from_torch_22 = ttnn.from_torch(arg284_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_27 = ttnn.reshape(ttnn_from_torch_22, (1, 64, 1, 1), )
  ttnn_from_device_41 = ttnn.from_device(ttnn_reshape_27, )
  ttnn_to_layout_73 = ttnn.to_layout(ttnn_from_device_41, ttnn.TILE_LAYOUT, )
  ttnn_to_device_26 = ttnn.to_device(ttnn_to_layout_73, device = device)
  ttnn_subtract_4 = ttnn.subtract(ttnn_permute_21, ttnn_to_device_26, )
  ttnn_from_device_42 = ttnn.from_device(ttnn_reshape_26, )
  ttnn_to_layout_74 = ttnn.to_layout(ttnn_from_device_42, ttnn.TILE_LAYOUT, )
  ttnn_to_device_27 = ttnn.to_device(ttnn_to_layout_74, device = device)
  ttnn_multiply_8 = ttnn.multiply(ttnn_subtract_4, ttnn_to_device_27, )
  ttnn_from_torch_23 = ttnn.from_torch(arg13_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_28 = ttnn.reshape(ttnn_from_torch_23, (1, 64, 1, 1), )
  ttnn_from_device_43 = ttnn.from_device(ttnn_reshape_28, )
  ttnn_to_layout_75 = ttnn.to_layout(ttnn_from_device_43, ttnn.TILE_LAYOUT, )
  ttnn_to_device_28 = ttnn.to_device(ttnn_to_layout_75, device = device)
  ttnn_multiply_9 = ttnn.multiply(ttnn_multiply_8, ttnn_to_device_28, )
  ttnn_from_torch_24 = ttnn.from_torch(arg14_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_29 = ttnn.reshape(ttnn_from_torch_24, (1, 64, 1, 1), )
  ttnn_from_device_44 = ttnn.from_device(ttnn_reshape_29, )
  ttnn_to_layout_76 = ttnn.to_layout(ttnn_from_device_44, ttnn.TILE_LAYOUT, )
  ttnn_to_device_29 = ttnn.to_device(ttnn_to_layout_76, device = device)
  ttnn_add_10 = ttnn.add(ttnn_multiply_9, ttnn_to_device_29, )
  ttnn_prefix_pack_to_tuple_4 = pack_to_tuple_wrapper(ttnn_add_10, )
  ttnn_prefix_getitem_4 = ttnn_prefix_pack_to_tuple_4[0]
  test_accuracy(getitem_12, ttnn_prefix_getitem_4)
  ttnn_relu_2 = ttnn.relu(ttnn_prefix_getitem_4, )
  test_accuracy(relu_2, ttnn_relu_2)
  ttnn_permute_22 = ttnn.permute(ttnn_relu_2, (0, 2, 3, 1), )
  ttnn_from_device_45 = ttnn.from_device(ttnn_permute_22, )
  ttnn_to_layout_77 = ttnn.to_layout(ttnn_from_device_45, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_30 = ttnn.reshape(ttnn_to_layout_77, (1, 1, 6400, 64), )
  ttnn_from_device_46 = ttnn.from_device(ttnn_reshape_30, )
  ttnn_to_layout_78 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_30 = ttnn.to_device(ttnn_to_layout_78, device = device)
  ttnn_from_torch_25 = ttnn.from_torch(arg15_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_5 = conv_wrapper(ttnn_to_device_30, ttnn_from_torch_25, None, 1, 64, 24, [80, 80], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_5 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_5, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_47 = ttnn.from_device(ttnn_sharded_to_interleaved_5, )
  ttnn_to_layout_79 = ttnn.to_layout(ttnn_from_device_47, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_31 = ttnn.reshape(ttnn_to_layout_79, (1, 80, 80, 24), )
  ttnn_from_device_48 = ttnn.from_device(ttnn_reshape_31, )
  ttnn_to_layout_80 = ttnn.to_layout(ttnn_from_device_48, ttnn.TILE_LAYOUT, )
  ttnn_to_device_31 = ttnn.to_device(ttnn_to_layout_80, device = device)
  ttnn_permute_23 = ttnn.permute(ttnn_to_device_31, (0, 3, 1, 2), )
  ttnn_from_torch_26 = ttnn.from_torch(arg288_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_11 = ttnn.add(ttnn_from_torch_26, 0.001, )
  ttnn_rsqrt_5 = ttnn.rsqrt(ttnn_add_11, )
  ttnn_from_device_49 = ttnn.from_device(ttnn_rsqrt_5, )
  ttnn_to_layout_81 = ttnn.to_layout(ttnn_from_device_49, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_32 = ttnn.reshape(ttnn_to_layout_81, (1, 24, 1, 1), )
  ttnn_from_torch_27 = ttnn.from_torch(arg287_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_33 = ttnn.reshape(ttnn_from_torch_27, (1, 24, 1, 1), )
  ttnn_from_device_50 = ttnn.from_device(ttnn_reshape_33, )
  ttnn_to_layout_82 = ttnn.to_layout(ttnn_from_device_50, ttnn.TILE_LAYOUT, )
  ttnn_to_device_32 = ttnn.to_device(ttnn_to_layout_82, device = device)
  ttnn_subtract_5 = ttnn.subtract(ttnn_permute_23, ttnn_to_device_32, )
  ttnn_from_device_51 = ttnn.from_device(ttnn_reshape_32, )
  ttnn_to_layout_83 = ttnn.to_layout(ttnn_from_device_51, ttnn.TILE_LAYOUT, )
  ttnn_to_device_33 = ttnn.to_device(ttnn_to_layout_83, device = device)
  ttnn_multiply_10 = ttnn.multiply(ttnn_subtract_5, ttnn_to_device_33, )
  ttnn_from_torch_28 = ttnn.from_torch(arg16_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_34 = ttnn.reshape(ttnn_from_torch_28, (1, 24, 1, 1), )
  ttnn_from_device_52 = ttnn.from_device(ttnn_reshape_34, )
  ttnn_to_layout_84 = ttnn.to_layout(ttnn_from_device_52, ttnn.TILE_LAYOUT, )
  ttnn_to_device_34 = ttnn.to_device(ttnn_to_layout_84, device = device)
  ttnn_multiply_11 = ttnn.multiply(ttnn_multiply_10, ttnn_to_device_34, )
  ttnn_from_torch_29 = ttnn.from_torch(arg17_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_35 = ttnn.reshape(ttnn_from_torch_29, (1, 24, 1, 1), )
  ttnn_from_device_53 = ttnn.from_device(ttnn_reshape_35, )
  ttnn_to_layout_85 = ttnn.to_layout(ttnn_from_device_53, ttnn.TILE_LAYOUT, )
  ttnn_to_device_35 = ttnn.to_device(ttnn_to_layout_85, device = device)
  ttnn_add_12 = ttnn.add(ttnn_multiply_11, ttnn_to_device_35, )
  ttnn_prefix_pack_to_tuple_5 = pack_to_tuple_wrapper(ttnn_add_12, )
  ttnn_prefix_getitem_5 = ttnn_prefix_pack_to_tuple_5[0]
  ttnn_permute_24 = ttnn.permute(ttnn_prefix_getitem_5, (0, 2, 3, 1), )
  ttnn_from_device_54 = ttnn.from_device(ttnn_permute_24, )
  ttnn_to_layout_86 = ttnn.to_layout(ttnn_from_device_54, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_36 = ttnn.reshape(ttnn_to_layout_86, (1, 1, 6400, 24), )
  ttnn_from_device_55 = ttnn.from_device(ttnn_reshape_36, )
  ttnn_to_layout_87 = ttnn.to_layout(ttnn_from_device_55, ttnn.TILE_LAYOUT, )
  ttnn_to_device_36 = ttnn.to_device(ttnn_to_layout_87, device = device)
  ttnn_from_torch_30 = ttnn.from_torch(arg18_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_6 = conv_wrapper(ttnn_to_device_36, ttnn_from_torch_30, None, 1, 24, 72, [80, 80], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_6 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_6, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_56 = ttnn.from_device(ttnn_sharded_to_interleaved_6, )
  ttnn_to_layout_88 = ttnn.to_layout(ttnn_from_device_56, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_37 = ttnn.reshape(ttnn_to_layout_88, (1, 80, 80, 72), )
  ttnn_from_device_57 = ttnn.from_device(ttnn_reshape_37, )
  ttnn_to_layout_89 = ttnn.to_layout(ttnn_from_device_57, ttnn.TILE_LAYOUT, )
  ttnn_to_device_37 = ttnn.to_device(ttnn_to_layout_89, device = device)
  ttnn_permute_25 = ttnn.permute(ttnn_to_device_37, (0, 3, 1, 2), )
  ttnn_from_torch_31 = ttnn.from_torch(arg291_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_13 = ttnn.add(ttnn_from_torch_31, 0.001, )
  ttnn_rsqrt_6 = ttnn.rsqrt(ttnn_add_13, )
  ttnn_from_device_58 = ttnn.from_device(ttnn_rsqrt_6, )
  ttnn_to_layout_90 = ttnn.to_layout(ttnn_from_device_58, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_38 = ttnn.reshape(ttnn_to_layout_90, (1, 72, 1, 1), )
  ttnn_from_torch_32 = ttnn.from_torch(arg290_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_39 = ttnn.reshape(ttnn_from_torch_32, (1, 72, 1, 1), )
  ttnn_from_device_59 = ttnn.from_device(ttnn_reshape_39, )
  ttnn_to_layout_91 = ttnn.to_layout(ttnn_from_device_59, ttnn.TILE_LAYOUT, )
  ttnn_to_device_38 = ttnn.to_device(ttnn_to_layout_91, device = device)
  ttnn_subtract_6 = ttnn.subtract(ttnn_permute_25, ttnn_to_device_38, )
  ttnn_from_device_60 = ttnn.from_device(ttnn_reshape_38, )
  ttnn_to_layout_92 = ttnn.to_layout(ttnn_from_device_60, ttnn.TILE_LAYOUT, )
  ttnn_to_device_39 = ttnn.to_device(ttnn_to_layout_92, device = device)
  ttnn_multiply_12 = ttnn.multiply(ttnn_subtract_6, ttnn_to_device_39, )
  ttnn_from_torch_33 = ttnn.from_torch(arg19_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_40 = ttnn.reshape(ttnn_from_torch_33, (1, 72, 1, 1), )
  ttnn_from_device_61 = ttnn.from_device(ttnn_reshape_40, )
  ttnn_to_layout_93 = ttnn.to_layout(ttnn_from_device_61, ttnn.TILE_LAYOUT, )
  ttnn_to_device_40 = ttnn.to_device(ttnn_to_layout_93, device = device)
  ttnn_multiply_13 = ttnn.multiply(ttnn_multiply_12, ttnn_to_device_40, )
  ttnn_from_torch_34 = ttnn.from_torch(arg20_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_41 = ttnn.reshape(ttnn_from_torch_34, (1, 72, 1, 1), )
  ttnn_from_device_62 = ttnn.from_device(ttnn_reshape_41, )
  ttnn_to_layout_94 = ttnn.to_layout(ttnn_from_device_62, ttnn.TILE_LAYOUT, )
  ttnn_to_device_41 = ttnn.to_device(ttnn_to_layout_94, device = device)
  ttnn_add_14 = ttnn.add(ttnn_multiply_13, ttnn_to_device_41, )
  ttnn_prefix_pack_to_tuple_6 = pack_to_tuple_wrapper(ttnn_add_14, )
  ttnn_prefix_getitem_6 = ttnn_prefix_pack_to_tuple_6[0]
  ttnn_relu_3 = ttnn.relu(ttnn_prefix_getitem_6, )
  ttnn_permute_26 = ttnn.permute(ttnn_relu_3, (0, 2, 3, 1), )
  ttnn_from_device_63 = ttnn.from_device(ttnn_permute_26, )
  ttnn_to_layout_95 = ttnn.to_layout(ttnn_from_device_63, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_42 = ttnn.reshape(ttnn_to_layout_95, (1, 1, 6400, 72), )
  ttnn_from_device_64 = ttnn.from_device(ttnn_reshape_42, )
  ttnn_to_layout_96 = ttnn.to_layout(ttnn_from_device_64, ttnn.TILE_LAYOUT, )
  ttnn_to_device_42 = ttnn.to_device(ttnn_to_layout_96, device = device)
  ttnn_from_torch_35 = ttnn.from_torch(arg21_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_7 = conv_wrapper(ttnn_to_device_42, ttnn_from_torch_35, None, 1, 72, 72, [80, 80], [3, 3], [1, 1], [1, 1], [1, 1], 72, device, False, None, )
  ttnn_sharded_to_interleaved_7 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_7, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_65 = ttnn.from_device(ttnn_sharded_to_interleaved_7, )
  ttnn_to_layout_97 = ttnn.to_layout(ttnn_from_device_65, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_43 = ttnn.reshape(ttnn_to_layout_97, (1, 80, 80, 72), )
  ttnn_from_device_66 = ttnn.from_device(ttnn_reshape_43, )
  ttnn_to_layout_98 = ttnn.to_layout(ttnn_from_device_66, ttnn.TILE_LAYOUT, )
  ttnn_to_device_43 = ttnn.to_device(ttnn_to_layout_98, device = device)
  ttnn_permute_27 = ttnn.permute(ttnn_to_device_43, (0, 3, 1, 2), )
  ttnn_from_torch_36 = ttnn.from_torch(arg294_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_15 = ttnn.add(ttnn_from_torch_36, 0.001, )
  ttnn_rsqrt_7 = ttnn.rsqrt(ttnn_add_15, )
  ttnn_from_device_67 = ttnn.from_device(ttnn_rsqrt_7, )
  ttnn_to_layout_99 = ttnn.to_layout(ttnn_from_device_67, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_44 = ttnn.reshape(ttnn_to_layout_99, (1, 72, 1, 1), )
  ttnn_from_torch_37 = ttnn.from_torch(arg293_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_45 = ttnn.reshape(ttnn_from_torch_37, (1, 72, 1, 1), )
  ttnn_from_device_68 = ttnn.from_device(ttnn_reshape_45, )
  ttnn_to_layout_100 = ttnn.to_layout(ttnn_from_device_68, ttnn.TILE_LAYOUT, )
  ttnn_to_device_44 = ttnn.to_device(ttnn_to_layout_100, device = device)
  ttnn_subtract_7 = ttnn.subtract(ttnn_permute_27, ttnn_to_device_44, )
  ttnn_from_device_69 = ttnn.from_device(ttnn_reshape_44, )
  ttnn_to_layout_101 = ttnn.to_layout(ttnn_from_device_69, ttnn.TILE_LAYOUT, )
  ttnn_to_device_45 = ttnn.to_device(ttnn_to_layout_101, device = device)
  ttnn_multiply_14 = ttnn.multiply(ttnn_subtract_7, ttnn_to_device_45, )
  ttnn_from_torch_38 = ttnn.from_torch(arg22_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_46 = ttnn.reshape(ttnn_from_torch_38, (1, 72, 1, 1), )
  ttnn_from_device_70 = ttnn.from_device(ttnn_reshape_46, )
  ttnn_to_layout_102 = ttnn.to_layout(ttnn_from_device_70, ttnn.TILE_LAYOUT, )
  ttnn_to_device_46 = ttnn.to_device(ttnn_to_layout_102, device = device)
  ttnn_multiply_15 = ttnn.multiply(ttnn_multiply_14, ttnn_to_device_46, )
  ttnn_from_torch_39 = ttnn.from_torch(arg23_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_47 = ttnn.reshape(ttnn_from_torch_39, (1, 72, 1, 1), )
  ttnn_from_device_71 = ttnn.from_device(ttnn_reshape_47, )
  ttnn_to_layout_103 = ttnn.to_layout(ttnn_from_device_71, ttnn.TILE_LAYOUT, )
  ttnn_to_device_47 = ttnn.to_device(ttnn_to_layout_103, device = device)
  ttnn_add_16 = ttnn.add(ttnn_multiply_15, ttnn_to_device_47, )
  ttnn_prefix_pack_to_tuple_7 = pack_to_tuple_wrapper(ttnn_add_16, )
  ttnn_prefix_getitem_7 = ttnn_prefix_pack_to_tuple_7[0]
  ttnn_relu_4 = ttnn.relu(ttnn_prefix_getitem_7, )
  ttnn_permute_28 = ttnn.permute(ttnn_relu_4, (0, 2, 3, 1), )
  ttnn_from_device_72 = ttnn.from_device(ttnn_permute_28, )
  ttnn_to_layout_104 = ttnn.to_layout(ttnn_from_device_72, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_48 = ttnn.reshape(ttnn_to_layout_104, (1, 1, 6400, 72), )
  ttnn_from_device_73 = ttnn.from_device(ttnn_reshape_48, )
  ttnn_to_layout_105 = ttnn.to_layout(ttnn_from_device_73, ttnn.TILE_LAYOUT, )
  ttnn_to_device_48 = ttnn.to_device(ttnn_to_layout_105, device = device)
  ttnn_from_torch_40 = ttnn.from_torch(arg24_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_8 = conv_wrapper(ttnn_to_device_48, ttnn_from_torch_40, None, 1, 72, 24, [80, 80], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_8 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_8, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_74 = ttnn.from_device(ttnn_sharded_to_interleaved_8, )
  ttnn_to_layout_106 = ttnn.to_layout(ttnn_from_device_74, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_49 = ttnn.reshape(ttnn_to_layout_106, (1, 80, 80, 24), )
  ttnn_from_device_75 = ttnn.from_device(ttnn_reshape_49, )
  ttnn_to_layout_107 = ttnn.to_layout(ttnn_from_device_75, ttnn.TILE_LAYOUT, )
  ttnn_to_device_49 = ttnn.to_device(ttnn_to_layout_107, device = device)
  ttnn_permute_29 = ttnn.permute(ttnn_to_device_49, (0, 3, 1, 2), )
  test_accuracy(convolution_8, ttnn_permute_29)
  ttnn_from_torch_41 = ttnn.from_torch(arg297_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_17 = ttnn.add(ttnn_from_torch_41, 0.001, )
  ttnn_rsqrt_8 = ttnn.rsqrt(ttnn_add_17, )
  ttnn_from_device_76 = ttnn.from_device(ttnn_rsqrt_8, )
  ttnn_to_layout_108 = ttnn.to_layout(ttnn_from_device_76, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_50 = ttnn.reshape(ttnn_to_layout_108, (1, 24, 1, 1), )
  ttnn_from_torch_42 = ttnn.from_torch(arg296_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_51 = ttnn.reshape(ttnn_from_torch_42, (1, 24, 1, 1), )
  ttnn_from_device_77 = ttnn.from_device(ttnn_reshape_51, )
  ttnn_to_layout_109 = ttnn.to_layout(ttnn_from_device_77, ttnn.TILE_LAYOUT, )
  ttnn_to_device_50 = ttnn.to_device(ttnn_to_layout_109, device = device)
  ttnn_subtract_8 = ttnn.subtract(ttnn_permute_29, ttnn_to_device_50, )
  ttnn_from_device_78 = ttnn.from_device(ttnn_reshape_50, )
  ttnn_to_layout_110 = ttnn.to_layout(ttnn_from_device_78, ttnn.TILE_LAYOUT, )
  ttnn_to_device_51 = ttnn.to_device(ttnn_to_layout_110, device = device)
  ttnn_multiply_16 = ttnn.multiply(ttnn_subtract_8, ttnn_to_device_51, )
  ttnn_from_torch_43 = ttnn.from_torch(arg25_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_52 = ttnn.reshape(ttnn_from_torch_43, (1, 24, 1, 1), )
  ttnn_from_device_79 = ttnn.from_device(ttnn_reshape_52, )
  ttnn_to_layout_111 = ttnn.to_layout(ttnn_from_device_79, ttnn.TILE_LAYOUT, )
  ttnn_to_device_52 = ttnn.to_device(ttnn_to_layout_111, device = device)
  ttnn_multiply_17 = ttnn.multiply(ttnn_multiply_16, ttnn_to_device_52, )
  ttnn_from_torch_44 = ttnn.from_torch(arg26_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_53 = ttnn.reshape(ttnn_from_torch_44, (1, 24, 1, 1), )
  ttnn_from_device_80 = ttnn.from_device(ttnn_reshape_53, )
  ttnn_to_layout_112 = ttnn.to_layout(ttnn_from_device_80, ttnn.TILE_LAYOUT, )
  ttnn_to_device_53 = ttnn.to_device(ttnn_to_layout_112, device = device)
  ttnn_add_18 = ttnn.add(ttnn_multiply_17, ttnn_to_device_53, )
  ttnn_prefix_pack_to_tuple_8 = pack_to_tuple_wrapper(ttnn_add_18, )
  ttnn_prefix_getitem_8 = ttnn_prefix_pack_to_tuple_8[0]
  test_accuracy(getitem_24, ttnn_prefix_getitem_8)
  ttnn_add_19 = ttnn.add(ttnn_prefix_getitem_8, ttnn_prefix_getitem_5, )
  test_accuracy(add_6, ttnn_add_19)
  ttnn_permute_30 = ttnn.permute(ttnn_add_19, (0, 2, 3, 1), )
  ttnn_from_device_81 = ttnn.from_device(ttnn_permute_30, )
  ttnn_to_layout_113 = ttnn.to_layout(ttnn_from_device_81, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_54 = ttnn.reshape(ttnn_to_layout_113, (1, 1, 6400, 24), )
  ttnn_from_device_82 = ttnn.from_device(ttnn_reshape_54, )
  ttnn_to_layout_114 = ttnn.to_layout(ttnn_from_device_82, ttnn.TILE_LAYOUT, )
  ttnn_to_device_54 = ttnn.to_device(ttnn_to_layout_114, device = device)
  ttnn_from_torch_45 = ttnn.from_torch(arg27_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_9 = conv_wrapper(ttnn_to_device_54, ttnn_from_torch_45, None, 1, 24, 72, [80, 80], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_9 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_9, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_83 = ttnn.from_device(ttnn_sharded_to_interleaved_9, )
  ttnn_to_layout_115 = ttnn.to_layout(ttnn_from_device_83, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_55 = ttnn.reshape(ttnn_to_layout_115, (1, 80, 80, 72), )
  ttnn_from_device_84 = ttnn.from_device(ttnn_reshape_55, )
  ttnn_to_layout_116 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_55 = ttnn.to_device(ttnn_to_layout_116, device = device)
  ttnn_permute_31 = ttnn.permute(ttnn_to_device_55, (0, 3, 1, 2), )
  test_accuracy(convolution_9, ttnn_permute_31)
  ttnn_from_torch_46 = ttnn.from_torch(arg300_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_20 = ttnn.add(ttnn_from_torch_46, 0.001, )
  ttnn_rsqrt_9 = ttnn.rsqrt(ttnn_add_20, )
  ttnn_from_device_85 = ttnn.from_device(ttnn_rsqrt_9, )
  ttnn_to_layout_117 = ttnn.to_layout(ttnn_from_device_85, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_56 = ttnn.reshape(ttnn_to_layout_117, (1, 72, 1, 1), )
  ttnn_from_torch_47 = ttnn.from_torch(arg299_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_57 = ttnn.reshape(ttnn_from_torch_47, (1, 72, 1, 1), )
  ttnn_from_device_86 = ttnn.from_device(ttnn_reshape_57, )
  ttnn_to_layout_118 = ttnn.to_layout(ttnn_from_device_86, ttnn.TILE_LAYOUT, )
  ttnn_to_device_56 = ttnn.to_device(ttnn_to_layout_118, device = device)
  ttnn_subtract_9 = ttnn.subtract(ttnn_permute_31, ttnn_to_device_56, )
  ttnn_from_device_87 = ttnn.from_device(ttnn_reshape_56, )
  ttnn_to_layout_119 = ttnn.to_layout(ttnn_from_device_87, ttnn.TILE_LAYOUT, )
  ttnn_to_device_57 = ttnn.to_device(ttnn_to_layout_119, device = device)
  ttnn_multiply_18 = ttnn.multiply(ttnn_subtract_9, ttnn_to_device_57, )
  ttnn_from_torch_48 = ttnn.from_torch(arg28_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_58 = ttnn.reshape(ttnn_from_torch_48, (1, 72, 1, 1), )
  ttnn_from_device_88 = ttnn.from_device(ttnn_reshape_58, )
  ttnn_to_layout_120 = ttnn.to_layout(ttnn_from_device_88, ttnn.TILE_LAYOUT, )
  ttnn_to_device_58 = ttnn.to_device(ttnn_to_layout_120, device = device)
  ttnn_multiply_19 = ttnn.multiply(ttnn_multiply_18, ttnn_to_device_58, )
  ttnn_from_torch_49 = ttnn.from_torch(arg29_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_59 = ttnn.reshape(ttnn_from_torch_49, (1, 72, 1, 1), )
  ttnn_from_device_89 = ttnn.from_device(ttnn_reshape_59, )
  ttnn_to_layout_121 = ttnn.to_layout(ttnn_from_device_89, ttnn.TILE_LAYOUT, )
  ttnn_to_device_59 = ttnn.to_device(ttnn_to_layout_121, device = device)
  ttnn_add_21 = ttnn.add(ttnn_multiply_19, ttnn_to_device_59, )
  ttnn_prefix_pack_to_tuple_9 = pack_to_tuple_wrapper(ttnn_add_21, )
  ttnn_prefix_getitem_9 = ttnn_prefix_pack_to_tuple_9[0]
  test_accuracy(getitem_27, ttnn_prefix_getitem_9)
  ttnn_relu_5 = ttnn.relu(ttnn_prefix_getitem_9, )
  test_accuracy(relu_5, ttnn_relu_5)
  ttnn_permute_32 = ttnn.permute(ttnn_relu_5, (0, 2, 3, 1), )
  ttnn_from_device_90 = ttnn.from_device(ttnn_permute_32, )
  ttnn_to_layout_122 = ttnn.to_layout(ttnn_from_device_90, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_60 = ttnn.reshape(ttnn_to_layout_122, (1, 1, 6400, 72), )
  ttnn_from_device_91 = ttnn.from_device(ttnn_reshape_60, )
  ttnn_to_layout_123 = ttnn.to_layout(ttnn_from_device_91, ttnn.TILE_LAYOUT, )
  ttnn_to_device_60 = ttnn.to_device(ttnn_to_layout_123, device = device)
  ttnn_from_torch_50 = ttnn.from_torch(arg30_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_10 = conv_wrapper(ttnn_to_device_60, ttnn_from_torch_50, None, 1, 72, 72, [80, 80], [5, 5], [2, 2], [2, 2], [1, 1], 72, device, False, None, )
  ttnn_sharded_to_interleaved_10 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_10, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_92 = ttnn.from_device(ttnn_sharded_to_interleaved_10, )
  ttnn_to_layout_124 = ttnn.to_layout(ttnn_from_device_92, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_61 = ttnn.reshape(ttnn_to_layout_124, (1, 40, 40, 72), )
  ttnn_from_device_93 = ttnn.from_device(ttnn_reshape_61, )
  ttnn_to_layout_125 = ttnn.to_layout(ttnn_from_device_93, ttnn.TILE_LAYOUT, )
  ttnn_to_device_61 = ttnn.to_device(ttnn_to_layout_125, device = device)
  ttnn_permute_33 = ttnn.permute(ttnn_to_device_61, (0, 3, 1, 2), )
  test_accuracy(convolution_10, ttnn_permute_33)
  ttnn_from_torch_51 = ttnn.from_torch(arg303_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_22 = ttnn.add(ttnn_from_torch_51, 0.001, )
  ttnn_rsqrt_10 = ttnn.rsqrt(ttnn_add_22, )
  ttnn_from_device_94 = ttnn.from_device(ttnn_rsqrt_10, )
  ttnn_to_layout_126 = ttnn.to_layout(ttnn_from_device_94, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_62 = ttnn.reshape(ttnn_to_layout_126, (1, 72, 1, 1), )
  ttnn_from_torch_52 = ttnn.from_torch(arg302_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_63 = ttnn.reshape(ttnn_from_torch_52, (1, 72, 1, 1), )
  ttnn_from_device_95 = ttnn.from_device(ttnn_reshape_63, )
  ttnn_to_layout_127 = ttnn.to_layout(ttnn_from_device_95, ttnn.TILE_LAYOUT, )
  ttnn_to_device_62 = ttnn.to_device(ttnn_to_layout_127, device = device)
  ttnn_subtract_10 = ttnn.subtract(ttnn_permute_33, ttnn_to_device_62, )
  ttnn_from_device_96 = ttnn.from_device(ttnn_reshape_62, )
  ttnn_to_layout_128 = ttnn.to_layout(ttnn_from_device_96, ttnn.TILE_LAYOUT, )
  ttnn_to_device_63 = ttnn.to_device(ttnn_to_layout_128, device = device)
  ttnn_multiply_20 = ttnn.multiply(ttnn_subtract_10, ttnn_to_device_63, )
  ttnn_from_torch_53 = ttnn.from_torch(arg31_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_64 = ttnn.reshape(ttnn_from_torch_53, (1, 72, 1, 1), )
  ttnn_from_device_97 = ttnn.from_device(ttnn_reshape_64, )
  ttnn_to_layout_129 = ttnn.to_layout(ttnn_from_device_97, ttnn.TILE_LAYOUT, )
  ttnn_to_device_64 = ttnn.to_device(ttnn_to_layout_129, device = device)
  ttnn_multiply_21 = ttnn.multiply(ttnn_multiply_20, ttnn_to_device_64, )
  ttnn_from_torch_54 = ttnn.from_torch(arg32_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_65 = ttnn.reshape(ttnn_from_torch_54, (1, 72, 1, 1), )
  ttnn_from_device_98 = ttnn.from_device(ttnn_reshape_65, )
  ttnn_to_layout_130 = ttnn.to_layout(ttnn_from_device_98, ttnn.TILE_LAYOUT, )
  ttnn_to_device_65 = ttnn.to_device(ttnn_to_layout_130, device = device)
  ttnn_add_23 = ttnn.add(ttnn_multiply_21, ttnn_to_device_65, )
  ttnn_prefix_pack_to_tuple_10 = pack_to_tuple_wrapper(ttnn_add_23, )
  ttnn_prefix_getitem_10 = ttnn_prefix_pack_to_tuple_10[0]
  test_accuracy(getitem_30, ttnn_prefix_getitem_10)
  ttnn_relu_6 = ttnn.relu(ttnn_prefix_getitem_10, )
  test_accuracy(relu_6, ttnn_relu_6)
  ttnn_mean = ttnn.mean(ttnn_relu_6, (-1, -2), )
  test_accuracy(mean, ttnn_mean)
  ttnn_permute_34 = ttnn.permute(ttnn_mean, (0, 2, 3, 1), )
  ttnn_from_device_99 = ttnn.from_device(ttnn_permute_34, )
  ttnn_to_layout_131 = ttnn.to_layout(ttnn_from_device_99, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_66 = ttnn.reshape(ttnn_to_layout_131, (1, 1, 1, 72), )
  ttnn_from_torch_55 = ttnn.from_torch(arg34_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_67 = ttnn.reshape(ttnn_from_torch_55, (1, 1, 1, 24), )
  ttnn_from_device_100 = ttnn.from_device(ttnn_reshape_67, )
  ttnn_to_layout_132 = ttnn.to_layout(ttnn_from_device_100, ttnn.TILE_LAYOUT, )
  ttnn_to_device_66 = ttnn.to_device(ttnn_to_layout_132, device = device)
  ttnn_prefix_move_to_host = move_to_host_wrapper(ttnn_to_device_66, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_101 = ttnn.from_device(ttnn_reshape_66, )
  ttnn_to_layout_133 = ttnn.to_layout(ttnn_from_device_101, ttnn.TILE_LAYOUT, )
  ttnn_to_device_67 = ttnn.to_device(ttnn_to_layout_133, device = device)
  ttnn_from_torch_56 = ttnn.from_torch(arg33_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_11 = conv_wrapper(ttnn_to_device_67, ttnn_from_torch_56, ttnn_prefix_move_to_host, 1, 72, 24, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_11 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_11, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_102 = ttnn.from_device(ttnn_sharded_to_interleaved_11, )
  ttnn_to_layout_134 = ttnn.to_layout(ttnn_from_device_102, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_68 = ttnn.reshape(ttnn_to_layout_134, (1, 1, 1, 24), )
  ttnn_from_device_103 = ttnn.from_device(ttnn_reshape_68, )
  ttnn_to_layout_135 = ttnn.to_layout(ttnn_from_device_103, ttnn.TILE_LAYOUT, )
  ttnn_to_device_68 = ttnn.to_device(ttnn_to_layout_135, device = device)
  ttnn_permute_35 = ttnn.permute(ttnn_to_device_68, (0, 3, 1, 2), )
  ttnn_relu_7 = ttnn.relu(ttnn_permute_35, )
  test_accuracy(relu_7, ttnn_relu_7)
  ttnn_permute_36 = ttnn.permute(ttnn_relu_7, (0, 2, 3, 1), )
  ttnn_from_device_104 = ttnn.from_device(ttnn_permute_36, )
  ttnn_to_layout_136 = ttnn.to_layout(ttnn_from_device_104, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_69 = ttnn.reshape(ttnn_to_layout_136, (1, 1, 1, 24), )
  ttnn_from_torch_57 = ttnn.from_torch(arg36_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_70 = ttnn.reshape(ttnn_from_torch_57, (1, 1, 1, 72), )
  ttnn_from_device_105 = ttnn.from_device(ttnn_reshape_70, )
  ttnn_to_layout_137 = ttnn.to_layout(ttnn_from_device_105, ttnn.TILE_LAYOUT, )
  ttnn_to_device_69 = ttnn.to_device(ttnn_to_layout_137, device = device)
  ttnn_prefix_move_to_host_1 = move_to_host_wrapper(ttnn_to_device_69, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_106 = ttnn.from_device(ttnn_reshape_69, )
  ttnn_to_layout_138 = ttnn.to_layout(ttnn_from_device_106, ttnn.TILE_LAYOUT, )
  ttnn_to_device_70 = ttnn.to_device(ttnn_to_layout_138, device = device)
  ttnn_from_torch_58 = ttnn.from_torch(arg35_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_12 = conv_wrapper(ttnn_to_device_70, ttnn_from_torch_58, ttnn_prefix_move_to_host_1, 1, 24, 72, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_12 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_12, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_107 = ttnn.from_device(ttnn_sharded_to_interleaved_12, )
  ttnn_to_layout_139 = ttnn.to_layout(ttnn_from_device_107, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_71 = ttnn.reshape(ttnn_to_layout_139, (1, 1, 1, 72), )
  ttnn_from_device_108 = ttnn.from_device(ttnn_reshape_71, )
  ttnn_to_layout_140 = ttnn.to_layout(ttnn_from_device_108, ttnn.TILE_LAYOUT, )
  ttnn_to_device_71 = ttnn.to_device(ttnn_to_layout_140, device = device)
  ttnn_permute_37 = ttnn.permute(ttnn_to_device_71, (0, 3, 1, 2), )
  test_accuracy(convolution_12, ttnn_permute_37)
  ttnn_hardsigmoid = ttnn.hardsigmoid(ttnn_permute_37, )
  test_accuracy(hardsigmoid, ttnn_hardsigmoid)
  ttnn_multiply_22 = ttnn.multiply(ttnn_hardsigmoid, ttnn_relu_6, )
  test_accuracy(mul_8, ttnn_multiply_22)
  ttnn_permute_38 = ttnn.permute(ttnn_multiply_22, (0, 2, 3, 1), )
  ttnn_from_device_109 = ttnn.from_device(ttnn_permute_38, )
  ttnn_to_layout_141 = ttnn.to_layout(ttnn_from_device_109, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_72 = ttnn.reshape(ttnn_to_layout_141, (1, 1, 1600, 72), )
  ttnn_from_device_110 = ttnn.from_device(ttnn_reshape_72, )
  ttnn_to_layout_142 = ttnn.to_layout(ttnn_from_device_110, ttnn.TILE_LAYOUT, )
  ttnn_to_device_72 = ttnn.to_device(ttnn_to_layout_142, device = device)
  ttnn_from_torch_59 = ttnn.from_torch(arg37_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_13 = conv_wrapper(ttnn_to_device_72, ttnn_from_torch_59, None, 1, 72, 40, [40, 40], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_13 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_13, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_111 = ttnn.from_device(ttnn_sharded_to_interleaved_13, )
  ttnn_to_layout_143 = ttnn.to_layout(ttnn_from_device_111, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_73 = ttnn.reshape(ttnn_to_layout_143, (1, 40, 40, 40), )
  ttnn_from_device_112 = ttnn.from_device(ttnn_reshape_73, )
  ttnn_to_layout_144 = ttnn.to_layout(ttnn_from_device_112, ttnn.TILE_LAYOUT, )
  ttnn_to_device_73 = ttnn.to_device(ttnn_to_layout_144, device = device)
  ttnn_permute_39 = ttnn.permute(ttnn_to_device_73, (0, 3, 1, 2), )
  ttnn_from_torch_60 = ttnn.from_torch(arg306_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_24 = ttnn.add(ttnn_from_torch_60, 0.001, )
  ttnn_rsqrt_11 = ttnn.rsqrt(ttnn_add_24, )
  ttnn_from_device_113 = ttnn.from_device(ttnn_rsqrt_11, )
  ttnn_to_layout_145 = ttnn.to_layout(ttnn_from_device_113, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_74 = ttnn.reshape(ttnn_to_layout_145, (1, 40, 1, 1), )
  ttnn_from_torch_61 = ttnn.from_torch(arg305_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_75 = ttnn.reshape(ttnn_from_torch_61, (1, 40, 1, 1), )
  ttnn_from_device_114 = ttnn.from_device(ttnn_reshape_75, )
  ttnn_to_layout_146 = ttnn.to_layout(ttnn_from_device_114, ttnn.TILE_LAYOUT, )
  ttnn_to_device_74 = ttnn.to_device(ttnn_to_layout_146, device = device)
  ttnn_subtract_11 = ttnn.subtract(ttnn_permute_39, ttnn_to_device_74, )
  ttnn_from_device_115 = ttnn.from_device(ttnn_reshape_74, )
  ttnn_to_layout_147 = ttnn.to_layout(ttnn_from_device_115, ttnn.TILE_LAYOUT, )
  ttnn_to_device_75 = ttnn.to_device(ttnn_to_layout_147, device = device)
  ttnn_multiply_23 = ttnn.multiply(ttnn_subtract_11, ttnn_to_device_75, )
  ttnn_from_torch_62 = ttnn.from_torch(arg38_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_76 = ttnn.reshape(ttnn_from_torch_62, (1, 40, 1, 1), )
  ttnn_from_device_116 = ttnn.from_device(ttnn_reshape_76, )
  ttnn_to_layout_148 = ttnn.to_layout(ttnn_from_device_116, ttnn.TILE_LAYOUT, )
  ttnn_to_device_76 = ttnn.to_device(ttnn_to_layout_148, device = device)
  ttnn_multiply_24 = ttnn.multiply(ttnn_multiply_23, ttnn_to_device_76, )
  ttnn_from_torch_63 = ttnn.from_torch(arg39_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_77 = ttnn.reshape(ttnn_from_torch_63, (1, 40, 1, 1), )
  ttnn_from_device_117 = ttnn.from_device(ttnn_reshape_77, )
  ttnn_to_layout_149 = ttnn.to_layout(ttnn_from_device_117, ttnn.TILE_LAYOUT, )
  ttnn_to_device_77 = ttnn.to_device(ttnn_to_layout_149, device = device)
  ttnn_add_25 = ttnn.add(ttnn_multiply_24, ttnn_to_device_77, )
  ttnn_prefix_pack_to_tuple_11 = pack_to_tuple_wrapper(ttnn_add_25, )
  ttnn_prefix_getitem_11 = ttnn_prefix_pack_to_tuple_11[0]
  ttnn_permute_40 = ttnn.permute(ttnn_prefix_getitem_11, (0, 2, 3, 1), )
  ttnn_from_device_118 = ttnn.from_device(ttnn_permute_40, )
  ttnn_to_layout_150 = ttnn.to_layout(ttnn_from_device_118, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_78 = ttnn.reshape(ttnn_to_layout_150, (1, 1, 1600, 40), )
  ttnn_from_device_119 = ttnn.from_device(ttnn_reshape_78, )
  ttnn_to_layout_151 = ttnn.to_layout(ttnn_from_device_119, ttnn.TILE_LAYOUT, )
  ttnn_to_device_78 = ttnn.to_device(ttnn_to_layout_151, device = device)
  ttnn_from_torch_64 = ttnn.from_torch(arg40_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_14 = conv_wrapper(ttnn_to_device_78, ttnn_from_torch_64, None, 1, 40, 120, [40, 40], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_14 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_14, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_120 = ttnn.from_device(ttnn_sharded_to_interleaved_14, )
  ttnn_to_layout_152 = ttnn.to_layout(ttnn_from_device_120, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_79 = ttnn.reshape(ttnn_to_layout_152, (1, 40, 40, 120), )
  ttnn_from_device_121 = ttnn.from_device(ttnn_reshape_79, )
  ttnn_to_layout_153 = ttnn.to_layout(ttnn_from_device_121, ttnn.TILE_LAYOUT, )
  ttnn_to_device_79 = ttnn.to_device(ttnn_to_layout_153, device = device)
  ttnn_permute_41 = ttnn.permute(ttnn_to_device_79, (0, 3, 1, 2), )
  ttnn_from_torch_65 = ttnn.from_torch(arg309_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_26 = ttnn.add(ttnn_from_torch_65, 0.001, )
  ttnn_rsqrt_12 = ttnn.rsqrt(ttnn_add_26, )
  ttnn_from_device_122 = ttnn.from_device(ttnn_rsqrt_12, )
  ttnn_to_layout_154 = ttnn.to_layout(ttnn_from_device_122, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_80 = ttnn.reshape(ttnn_to_layout_154, (1, 120, 1, 1), )
  ttnn_from_torch_66 = ttnn.from_torch(arg308_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_81 = ttnn.reshape(ttnn_from_torch_66, (1, 120, 1, 1), )
  ttnn_from_device_123 = ttnn.from_device(ttnn_reshape_81, )
  ttnn_to_layout_155 = ttnn.to_layout(ttnn_from_device_123, ttnn.TILE_LAYOUT, )
  ttnn_to_device_80 = ttnn.to_device(ttnn_to_layout_155, device = device)
  ttnn_subtract_12 = ttnn.subtract(ttnn_permute_41, ttnn_to_device_80, )
  ttnn_from_device_124 = ttnn.from_device(ttnn_reshape_80, )
  ttnn_to_layout_156 = ttnn.to_layout(ttnn_from_device_124, ttnn.TILE_LAYOUT, )
  ttnn_to_device_81 = ttnn.to_device(ttnn_to_layout_156, device = device)
  ttnn_multiply_25 = ttnn.multiply(ttnn_subtract_12, ttnn_to_device_81, )
  ttnn_from_torch_67 = ttnn.from_torch(arg41_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_82 = ttnn.reshape(ttnn_from_torch_67, (1, 120, 1, 1), )
  ttnn_from_device_125 = ttnn.from_device(ttnn_reshape_82, )
  ttnn_to_layout_157 = ttnn.to_layout(ttnn_from_device_125, ttnn.TILE_LAYOUT, )
  ttnn_to_device_82 = ttnn.to_device(ttnn_to_layout_157, device = device)
  ttnn_multiply_26 = ttnn.multiply(ttnn_multiply_25, ttnn_to_device_82, )
  ttnn_from_torch_68 = ttnn.from_torch(arg42_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_83 = ttnn.reshape(ttnn_from_torch_68, (1, 120, 1, 1), )
  ttnn_from_device_126 = ttnn.from_device(ttnn_reshape_83, )
  ttnn_to_layout_158 = ttnn.to_layout(ttnn_from_device_126, ttnn.TILE_LAYOUT, )
  ttnn_to_device_83 = ttnn.to_device(ttnn_to_layout_158, device = device)
  ttnn_add_27 = ttnn.add(ttnn_multiply_26, ttnn_to_device_83, )
  ttnn_prefix_pack_to_tuple_12 = pack_to_tuple_wrapper(ttnn_add_27, )
  ttnn_prefix_getitem_12 = ttnn_prefix_pack_to_tuple_12[0]
  ttnn_relu_8 = ttnn.relu(ttnn_prefix_getitem_12, )
  ttnn_permute_42 = ttnn.permute(ttnn_relu_8, (0, 2, 3, 1), )
  ttnn_from_device_127 = ttnn.from_device(ttnn_permute_42, )
  ttnn_to_layout_159 = ttnn.to_layout(ttnn_from_device_127, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_84 = ttnn.reshape(ttnn_to_layout_159, (1, 1, 1600, 120), )
  ttnn_from_device_128 = ttnn.from_device(ttnn_reshape_84, )
  ttnn_to_layout_160 = ttnn.to_layout(ttnn_from_device_128, ttnn.TILE_LAYOUT, )
  ttnn_to_device_84 = ttnn.to_device(ttnn_to_layout_160, device = device)
  ttnn_from_torch_69 = ttnn.from_torch(arg43_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_15 = conv_wrapper(ttnn_to_device_84, ttnn_from_torch_69, None, 1, 120, 120, [40, 40], [5, 5], [1, 1], [2, 2], [1, 1], 120, device, False, None, )
  ttnn_sharded_to_interleaved_15 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_15, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_129 = ttnn.from_device(ttnn_sharded_to_interleaved_15, )
  ttnn_to_layout_161 = ttnn.to_layout(ttnn_from_device_129, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_85 = ttnn.reshape(ttnn_to_layout_161, (1, 40, 40, 120), )
  ttnn_from_device_130 = ttnn.from_device(ttnn_reshape_85, )
  ttnn_to_layout_162 = ttnn.to_layout(ttnn_from_device_130, ttnn.TILE_LAYOUT, )
  ttnn_to_device_85 = ttnn.to_device(ttnn_to_layout_162, device = device)
  ttnn_permute_43 = ttnn.permute(ttnn_to_device_85, (0, 3, 1, 2), )
  ttnn_from_torch_70 = ttnn.from_torch(arg312_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_28 = ttnn.add(ttnn_from_torch_70, 0.001, )
  ttnn_rsqrt_13 = ttnn.rsqrt(ttnn_add_28, )
  ttnn_from_device_131 = ttnn.from_device(ttnn_rsqrt_13, )
  ttnn_to_layout_163 = ttnn.to_layout(ttnn_from_device_131, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_86 = ttnn.reshape(ttnn_to_layout_163, (1, 120, 1, 1), )
  ttnn_from_torch_71 = ttnn.from_torch(arg311_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_87 = ttnn.reshape(ttnn_from_torch_71, (1, 120, 1, 1), )
  ttnn_from_device_132 = ttnn.from_device(ttnn_reshape_87, )
  ttnn_to_layout_164 = ttnn.to_layout(ttnn_from_device_132, ttnn.TILE_LAYOUT, )
  ttnn_to_device_86 = ttnn.to_device(ttnn_to_layout_164, device = device)
  ttnn_subtract_13 = ttnn.subtract(ttnn_permute_43, ttnn_to_device_86, )
  ttnn_from_device_133 = ttnn.from_device(ttnn_reshape_86, )
  ttnn_to_layout_165 = ttnn.to_layout(ttnn_from_device_133, ttnn.TILE_LAYOUT, )
  ttnn_to_device_87 = ttnn.to_device(ttnn_to_layout_165, device = device)
  ttnn_multiply_27 = ttnn.multiply(ttnn_subtract_13, ttnn_to_device_87, )
  ttnn_from_torch_72 = ttnn.from_torch(arg44_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_88 = ttnn.reshape(ttnn_from_torch_72, (1, 120, 1, 1), )
  ttnn_from_device_134 = ttnn.from_device(ttnn_reshape_88, )
  ttnn_to_layout_166 = ttnn.to_layout(ttnn_from_device_134, ttnn.TILE_LAYOUT, )
  ttnn_to_device_88 = ttnn.to_device(ttnn_to_layout_166, device = device)
  ttnn_multiply_28 = ttnn.multiply(ttnn_multiply_27, ttnn_to_device_88, )
  ttnn_from_torch_73 = ttnn.from_torch(arg45_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_89 = ttnn.reshape(ttnn_from_torch_73, (1, 120, 1, 1), )
  ttnn_from_device_135 = ttnn.from_device(ttnn_reshape_89, )
  ttnn_to_layout_167 = ttnn.to_layout(ttnn_from_device_135, ttnn.TILE_LAYOUT, )
  ttnn_to_device_89 = ttnn.to_device(ttnn_to_layout_167, device = device)
  ttnn_add_29 = ttnn.add(ttnn_multiply_28, ttnn_to_device_89, )
  ttnn_prefix_pack_to_tuple_13 = pack_to_tuple_wrapper(ttnn_add_29, )
  ttnn_prefix_getitem_13 = ttnn_prefix_pack_to_tuple_13[0]
  ttnn_relu_9 = ttnn.relu(ttnn_prefix_getitem_13, )
  ttnn_mean_1 = ttnn.mean(ttnn_relu_9, (-1, -2), )
  ttnn_permute_44 = ttnn.permute(ttnn_mean_1, (0, 2, 3, 1), )
  ttnn_from_device_136 = ttnn.from_device(ttnn_permute_44, )
  ttnn_to_layout_168 = ttnn.to_layout(ttnn_from_device_136, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_90 = ttnn.reshape(ttnn_to_layout_168, (1, 1, 1, 120), )
  ttnn_from_torch_74 = ttnn.from_torch(arg47_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_91 = ttnn.reshape(ttnn_from_torch_74, (1, 1, 1, 32), )
  ttnn_from_device_137 = ttnn.from_device(ttnn_reshape_91, )
  ttnn_to_layout_169 = ttnn.to_layout(ttnn_from_device_137, ttnn.TILE_LAYOUT, )
  ttnn_to_device_90 = ttnn.to_device(ttnn_to_layout_169, device = device)
  ttnn_prefix_move_to_host_2 = move_to_host_wrapper(ttnn_to_device_90, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_138 = ttnn.from_device(ttnn_reshape_90, )
  ttnn_to_layout_170 = ttnn.to_layout(ttnn_from_device_138, ttnn.TILE_LAYOUT, )
  ttnn_to_device_91 = ttnn.to_device(ttnn_to_layout_170, device = device)
  ttnn_from_torch_75 = ttnn.from_torch(arg46_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_16 = conv_wrapper(ttnn_to_device_91, ttnn_from_torch_75, ttnn_prefix_move_to_host_2, 1, 120, 32, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_16 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_16, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_139 = ttnn.from_device(ttnn_sharded_to_interleaved_16, )
  ttnn_to_layout_171 = ttnn.to_layout(ttnn_from_device_139, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_92 = ttnn.reshape(ttnn_to_layout_171, (1, 1, 1, 32), )
  ttnn_from_device_140 = ttnn.from_device(ttnn_reshape_92, )
  ttnn_to_layout_172 = ttnn.to_layout(ttnn_from_device_140, ttnn.TILE_LAYOUT, )
  ttnn_to_device_92 = ttnn.to_device(ttnn_to_layout_172, device = device)
  ttnn_permute_45 = ttnn.permute(ttnn_to_device_92, (0, 3, 1, 2), )
  ttnn_relu_10 = ttnn.relu(ttnn_permute_45, )
  ttnn_permute_46 = ttnn.permute(ttnn_relu_10, (0, 2, 3, 1), )
  ttnn_from_device_141 = ttnn.from_device(ttnn_permute_46, )
  ttnn_to_layout_173 = ttnn.to_layout(ttnn_from_device_141, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_93 = ttnn.reshape(ttnn_to_layout_173, (1, 1, 1, 32), )
  ttnn_from_torch_76 = ttnn.from_torch(arg49_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_94 = ttnn.reshape(ttnn_from_torch_76, (1, 1, 1, 120), )
  ttnn_from_device_142 = ttnn.from_device(ttnn_reshape_94, )
  ttnn_to_layout_174 = ttnn.to_layout(ttnn_from_device_142, ttnn.TILE_LAYOUT, )
  ttnn_to_device_93 = ttnn.to_device(ttnn_to_layout_174, device = device)
  ttnn_prefix_move_to_host_3 = move_to_host_wrapper(ttnn_to_device_93, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_143 = ttnn.from_device(ttnn_reshape_93, )
  ttnn_to_layout_175 = ttnn.to_layout(ttnn_from_device_143, ttnn.TILE_LAYOUT, )
  ttnn_to_device_94 = ttnn.to_device(ttnn_to_layout_175, device = device)
  ttnn_from_torch_77 = ttnn.from_torch(arg48_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_17 = conv_wrapper(ttnn_to_device_94, ttnn_from_torch_77, ttnn_prefix_move_to_host_3, 1, 32, 120, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_17 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_17, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_144 = ttnn.from_device(ttnn_sharded_to_interleaved_17, )
  ttnn_to_layout_176 = ttnn.to_layout(ttnn_from_device_144, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_95 = ttnn.reshape(ttnn_to_layout_176, (1, 1, 1, 120), )
  ttnn_from_device_145 = ttnn.from_device(ttnn_reshape_95, )
  ttnn_to_layout_177 = ttnn.to_layout(ttnn_from_device_145, ttnn.TILE_LAYOUT, )
  ttnn_to_device_95 = ttnn.to_device(ttnn_to_layout_177, device = device)
  ttnn_permute_47 = ttnn.permute(ttnn_to_device_95, (0, 3, 1, 2), )
  ttnn_hardsigmoid_1 = ttnn.hardsigmoid(ttnn_permute_47, )
  ttnn_multiply_29 = ttnn.multiply(ttnn_hardsigmoid_1, ttnn_relu_9, )
  ttnn_permute_48 = ttnn.permute(ttnn_multiply_29, (0, 2, 3, 1), )
  ttnn_from_device_146 = ttnn.from_device(ttnn_permute_48, )
  ttnn_to_layout_178 = ttnn.to_layout(ttnn_from_device_146, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_96 = ttnn.reshape(ttnn_to_layout_178, (1, 1, 1600, 120), )
  ttnn_from_device_147 = ttnn.from_device(ttnn_reshape_96, )
  ttnn_to_layout_179 = ttnn.to_layout(ttnn_from_device_147, ttnn.TILE_LAYOUT, )
  ttnn_to_device_96 = ttnn.to_device(ttnn_to_layout_179, device = device)
  ttnn_from_torch_78 = ttnn.from_torch(arg50_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_18 = conv_wrapper(ttnn_to_device_96, ttnn_from_torch_78, None, 1, 120, 40, [40, 40], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_18 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_18, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_148 = ttnn.from_device(ttnn_sharded_to_interleaved_18, )
  ttnn_to_layout_180 = ttnn.to_layout(ttnn_from_device_148, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_97 = ttnn.reshape(ttnn_to_layout_180, (1, 40, 40, 40), )
  ttnn_from_device_149 = ttnn.from_device(ttnn_reshape_97, )
  ttnn_to_layout_181 = ttnn.to_layout(ttnn_from_device_149, ttnn.TILE_LAYOUT, )
  ttnn_to_device_97 = ttnn.to_device(ttnn_to_layout_181, device = device)
  ttnn_permute_49 = ttnn.permute(ttnn_to_device_97, (0, 3, 1, 2), )
  ttnn_from_torch_79 = ttnn.from_torch(arg315_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_30 = ttnn.add(ttnn_from_torch_79, 0.001, )
  ttnn_rsqrt_14 = ttnn.rsqrt(ttnn_add_30, )
  ttnn_from_device_150 = ttnn.from_device(ttnn_rsqrt_14, )
  ttnn_to_layout_182 = ttnn.to_layout(ttnn_from_device_150, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_98 = ttnn.reshape(ttnn_to_layout_182, (1, 40, 1, 1), )
  ttnn_from_torch_80 = ttnn.from_torch(arg314_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_99 = ttnn.reshape(ttnn_from_torch_80, (1, 40, 1, 1), )
  ttnn_from_device_151 = ttnn.from_device(ttnn_reshape_99, )
  ttnn_to_layout_183 = ttnn.to_layout(ttnn_from_device_151, ttnn.TILE_LAYOUT, )
  ttnn_to_device_98 = ttnn.to_device(ttnn_to_layout_183, device = device)
  ttnn_subtract_14 = ttnn.subtract(ttnn_permute_49, ttnn_to_device_98, )
  ttnn_from_device_152 = ttnn.from_device(ttnn_reshape_98, )
  ttnn_to_layout_184 = ttnn.to_layout(ttnn_from_device_152, ttnn.TILE_LAYOUT, )
  ttnn_to_device_99 = ttnn.to_device(ttnn_to_layout_184, device = device)
  ttnn_multiply_30 = ttnn.multiply(ttnn_subtract_14, ttnn_to_device_99, )
  ttnn_from_torch_81 = ttnn.from_torch(arg51_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_100 = ttnn.reshape(ttnn_from_torch_81, (1, 40, 1, 1), )
  ttnn_from_device_153 = ttnn.from_device(ttnn_reshape_100, )
  ttnn_to_layout_185 = ttnn.to_layout(ttnn_from_device_153, ttnn.TILE_LAYOUT, )
  ttnn_to_device_100 = ttnn.to_device(ttnn_to_layout_185, device = device)
  ttnn_multiply_31 = ttnn.multiply(ttnn_multiply_30, ttnn_to_device_100, )
  ttnn_from_torch_82 = ttnn.from_torch(arg52_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_101 = ttnn.reshape(ttnn_from_torch_82, (1, 40, 1, 1), )
  ttnn_from_device_154 = ttnn.from_device(ttnn_reshape_101, )
  ttnn_to_layout_186 = ttnn.to_layout(ttnn_from_device_154, ttnn.TILE_LAYOUT, )
  ttnn_to_device_101 = ttnn.to_device(ttnn_to_layout_186, device = device)
  ttnn_add_31 = ttnn.add(ttnn_multiply_31, ttnn_to_device_101, )
  ttnn_prefix_pack_to_tuple_14 = pack_to_tuple_wrapper(ttnn_add_31, )
  ttnn_prefix_getitem_14 = ttnn_prefix_pack_to_tuple_14[0]
  ttnn_add_32 = ttnn.add(ttnn_prefix_getitem_14, ttnn_prefix_getitem_11, )
  ttnn_permute_50 = ttnn.permute(ttnn_add_32, (0, 2, 3, 1), )
  ttnn_from_device_155 = ttnn.from_device(ttnn_permute_50, )
  ttnn_to_layout_187 = ttnn.to_layout(ttnn_from_device_155, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_102 = ttnn.reshape(ttnn_to_layout_187, (1, 1, 1600, 40), )
  ttnn_from_device_156 = ttnn.from_device(ttnn_reshape_102, )
  ttnn_to_layout_188 = ttnn.to_layout(ttnn_from_device_156, ttnn.TILE_LAYOUT, )
  ttnn_to_device_102 = ttnn.to_device(ttnn_to_layout_188, device = device)
  ttnn_from_torch_83 = ttnn.from_torch(arg53_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_19 = conv_wrapper(ttnn_to_device_102, ttnn_from_torch_83, None, 1, 40, 120, [40, 40], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_19 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_19, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_157 = ttnn.from_device(ttnn_sharded_to_interleaved_19, )
  ttnn_to_layout_189 = ttnn.to_layout(ttnn_from_device_157, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_103 = ttnn.reshape(ttnn_to_layout_189, (1, 40, 40, 120), )
  ttnn_from_device_158 = ttnn.from_device(ttnn_reshape_103, )
  ttnn_to_layout_190 = ttnn.to_layout(ttnn_from_device_158, ttnn.TILE_LAYOUT, )
  ttnn_to_device_103 = ttnn.to_device(ttnn_to_layout_190, device = device)
  ttnn_permute_51 = ttnn.permute(ttnn_to_device_103, (0, 3, 1, 2), )
  ttnn_from_torch_84 = ttnn.from_torch(arg318_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_33 = ttnn.add(ttnn_from_torch_84, 0.001, )
  ttnn_rsqrt_15 = ttnn.rsqrt(ttnn_add_33, )
  ttnn_from_device_159 = ttnn.from_device(ttnn_rsqrt_15, )
  ttnn_to_layout_191 = ttnn.to_layout(ttnn_from_device_159, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_104 = ttnn.reshape(ttnn_to_layout_191, (1, 120, 1, 1), )
  ttnn_from_torch_85 = ttnn.from_torch(arg317_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_105 = ttnn.reshape(ttnn_from_torch_85, (1, 120, 1, 1), )
  ttnn_from_device_160 = ttnn.from_device(ttnn_reshape_105, )
  ttnn_to_layout_192 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_104 = ttnn.to_device(ttnn_to_layout_192, device = device)
  ttnn_subtract_15 = ttnn.subtract(ttnn_permute_51, ttnn_to_device_104, )
  ttnn_from_device_161 = ttnn.from_device(ttnn_reshape_104, )
  ttnn_to_layout_193 = ttnn.to_layout(ttnn_from_device_161, ttnn.TILE_LAYOUT, )
  ttnn_to_device_105 = ttnn.to_device(ttnn_to_layout_193, device = device)
  ttnn_multiply_32 = ttnn.multiply(ttnn_subtract_15, ttnn_to_device_105, )
  ttnn_from_torch_86 = ttnn.from_torch(arg54_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_106 = ttnn.reshape(ttnn_from_torch_86, (1, 120, 1, 1), )
  ttnn_from_device_162 = ttnn.from_device(ttnn_reshape_106, )
  ttnn_to_layout_194 = ttnn.to_layout(ttnn_from_device_162, ttnn.TILE_LAYOUT, )
  ttnn_to_device_106 = ttnn.to_device(ttnn_to_layout_194, device = device)
  ttnn_multiply_33 = ttnn.multiply(ttnn_multiply_32, ttnn_to_device_106, )
  ttnn_from_torch_87 = ttnn.from_torch(arg55_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_107 = ttnn.reshape(ttnn_from_torch_87, (1, 120, 1, 1), )
  ttnn_from_device_163 = ttnn.from_device(ttnn_reshape_107, )
  ttnn_to_layout_195 = ttnn.to_layout(ttnn_from_device_163, ttnn.TILE_LAYOUT, )
  ttnn_to_device_107 = ttnn.to_device(ttnn_to_layout_195, device = device)
  ttnn_add_34 = ttnn.add(ttnn_multiply_33, ttnn_to_device_107, )
  ttnn_prefix_pack_to_tuple_15 = pack_to_tuple_wrapper(ttnn_add_34, )
  ttnn_prefix_getitem_15 = ttnn_prefix_pack_to_tuple_15[0]
  ttnn_relu_11 = ttnn.relu(ttnn_prefix_getitem_15, )
  ttnn_permute_52 = ttnn.permute(ttnn_relu_11, (0, 2, 3, 1), )
  ttnn_from_device_164 = ttnn.from_device(ttnn_permute_52, )
  ttnn_to_layout_196 = ttnn.to_layout(ttnn_from_device_164, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_108 = ttnn.reshape(ttnn_to_layout_196, (1, 1, 1600, 120), )
  ttnn_from_device_165 = ttnn.from_device(ttnn_reshape_108, )
  ttnn_to_layout_197 = ttnn.to_layout(ttnn_from_device_165, ttnn.TILE_LAYOUT, )
  ttnn_to_device_108 = ttnn.to_device(ttnn_to_layout_197, device = device)
  ttnn_from_torch_88 = ttnn.from_torch(arg56_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_20 = conv_wrapper(ttnn_to_device_108, ttnn_from_torch_88, None, 1, 120, 120, [40, 40], [5, 5], [1, 1], [2, 2], [1, 1], 120, device, False, None, )
  ttnn_sharded_to_interleaved_20 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_20, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_166 = ttnn.from_device(ttnn_sharded_to_interleaved_20, )
  ttnn_to_layout_198 = ttnn.to_layout(ttnn_from_device_166, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_109 = ttnn.reshape(ttnn_to_layout_198, (1, 40, 40, 120), )
  ttnn_from_device_167 = ttnn.from_device(ttnn_reshape_109, )
  ttnn_to_layout_199 = ttnn.to_layout(ttnn_from_device_167, ttnn.TILE_LAYOUT, )
  ttnn_to_device_109 = ttnn.to_device(ttnn_to_layout_199, device = device)
  ttnn_permute_53 = ttnn.permute(ttnn_to_device_109, (0, 3, 1, 2), )
  test_accuracy(convolution_20, ttnn_permute_53)
  ttnn_from_torch_89 = ttnn.from_torch(arg321_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_35 = ttnn.add(ttnn_from_torch_89, 0.001, )
  ttnn_rsqrt_16 = ttnn.rsqrt(ttnn_add_35, )
  ttnn_from_device_168 = ttnn.from_device(ttnn_rsqrt_16, )
  ttnn_to_layout_200 = ttnn.to_layout(ttnn_from_device_168, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_110 = ttnn.reshape(ttnn_to_layout_200, (1, 120, 1, 1), )
  ttnn_from_torch_90 = ttnn.from_torch(arg320_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_111 = ttnn.reshape(ttnn_from_torch_90, (1, 120, 1, 1), )
  ttnn_from_device_169 = ttnn.from_device(ttnn_reshape_111, )
  ttnn_to_layout_201 = ttnn.to_layout(ttnn_from_device_169, ttnn.TILE_LAYOUT, )
  ttnn_to_device_110 = ttnn.to_device(ttnn_to_layout_201, device = device)
  ttnn_subtract_16 = ttnn.subtract(ttnn_permute_53, ttnn_to_device_110, )
  ttnn_from_device_170 = ttnn.from_device(ttnn_reshape_110, )
  ttnn_to_layout_202 = ttnn.to_layout(ttnn_from_device_170, ttnn.TILE_LAYOUT, )
  ttnn_to_device_111 = ttnn.to_device(ttnn_to_layout_202, device = device)
  ttnn_multiply_34 = ttnn.multiply(ttnn_subtract_16, ttnn_to_device_111, )
  ttnn_from_torch_91 = ttnn.from_torch(arg57_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_112 = ttnn.reshape(ttnn_from_torch_91, (1, 120, 1, 1), )
  ttnn_from_device_171 = ttnn.from_device(ttnn_reshape_112, )
  ttnn_to_layout_203 = ttnn.to_layout(ttnn_from_device_171, ttnn.TILE_LAYOUT, )
  ttnn_to_device_112 = ttnn.to_device(ttnn_to_layout_203, device = device)
  ttnn_multiply_35 = ttnn.multiply(ttnn_multiply_34, ttnn_to_device_112, )
  ttnn_from_torch_92 = ttnn.from_torch(arg58_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_113 = ttnn.reshape(ttnn_from_torch_92, (1, 120, 1, 1), )
  ttnn_from_device_172 = ttnn.from_device(ttnn_reshape_113, )
  ttnn_to_layout_204 = ttnn.to_layout(ttnn_from_device_172, ttnn.TILE_LAYOUT, )
  ttnn_to_device_113 = ttnn.to_device(ttnn_to_layout_204, device = device)
  ttnn_add_36 = ttnn.add(ttnn_multiply_35, ttnn_to_device_113, )
  ttnn_prefix_pack_to_tuple_16 = pack_to_tuple_wrapper(ttnn_add_36, )
  ttnn_prefix_getitem_16 = ttnn_prefix_pack_to_tuple_16[0]
  test_accuracy(getitem_48, ttnn_prefix_getitem_16)
  ttnn_relu_12 = ttnn.relu(ttnn_prefix_getitem_16, )
  test_accuracy(relu_12, ttnn_relu_12)
  ttnn_mean_2 = ttnn.mean(ttnn_relu_12, (-1, -2), )
  test_accuracy(mean_2, ttnn_mean_2)
  ttnn_permute_54 = ttnn.permute(ttnn_mean_2, (0, 2, 3, 1), )
  ttnn_from_device_173 = ttnn.from_device(ttnn_permute_54, )
  ttnn_to_layout_205 = ttnn.to_layout(ttnn_from_device_173, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_114 = ttnn.reshape(ttnn_to_layout_205, (1, 1, 1, 120), )
  ttnn_from_torch_93 = ttnn.from_torch(arg60_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_115 = ttnn.reshape(ttnn_from_torch_93, (1, 1, 1, 32), )
  ttnn_from_device_174 = ttnn.from_device(ttnn_reshape_115, )
  ttnn_to_layout_206 = ttnn.to_layout(ttnn_from_device_174, ttnn.TILE_LAYOUT, )
  ttnn_to_device_114 = ttnn.to_device(ttnn_to_layout_206, device = device)
  ttnn_prefix_move_to_host_4 = move_to_host_wrapper(ttnn_to_device_114, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_175 = ttnn.from_device(ttnn_reshape_114, )
  ttnn_to_layout_207 = ttnn.to_layout(ttnn_from_device_175, ttnn.TILE_LAYOUT, )
  ttnn_to_device_115 = ttnn.to_device(ttnn_to_layout_207, device = device)
  ttnn_from_torch_94 = ttnn.from_torch(arg59_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_21 = conv_wrapper(ttnn_to_device_115, ttnn_from_torch_94, ttnn_prefix_move_to_host_4, 1, 120, 32, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_21 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_21, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_176 = ttnn.from_device(ttnn_sharded_to_interleaved_21, )
  ttnn_to_layout_208 = ttnn.to_layout(ttnn_from_device_176, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_116 = ttnn.reshape(ttnn_to_layout_208, (1, 1, 1, 32), )
  ttnn_from_device_177 = ttnn.from_device(ttnn_reshape_116, )
  ttnn_to_layout_209 = ttnn.to_layout(ttnn_from_device_177, ttnn.TILE_LAYOUT, )
  ttnn_to_device_116 = ttnn.to_device(ttnn_to_layout_209, device = device)
  ttnn_permute_55 = ttnn.permute(ttnn_to_device_116, (0, 3, 1, 2), )
  test_accuracy(convolution_21, ttnn_permute_55)
  ttnn_relu_13 = ttnn.relu(ttnn_permute_55, )
  test_accuracy(relu_13, ttnn_relu_13)
  ttnn_permute_56 = ttnn.permute(ttnn_relu_13, (0, 2, 3, 1), )
  ttnn_from_device_178 = ttnn.from_device(ttnn_permute_56, )
  ttnn_to_layout_210 = ttnn.to_layout(ttnn_from_device_178, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_117 = ttnn.reshape(ttnn_to_layout_210, (1, 1, 1, 32), )
  ttnn_from_torch_95 = ttnn.from_torch(arg62_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_118 = ttnn.reshape(ttnn_from_torch_95, (1, 1, 1, 120), )
  ttnn_from_device_179 = ttnn.from_device(ttnn_reshape_118, )
  ttnn_to_layout_211 = ttnn.to_layout(ttnn_from_device_179, ttnn.TILE_LAYOUT, )
  ttnn_to_device_117 = ttnn.to_device(ttnn_to_layout_211, device = device)
  ttnn_prefix_move_to_host_5 = move_to_host_wrapper(ttnn_to_device_117, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_180 = ttnn.from_device(ttnn_reshape_117, )
  ttnn_to_layout_212 = ttnn.to_layout(ttnn_from_device_180, ttnn.TILE_LAYOUT, )
  ttnn_to_device_118 = ttnn.to_device(ttnn_to_layout_212, device = device)
  ttnn_from_torch_96 = ttnn.from_torch(arg61_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_22 = conv_wrapper(ttnn_to_device_118, ttnn_from_torch_96, ttnn_prefix_move_to_host_5, 1, 32, 120, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_22 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_22, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_181 = ttnn.from_device(ttnn_sharded_to_interleaved_22, )
  ttnn_to_layout_213 = ttnn.to_layout(ttnn_from_device_181, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_119 = ttnn.reshape(ttnn_to_layout_213, (1, 1, 1, 120), )
  ttnn_from_device_182 = ttnn.from_device(ttnn_reshape_119, )
  ttnn_to_layout_214 = ttnn.to_layout(ttnn_from_device_182, ttnn.TILE_LAYOUT, )
  ttnn_to_device_119 = ttnn.to_device(ttnn_to_layout_214, device = device)
  ttnn_permute_57 = ttnn.permute(ttnn_to_device_119, (0, 3, 1, 2), )
  ttnn_hardsigmoid_2 = ttnn.hardsigmoid(ttnn_permute_57, )
  test_accuracy(hardsigmoid_2, ttnn_hardsigmoid_2)
  ttnn_multiply_36 = ttnn.multiply(ttnn_hardsigmoid_2, ttnn_relu_12, )
  test_accuracy(mul_10, ttnn_multiply_36)
  ttnn_permute_58 = ttnn.permute(ttnn_multiply_36, (0, 2, 3, 1), )
  ttnn_from_device_183 = ttnn.from_device(ttnn_permute_58, )
  ttnn_to_layout_215 = ttnn.to_layout(ttnn_from_device_183, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_120 = ttnn.reshape(ttnn_to_layout_215, (1, 1, 1600, 120), )
  ttnn_from_device_184 = ttnn.from_device(ttnn_reshape_120, )
  ttnn_to_layout_216 = ttnn.to_layout(ttnn_from_device_184, ttnn.TILE_LAYOUT, )
  ttnn_to_device_120 = ttnn.to_device(ttnn_to_layout_216, device = device)
  ttnn_from_torch_97 = ttnn.from_torch(arg63_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_23 = conv_wrapper(ttnn_to_device_120, ttnn_from_torch_97, None, 1, 120, 40, [40, 40], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_23 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_23, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_185 = ttnn.from_device(ttnn_sharded_to_interleaved_23, )
  ttnn_to_layout_217 = ttnn.to_layout(ttnn_from_device_185, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_121 = ttnn.reshape(ttnn_to_layout_217, (1, 40, 40, 40), )
  ttnn_from_device_186 = ttnn.from_device(ttnn_reshape_121, )
  ttnn_to_layout_218 = ttnn.to_layout(ttnn_from_device_186, ttnn.TILE_LAYOUT, )
  ttnn_to_device_121 = ttnn.to_device(ttnn_to_layout_218, device = device)
  ttnn_permute_59 = ttnn.permute(ttnn_to_device_121, (0, 3, 1, 2), )
  test_accuracy(convolution_23, ttnn_permute_59)
  ttnn_from_torch_98 = ttnn.from_torch(arg324_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_37 = ttnn.add(ttnn_from_torch_98, 0.001, )
  ttnn_rsqrt_17 = ttnn.rsqrt(ttnn_add_37, )
  ttnn_from_device_187 = ttnn.from_device(ttnn_rsqrt_17, )
  ttnn_to_layout_219 = ttnn.to_layout(ttnn_from_device_187, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_122 = ttnn.reshape(ttnn_to_layout_219, (1, 40, 1, 1), )
  ttnn_from_torch_99 = ttnn.from_torch(arg323_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_123 = ttnn.reshape(ttnn_from_torch_99, (1, 40, 1, 1), )
  ttnn_from_device_188 = ttnn.from_device(ttnn_reshape_123, )
  ttnn_to_layout_220 = ttnn.to_layout(ttnn_from_device_188, ttnn.TILE_LAYOUT, )
  ttnn_to_device_122 = ttnn.to_device(ttnn_to_layout_220, device = device)
  ttnn_subtract_17 = ttnn.subtract(ttnn_permute_59, ttnn_to_device_122, )
  ttnn_from_device_189 = ttnn.from_device(ttnn_reshape_122, )
  ttnn_to_layout_221 = ttnn.to_layout(ttnn_from_device_189, ttnn.TILE_LAYOUT, )
  ttnn_to_device_123 = ttnn.to_device(ttnn_to_layout_221, device = device)
  ttnn_multiply_37 = ttnn.multiply(ttnn_subtract_17, ttnn_to_device_123, )
  ttnn_from_torch_100 = ttnn.from_torch(arg64_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_124 = ttnn.reshape(ttnn_from_torch_100, (1, 40, 1, 1), )
  ttnn_from_device_190 = ttnn.from_device(ttnn_reshape_124, )
  ttnn_to_layout_222 = ttnn.to_layout(ttnn_from_device_190, ttnn.TILE_LAYOUT, )
  ttnn_to_device_124 = ttnn.to_device(ttnn_to_layout_222, device = device)
  ttnn_multiply_38 = ttnn.multiply(ttnn_multiply_37, ttnn_to_device_124, )
  ttnn_from_torch_101 = ttnn.from_torch(arg65_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_125 = ttnn.reshape(ttnn_from_torch_101, (1, 40, 1, 1), )
  ttnn_from_device_191 = ttnn.from_device(ttnn_reshape_125, )
  ttnn_to_layout_223 = ttnn.to_layout(ttnn_from_device_191, ttnn.TILE_LAYOUT, )
  ttnn_to_device_125 = ttnn.to_device(ttnn_to_layout_223, device = device)
  ttnn_add_38 = ttnn.add(ttnn_multiply_38, ttnn_to_device_125, )
  ttnn_prefix_pack_to_tuple_17 = pack_to_tuple_wrapper(ttnn_add_38, )
  ttnn_prefix_getitem_17 = ttnn_prefix_pack_to_tuple_17[0]
  test_accuracy(getitem_51, ttnn_prefix_getitem_17)
  ttnn_add_39 = ttnn.add(ttnn_prefix_getitem_17, ttnn_add_32, )
  test_accuracy(add_8, ttnn_add_39)
  ttnn_permute_60 = ttnn.permute(ttnn_add_39, (0, 2, 3, 1), )
  ttnn_from_device_192 = ttnn.from_device(ttnn_permute_60, )
  ttnn_to_layout_224 = ttnn.to_layout(ttnn_from_device_192, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_126 = ttnn.reshape(ttnn_to_layout_224, (1, 1, 1600, 40), )
  ttnn_from_device_193 = ttnn.from_device(ttnn_reshape_126, )
  ttnn_to_layout_225 = ttnn.to_layout(ttnn_from_device_193, ttnn.TILE_LAYOUT, )
  ttnn_to_device_126 = ttnn.to_device(ttnn_to_layout_225, device = device)
  ttnn_from_torch_102 = ttnn.from_torch(arg66_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_24 = conv_wrapper(ttnn_to_device_126, ttnn_from_torch_102, None, 1, 40, 240, [40, 40], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_24 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_24, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_194 = ttnn.from_device(ttnn_sharded_to_interleaved_24, )
  ttnn_to_layout_226 = ttnn.to_layout(ttnn_from_device_194, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_127 = ttnn.reshape(ttnn_to_layout_226, (1, 40, 40, 240), )
  ttnn_from_device_195 = ttnn.from_device(ttnn_reshape_127, )
  ttnn_to_layout_227 = ttnn.to_layout(ttnn_from_device_195, ttnn.TILE_LAYOUT, )
  ttnn_to_device_127 = ttnn.to_device(ttnn_to_layout_227, device = device)
  ttnn_permute_61 = ttnn.permute(ttnn_to_device_127, (0, 3, 1, 2), )
  test_accuracy(convolution_24, ttnn_permute_61)
  ttnn_from_torch_103 = ttnn.from_torch(arg327_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_40 = ttnn.add(ttnn_from_torch_103, 0.001, )
  ttnn_rsqrt_18 = ttnn.rsqrt(ttnn_add_40, )
  ttnn_from_device_196 = ttnn.from_device(ttnn_rsqrt_18, )
  ttnn_to_layout_228 = ttnn.to_layout(ttnn_from_device_196, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_128 = ttnn.reshape(ttnn_to_layout_228, (1, 240, 1, 1), )
  ttnn_from_torch_104 = ttnn.from_torch(arg326_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_129 = ttnn.reshape(ttnn_from_torch_104, (1, 240, 1, 1), )
  ttnn_from_device_197 = ttnn.from_device(ttnn_reshape_129, )
  ttnn_to_layout_229 = ttnn.to_layout(ttnn_from_device_197, ttnn.TILE_LAYOUT, )
  ttnn_to_device_128 = ttnn.to_device(ttnn_to_layout_229, device = device)
  ttnn_subtract_18 = ttnn.subtract(ttnn_permute_61, ttnn_to_device_128, )
  ttnn_from_device_198 = ttnn.from_device(ttnn_reshape_128, )
  ttnn_to_layout_230 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_129 = ttnn.to_device(ttnn_to_layout_230, device = device)
  ttnn_multiply_39 = ttnn.multiply(ttnn_subtract_18, ttnn_to_device_129, )
  ttnn_from_torch_105 = ttnn.from_torch(arg67_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_130 = ttnn.reshape(ttnn_from_torch_105, (1, 240, 1, 1), )
  ttnn_from_device_199 = ttnn.from_device(ttnn_reshape_130, )
  ttnn_to_layout_231 = ttnn.to_layout(ttnn_from_device_199, ttnn.TILE_LAYOUT, )
  ttnn_to_device_130 = ttnn.to_device(ttnn_to_layout_231, device = device)
  ttnn_multiply_40 = ttnn.multiply(ttnn_multiply_39, ttnn_to_device_130, )
  ttnn_from_torch_106 = ttnn.from_torch(arg68_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_131 = ttnn.reshape(ttnn_from_torch_106, (1, 240, 1, 1), )
  ttnn_from_device_200 = ttnn.from_device(ttnn_reshape_131, )
  ttnn_to_layout_232 = ttnn.to_layout(ttnn_from_device_200, ttnn.TILE_LAYOUT, )
  ttnn_to_device_131 = ttnn.to_device(ttnn_to_layout_232, device = device)
  ttnn_add_41 = ttnn.add(ttnn_multiply_40, ttnn_to_device_131, )
  ttnn_prefix_pack_to_tuple_18 = pack_to_tuple_wrapper(ttnn_add_41, )
  ttnn_prefix_getitem_18 = ttnn_prefix_pack_to_tuple_18[0]
  test_accuracy(getitem_54, ttnn_prefix_getitem_18)
  ttnn_hardswish_1 = ttnn.hardswish(ttnn_prefix_getitem_18, )
  test_accuracy(hardswish_1, ttnn_hardswish_1)
  ttnn_permute_62 = ttnn.permute(ttnn_hardswish_1, (0, 2, 3, 1), )
  ttnn_from_device_201 = ttnn.from_device(ttnn_permute_62, )
  ttnn_to_layout_233 = ttnn.to_layout(ttnn_from_device_201, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_132 = ttnn.reshape(ttnn_to_layout_233, (1, 1, 1600, 240), )
  ttnn_from_device_202 = ttnn.from_device(ttnn_reshape_132, )
  ttnn_to_layout_234 = ttnn.to_layout(ttnn_from_device_202, ttnn.TILE_LAYOUT, )
  ttnn_to_device_132 = ttnn.to_device(ttnn_to_layout_234, device = device)
  ttnn_from_torch_107 = ttnn.from_torch(arg69_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_25 = conv_wrapper(ttnn_to_device_132, ttnn_from_torch_107, None, 1, 240, 240, [40, 40], [3, 3], [2, 2], [1, 1], [1, 1], 240, device, False, None, )
  ttnn_sharded_to_interleaved_25 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_25, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_203 = ttnn.from_device(ttnn_sharded_to_interleaved_25, )
  ttnn_to_layout_235 = ttnn.to_layout(ttnn_from_device_203, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_133 = ttnn.reshape(ttnn_to_layout_235, (1, 20, 20, 240), )
  ttnn_from_device_204 = ttnn.from_device(ttnn_reshape_133, )
  ttnn_to_layout_236 = ttnn.to_layout(ttnn_from_device_204, ttnn.TILE_LAYOUT, )
  ttnn_to_device_133 = ttnn.to_device(ttnn_to_layout_236, device = device)
  ttnn_permute_63 = ttnn.permute(ttnn_to_device_133, (0, 3, 1, 2), )
  test_accuracy(convolution_25, ttnn_permute_63)
  ttnn_from_torch_108 = ttnn.from_torch(arg330_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_42 = ttnn.add(ttnn_from_torch_108, 0.001, )
  ttnn_rsqrt_19 = ttnn.rsqrt(ttnn_add_42, )
  ttnn_from_device_205 = ttnn.from_device(ttnn_rsqrt_19, )
  ttnn_to_layout_237 = ttnn.to_layout(ttnn_from_device_205, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_134 = ttnn.reshape(ttnn_to_layout_237, (1, 240, 1, 1), )
  ttnn_from_torch_109 = ttnn.from_torch(arg329_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_135 = ttnn.reshape(ttnn_from_torch_109, (1, 240, 1, 1), )
  ttnn_from_device_206 = ttnn.from_device(ttnn_reshape_135, )
  ttnn_to_layout_238 = ttnn.to_layout(ttnn_from_device_206, ttnn.TILE_LAYOUT, )
  ttnn_to_device_134 = ttnn.to_device(ttnn_to_layout_238, device = device)
  ttnn_subtract_19 = ttnn.subtract(ttnn_permute_63, ttnn_to_device_134, )
  ttnn_from_device_207 = ttnn.from_device(ttnn_reshape_134, )
  ttnn_to_layout_239 = ttnn.to_layout(ttnn_from_device_207, ttnn.TILE_LAYOUT, )
  ttnn_to_device_135 = ttnn.to_device(ttnn_to_layout_239, device = device)
  ttnn_multiply_41 = ttnn.multiply(ttnn_subtract_19, ttnn_to_device_135, )
  ttnn_from_torch_110 = ttnn.from_torch(arg70_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_136 = ttnn.reshape(ttnn_from_torch_110, (1, 240, 1, 1), )
  ttnn_from_device_208 = ttnn.from_device(ttnn_reshape_136, )
  ttnn_to_layout_240 = ttnn.to_layout(ttnn_from_device_208, ttnn.TILE_LAYOUT, )
  ttnn_to_device_136 = ttnn.to_device(ttnn_to_layout_240, device = device)
  ttnn_multiply_42 = ttnn.multiply(ttnn_multiply_41, ttnn_to_device_136, )
  ttnn_from_torch_111 = ttnn.from_torch(arg71_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_137 = ttnn.reshape(ttnn_from_torch_111, (1, 240, 1, 1), )
  ttnn_from_device_209 = ttnn.from_device(ttnn_reshape_137, )
  ttnn_to_layout_241 = ttnn.to_layout(ttnn_from_device_209, ttnn.TILE_LAYOUT, )
  ttnn_to_device_137 = ttnn.to_device(ttnn_to_layout_241, device = device)
  ttnn_add_43 = ttnn.add(ttnn_multiply_42, ttnn_to_device_137, )
  ttnn_prefix_pack_to_tuple_19 = pack_to_tuple_wrapper(ttnn_add_43, )
  ttnn_prefix_getitem_19 = ttnn_prefix_pack_to_tuple_19[0]
  test_accuracy(getitem_57, ttnn_prefix_getitem_19)
  ttnn_hardswish_2 = ttnn.hardswish(ttnn_prefix_getitem_19, )
  test_accuracy(hardswish_2, ttnn_hardswish_2)
  ttnn_permute_64 = ttnn.permute(ttnn_hardswish_2, (0, 2, 3, 1), )
  ttnn_from_device_210 = ttnn.from_device(ttnn_permute_64, )
  ttnn_to_layout_242 = ttnn.to_layout(ttnn_from_device_210, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_138 = ttnn.reshape(ttnn_to_layout_242, (1, 1, 400, 240), )
  ttnn_from_device_211 = ttnn.from_device(ttnn_reshape_138, )
  ttnn_to_layout_243 = ttnn.to_layout(ttnn_from_device_211, ttnn.TILE_LAYOUT, )
  ttnn_to_device_138 = ttnn.to_device(ttnn_to_layout_243, device = device)
  ttnn_from_torch_112 = ttnn.from_torch(arg72_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_26 = conv_wrapper(ttnn_to_device_138, ttnn_from_torch_112, None, 1, 240, 80, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_26 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_26, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_212 = ttnn.from_device(ttnn_sharded_to_interleaved_26, )
  ttnn_to_layout_244 = ttnn.to_layout(ttnn_from_device_212, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_139 = ttnn.reshape(ttnn_to_layout_244, (1, 20, 20, 80), )
  ttnn_from_device_213 = ttnn.from_device(ttnn_reshape_139, )
  ttnn_to_layout_245 = ttnn.to_layout(ttnn_from_device_213, ttnn.TILE_LAYOUT, )
  ttnn_to_device_139 = ttnn.to_device(ttnn_to_layout_245, device = device)
  ttnn_permute_65 = ttnn.permute(ttnn_to_device_139, (0, 3, 1, 2), )
  ttnn_from_torch_113 = ttnn.from_torch(arg333_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_44 = ttnn.add(ttnn_from_torch_113, 0.001, )
  ttnn_rsqrt_20 = ttnn.rsqrt(ttnn_add_44, )
  ttnn_from_device_214 = ttnn.from_device(ttnn_rsqrt_20, )
  ttnn_to_layout_246 = ttnn.to_layout(ttnn_from_device_214, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_140 = ttnn.reshape(ttnn_to_layout_246, (1, 80, 1, 1), )
  ttnn_from_torch_114 = ttnn.from_torch(arg332_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_141 = ttnn.reshape(ttnn_from_torch_114, (1, 80, 1, 1), )
  ttnn_from_device_215 = ttnn.from_device(ttnn_reshape_141, )
  ttnn_to_layout_247 = ttnn.to_layout(ttnn_from_device_215, ttnn.TILE_LAYOUT, )
  ttnn_to_device_140 = ttnn.to_device(ttnn_to_layout_247, device = device)
  ttnn_subtract_20 = ttnn.subtract(ttnn_permute_65, ttnn_to_device_140, )
  ttnn_from_device_216 = ttnn.from_device(ttnn_reshape_140, )
  ttnn_to_layout_248 = ttnn.to_layout(ttnn_from_device_216, ttnn.TILE_LAYOUT, )
  ttnn_to_device_141 = ttnn.to_device(ttnn_to_layout_248, device = device)
  ttnn_multiply_43 = ttnn.multiply(ttnn_subtract_20, ttnn_to_device_141, )
  ttnn_from_torch_115 = ttnn.from_torch(arg73_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_142 = ttnn.reshape(ttnn_from_torch_115, (1, 80, 1, 1), )
  ttnn_from_device_217 = ttnn.from_device(ttnn_reshape_142, )
  ttnn_to_layout_249 = ttnn.to_layout(ttnn_from_device_217, ttnn.TILE_LAYOUT, )
  ttnn_to_device_142 = ttnn.to_device(ttnn_to_layout_249, device = device)
  ttnn_multiply_44 = ttnn.multiply(ttnn_multiply_43, ttnn_to_device_142, )
  ttnn_from_torch_116 = ttnn.from_torch(arg74_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_143 = ttnn.reshape(ttnn_from_torch_116, (1, 80, 1, 1), )
  ttnn_from_device_218 = ttnn.from_device(ttnn_reshape_143, )
  ttnn_to_layout_250 = ttnn.to_layout(ttnn_from_device_218, ttnn.TILE_LAYOUT, )
  ttnn_to_device_143 = ttnn.to_device(ttnn_to_layout_250, device = device)
  ttnn_add_45 = ttnn.add(ttnn_multiply_44, ttnn_to_device_143, )
  ttnn_prefix_pack_to_tuple_20 = pack_to_tuple_wrapper(ttnn_add_45, )
  ttnn_prefix_getitem_20 = ttnn_prefix_pack_to_tuple_20[0]
  ttnn_permute_66 = ttnn.permute(ttnn_prefix_getitem_20, (0, 2, 3, 1), )
  ttnn_from_device_219 = ttnn.from_device(ttnn_permute_66, )
  ttnn_to_layout_251 = ttnn.to_layout(ttnn_from_device_219, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_144 = ttnn.reshape(ttnn_to_layout_251, (1, 1, 400, 80), )
  ttnn_from_device_220 = ttnn.from_device(ttnn_reshape_144, )
  ttnn_to_layout_252 = ttnn.to_layout(ttnn_from_device_220, ttnn.TILE_LAYOUT, )
  ttnn_to_device_144 = ttnn.to_device(ttnn_to_layout_252, device = device)
  ttnn_from_torch_117 = ttnn.from_torch(arg75_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_27 = conv_wrapper(ttnn_to_device_144, ttnn_from_torch_117, None, 1, 80, 200, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_27 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_27, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_221 = ttnn.from_device(ttnn_sharded_to_interleaved_27, )
  ttnn_to_layout_253 = ttnn.to_layout(ttnn_from_device_221, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_145 = ttnn.reshape(ttnn_to_layout_253, (1, 20, 20, 200), )
  ttnn_from_device_222 = ttnn.from_device(ttnn_reshape_145, )
  ttnn_to_layout_254 = ttnn.to_layout(ttnn_from_device_222, ttnn.TILE_LAYOUT, )
  ttnn_to_device_145 = ttnn.to_device(ttnn_to_layout_254, device = device)
  ttnn_permute_67 = ttnn.permute(ttnn_to_device_145, (0, 3, 1, 2), )
  ttnn_from_torch_118 = ttnn.from_torch(arg336_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_46 = ttnn.add(ttnn_from_torch_118, 0.001, )
  ttnn_rsqrt_21 = ttnn.rsqrt(ttnn_add_46, )
  ttnn_from_device_223 = ttnn.from_device(ttnn_rsqrt_21, )
  ttnn_to_layout_255 = ttnn.to_layout(ttnn_from_device_223, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_146 = ttnn.reshape(ttnn_to_layout_255, (1, 200, 1, 1), )
  ttnn_from_torch_119 = ttnn.from_torch(arg335_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_147 = ttnn.reshape(ttnn_from_torch_119, (1, 200, 1, 1), )
  ttnn_from_device_224 = ttnn.from_device(ttnn_reshape_147, )
  ttnn_to_layout_256 = ttnn.to_layout(ttnn_from_device_224, ttnn.TILE_LAYOUT, )
  ttnn_to_device_146 = ttnn.to_device(ttnn_to_layout_256, device = device)
  ttnn_subtract_21 = ttnn.subtract(ttnn_permute_67, ttnn_to_device_146, )
  ttnn_from_device_225 = ttnn.from_device(ttnn_reshape_146, )
  ttnn_to_layout_257 = ttnn.to_layout(ttnn_from_device_225, ttnn.TILE_LAYOUT, )
  ttnn_to_device_147 = ttnn.to_device(ttnn_to_layout_257, device = device)
  ttnn_multiply_45 = ttnn.multiply(ttnn_subtract_21, ttnn_to_device_147, )
  ttnn_from_torch_120 = ttnn.from_torch(arg76_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_148 = ttnn.reshape(ttnn_from_torch_120, (1, 200, 1, 1), )
  ttnn_from_device_226 = ttnn.from_device(ttnn_reshape_148, )
  ttnn_to_layout_258 = ttnn.to_layout(ttnn_from_device_226, ttnn.TILE_LAYOUT, )
  ttnn_to_device_148 = ttnn.to_device(ttnn_to_layout_258, device = device)
  ttnn_multiply_46 = ttnn.multiply(ttnn_multiply_45, ttnn_to_device_148, )
  ttnn_from_torch_121 = ttnn.from_torch(arg77_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_149 = ttnn.reshape(ttnn_from_torch_121, (1, 200, 1, 1), )
  ttnn_from_device_227 = ttnn.from_device(ttnn_reshape_149, )
  ttnn_to_layout_259 = ttnn.to_layout(ttnn_from_device_227, ttnn.TILE_LAYOUT, )
  ttnn_to_device_149 = ttnn.to_device(ttnn_to_layout_259, device = device)
  ttnn_add_47 = ttnn.add(ttnn_multiply_46, ttnn_to_device_149, )
  ttnn_prefix_pack_to_tuple_21 = pack_to_tuple_wrapper(ttnn_add_47, )
  ttnn_prefix_getitem_21 = ttnn_prefix_pack_to_tuple_21[0]
  ttnn_hardswish_3 = ttnn.hardswish(ttnn_prefix_getitem_21, )
  ttnn_permute_68 = ttnn.permute(ttnn_hardswish_3, (0, 2, 3, 1), )
  ttnn_from_device_228 = ttnn.from_device(ttnn_permute_68, )
  ttnn_to_layout_260 = ttnn.to_layout(ttnn_from_device_228, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_150 = ttnn.reshape(ttnn_to_layout_260, (1, 1, 400, 200), )
  ttnn_from_device_229 = ttnn.from_device(ttnn_reshape_150, )
  ttnn_to_layout_261 = ttnn.to_layout(ttnn_from_device_229, ttnn.TILE_LAYOUT, )
  ttnn_to_device_150 = ttnn.to_device(ttnn_to_layout_261, device = device)
  ttnn_from_torch_122 = ttnn.from_torch(arg78_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_28 = conv_wrapper(ttnn_to_device_150, ttnn_from_torch_122, None, 1, 200, 200, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 200, device, False, None, )
  ttnn_sharded_to_interleaved_28 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_28, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_230 = ttnn.from_device(ttnn_sharded_to_interleaved_28, )
  ttnn_to_layout_262 = ttnn.to_layout(ttnn_from_device_230, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_151 = ttnn.reshape(ttnn_to_layout_262, (1, 20, 20, 200), )
  ttnn_from_device_231 = ttnn.from_device(ttnn_reshape_151, )
  ttnn_to_layout_263 = ttnn.to_layout(ttnn_from_device_231, ttnn.TILE_LAYOUT, )
  ttnn_to_device_151 = ttnn.to_device(ttnn_to_layout_263, device = device)
  ttnn_permute_69 = ttnn.permute(ttnn_to_device_151, (0, 3, 1, 2), )
  test_accuracy(convolution_28, ttnn_permute_69)
  ttnn_from_torch_123 = ttnn.from_torch(arg339_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_48 = ttnn.add(ttnn_from_torch_123, 0.001, )
  ttnn_rsqrt_22 = ttnn.rsqrt(ttnn_add_48, )
  ttnn_from_device_232 = ttnn.from_device(ttnn_rsqrt_22, )
  ttnn_to_layout_264 = ttnn.to_layout(ttnn_from_device_232, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_152 = ttnn.reshape(ttnn_to_layout_264, (1, 200, 1, 1), )
  ttnn_from_torch_124 = ttnn.from_torch(arg338_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_153 = ttnn.reshape(ttnn_from_torch_124, (1, 200, 1, 1), )
  ttnn_from_device_233 = ttnn.from_device(ttnn_reshape_153, )
  ttnn_to_layout_265 = ttnn.to_layout(ttnn_from_device_233, ttnn.TILE_LAYOUT, )
  ttnn_to_device_152 = ttnn.to_device(ttnn_to_layout_265, device = device)
  ttnn_subtract_22 = ttnn.subtract(ttnn_permute_69, ttnn_to_device_152, )
  ttnn_from_device_234 = ttnn.from_device(ttnn_reshape_152, )
  ttnn_to_layout_266 = ttnn.to_layout(ttnn_from_device_234, ttnn.TILE_LAYOUT, )
  ttnn_to_device_153 = ttnn.to_device(ttnn_to_layout_266, device = device)
  ttnn_multiply_47 = ttnn.multiply(ttnn_subtract_22, ttnn_to_device_153, )
  ttnn_from_torch_125 = ttnn.from_torch(arg79_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_154 = ttnn.reshape(ttnn_from_torch_125, (1, 200, 1, 1), )
  ttnn_from_device_235 = ttnn.from_device(ttnn_reshape_154, )
  ttnn_to_layout_267 = ttnn.to_layout(ttnn_from_device_235, ttnn.TILE_LAYOUT, )
  ttnn_to_device_154 = ttnn.to_device(ttnn_to_layout_267, device = device)
  ttnn_multiply_48 = ttnn.multiply(ttnn_multiply_47, ttnn_to_device_154, )
  ttnn_from_torch_126 = ttnn.from_torch(arg80_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_155 = ttnn.reshape(ttnn_from_torch_126, (1, 200, 1, 1), )
  ttnn_from_device_236 = ttnn.from_device(ttnn_reshape_155, )
  ttnn_to_layout_268 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_155 = ttnn.to_device(ttnn_to_layout_268, device = device)
  ttnn_add_49 = ttnn.add(ttnn_multiply_48, ttnn_to_device_155, )
  ttnn_prefix_pack_to_tuple_22 = pack_to_tuple_wrapper(ttnn_add_49, )
  ttnn_prefix_getitem_22 = ttnn_prefix_pack_to_tuple_22[0]
  test_accuracy(getitem_66, ttnn_prefix_getitem_22)
  ttnn_hardswish_4 = ttnn.hardswish(ttnn_prefix_getitem_22, )
  test_accuracy(hardswish_4, ttnn_hardswish_4)
  ttnn_permute_70 = ttnn.permute(ttnn_hardswish_4, (0, 2, 3, 1), )
  ttnn_from_device_237 = ttnn.from_device(ttnn_permute_70, )
  ttnn_to_layout_269 = ttnn.to_layout(ttnn_from_device_237, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_156 = ttnn.reshape(ttnn_to_layout_269, (1, 1, 400, 200), )
  ttnn_from_device_238 = ttnn.from_device(ttnn_reshape_156, )
  ttnn_to_layout_270 = ttnn.to_layout(ttnn_from_device_238, ttnn.TILE_LAYOUT, )
  ttnn_to_device_156 = ttnn.to_device(ttnn_to_layout_270, device = device)
  ttnn_from_torch_127 = ttnn.from_torch(arg81_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_29 = conv_wrapper(ttnn_to_device_156, ttnn_from_torch_127, None, 1, 200, 80, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_29 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_29, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_239 = ttnn.from_device(ttnn_sharded_to_interleaved_29, )
  ttnn_to_layout_271 = ttnn.to_layout(ttnn_from_device_239, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_157 = ttnn.reshape(ttnn_to_layout_271, (1, 20, 20, 80), )
  ttnn_from_device_240 = ttnn.from_device(ttnn_reshape_157, )
  ttnn_to_layout_272 = ttnn.to_layout(ttnn_from_device_240, ttnn.TILE_LAYOUT, )
  ttnn_to_device_157 = ttnn.to_device(ttnn_to_layout_272, device = device)
  ttnn_permute_71 = ttnn.permute(ttnn_to_device_157, (0, 3, 1, 2), )
  ttnn_from_torch_128 = ttnn.from_torch(arg342_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_50 = ttnn.add(ttnn_from_torch_128, 0.001, )
  ttnn_rsqrt_23 = ttnn.rsqrt(ttnn_add_50, )
  ttnn_from_device_241 = ttnn.from_device(ttnn_rsqrt_23, )
  ttnn_to_layout_273 = ttnn.to_layout(ttnn_from_device_241, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_158 = ttnn.reshape(ttnn_to_layout_273, (1, 80, 1, 1), )
  ttnn_from_torch_129 = ttnn.from_torch(arg341_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_159 = ttnn.reshape(ttnn_from_torch_129, (1, 80, 1, 1), )
  ttnn_from_device_242 = ttnn.from_device(ttnn_reshape_159, )
  ttnn_to_layout_274 = ttnn.to_layout(ttnn_from_device_242, ttnn.TILE_LAYOUT, )
  ttnn_to_device_158 = ttnn.to_device(ttnn_to_layout_274, device = device)
  ttnn_subtract_23 = ttnn.subtract(ttnn_permute_71, ttnn_to_device_158, )
  ttnn_from_device_243 = ttnn.from_device(ttnn_reshape_158, )
  ttnn_to_layout_275 = ttnn.to_layout(ttnn_from_device_243, ttnn.TILE_LAYOUT, )
  ttnn_to_device_159 = ttnn.to_device(ttnn_to_layout_275, device = device)
  ttnn_multiply_49 = ttnn.multiply(ttnn_subtract_23, ttnn_to_device_159, )
  ttnn_from_torch_130 = ttnn.from_torch(arg82_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_160 = ttnn.reshape(ttnn_from_torch_130, (1, 80, 1, 1), )
  ttnn_from_device_244 = ttnn.from_device(ttnn_reshape_160, )
  ttnn_to_layout_276 = ttnn.to_layout(ttnn_from_device_244, ttnn.TILE_LAYOUT, )
  ttnn_to_device_160 = ttnn.to_device(ttnn_to_layout_276, device = device)
  ttnn_multiply_50 = ttnn.multiply(ttnn_multiply_49, ttnn_to_device_160, )
  ttnn_from_torch_131 = ttnn.from_torch(arg83_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_161 = ttnn.reshape(ttnn_from_torch_131, (1, 80, 1, 1), )
  ttnn_from_device_245 = ttnn.from_device(ttnn_reshape_161, )
  ttnn_to_layout_277 = ttnn.to_layout(ttnn_from_device_245, ttnn.TILE_LAYOUT, )
  ttnn_to_device_161 = ttnn.to_device(ttnn_to_layout_277, device = device)
  ttnn_add_51 = ttnn.add(ttnn_multiply_50, ttnn_to_device_161, )
  ttnn_prefix_pack_to_tuple_23 = pack_to_tuple_wrapper(ttnn_add_51, )
  ttnn_prefix_getitem_23 = ttnn_prefix_pack_to_tuple_23[0]
  ttnn_add_52 = ttnn.add(ttnn_prefix_getitem_23, ttnn_prefix_getitem_20, )
  ttnn_permute_72 = ttnn.permute(ttnn_add_52, (0, 2, 3, 1), )
  ttnn_from_device_246 = ttnn.from_device(ttnn_permute_72, )
  ttnn_to_layout_278 = ttnn.to_layout(ttnn_from_device_246, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_162 = ttnn.reshape(ttnn_to_layout_278, (1, 1, 400, 80), )
  ttnn_from_device_247 = ttnn.from_device(ttnn_reshape_162, )
  ttnn_to_layout_279 = ttnn.to_layout(ttnn_from_device_247, ttnn.TILE_LAYOUT, )
  ttnn_to_device_162 = ttnn.to_device(ttnn_to_layout_279, device = device)
  ttnn_from_torch_132 = ttnn.from_torch(arg84_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_30 = conv_wrapper(ttnn_to_device_162, ttnn_from_torch_132, None, 1, 80, 184, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_30 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_30, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_248 = ttnn.from_device(ttnn_sharded_to_interleaved_30, )
  ttnn_to_layout_280 = ttnn.to_layout(ttnn_from_device_248, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_163 = ttnn.reshape(ttnn_to_layout_280, (1, 20, 20, 184), )
  ttnn_from_device_249 = ttnn.from_device(ttnn_reshape_163, )
  ttnn_to_layout_281 = ttnn.to_layout(ttnn_from_device_249, ttnn.TILE_LAYOUT, )
  ttnn_to_device_163 = ttnn.to_device(ttnn_to_layout_281, device = device)
  ttnn_permute_73 = ttnn.permute(ttnn_to_device_163, (0, 3, 1, 2), )
  ttnn_from_torch_133 = ttnn.from_torch(arg345_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_53 = ttnn.add(ttnn_from_torch_133, 0.001, )
  ttnn_rsqrt_24 = ttnn.rsqrt(ttnn_add_53, )
  ttnn_from_device_250 = ttnn.from_device(ttnn_rsqrt_24, )
  ttnn_to_layout_282 = ttnn.to_layout(ttnn_from_device_250, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_164 = ttnn.reshape(ttnn_to_layout_282, (1, 184, 1, 1), )
  ttnn_from_torch_134 = ttnn.from_torch(arg344_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_165 = ttnn.reshape(ttnn_from_torch_134, (1, 184, 1, 1), )
  ttnn_from_device_251 = ttnn.from_device(ttnn_reshape_165, )
  ttnn_to_layout_283 = ttnn.to_layout(ttnn_from_device_251, ttnn.TILE_LAYOUT, )
  ttnn_to_device_164 = ttnn.to_device(ttnn_to_layout_283, device = device)
  ttnn_subtract_24 = ttnn.subtract(ttnn_permute_73, ttnn_to_device_164, )
  ttnn_from_device_252 = ttnn.from_device(ttnn_reshape_164, )
  ttnn_to_layout_284 = ttnn.to_layout(ttnn_from_device_252, ttnn.TILE_LAYOUT, )
  ttnn_to_device_165 = ttnn.to_device(ttnn_to_layout_284, device = device)
  ttnn_multiply_51 = ttnn.multiply(ttnn_subtract_24, ttnn_to_device_165, )
  ttnn_from_torch_135 = ttnn.from_torch(arg85_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_166 = ttnn.reshape(ttnn_from_torch_135, (1, 184, 1, 1), )
  ttnn_from_device_253 = ttnn.from_device(ttnn_reshape_166, )
  ttnn_to_layout_285 = ttnn.to_layout(ttnn_from_device_253, ttnn.TILE_LAYOUT, )
  ttnn_to_device_166 = ttnn.to_device(ttnn_to_layout_285, device = device)
  ttnn_multiply_52 = ttnn.multiply(ttnn_multiply_51, ttnn_to_device_166, )
  ttnn_from_torch_136 = ttnn.from_torch(arg86_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_167 = ttnn.reshape(ttnn_from_torch_136, (1, 184, 1, 1), )
  ttnn_from_device_254 = ttnn.from_device(ttnn_reshape_167, )
  ttnn_to_layout_286 = ttnn.to_layout(ttnn_from_device_254, ttnn.TILE_LAYOUT, )
  ttnn_to_device_167 = ttnn.to_device(ttnn_to_layout_286, device = device)
  ttnn_add_54 = ttnn.add(ttnn_multiply_52, ttnn_to_device_167, )
  ttnn_prefix_pack_to_tuple_24 = pack_to_tuple_wrapper(ttnn_add_54, )
  ttnn_prefix_getitem_24 = ttnn_prefix_pack_to_tuple_24[0]
  ttnn_hardswish_5 = ttnn.hardswish(ttnn_prefix_getitem_24, )
  ttnn_permute_74 = ttnn.permute(ttnn_hardswish_5, (0, 2, 3, 1), )
  ttnn_from_device_255 = ttnn.from_device(ttnn_permute_74, )
  ttnn_to_layout_287 = ttnn.to_layout(ttnn_from_device_255, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_168 = ttnn.reshape(ttnn_to_layout_287, (1, 1, 400, 184), )
  ttnn_from_device_256 = ttnn.from_device(ttnn_reshape_168, )
  ttnn_to_layout_288 = ttnn.to_layout(ttnn_from_device_256, ttnn.TILE_LAYOUT, )
  ttnn_to_device_168 = ttnn.to_device(ttnn_to_layout_288, device = device)
  ttnn_from_torch_137 = ttnn.from_torch(arg87_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_31 = conv_wrapper(ttnn_to_device_168, ttnn_from_torch_137, None, 1, 184, 184, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 184, device, False, None, )
  ttnn_sharded_to_interleaved_31 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_31, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_257 = ttnn.from_device(ttnn_sharded_to_interleaved_31, )
  ttnn_to_layout_289 = ttnn.to_layout(ttnn_from_device_257, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_169 = ttnn.reshape(ttnn_to_layout_289, (1, 20, 20, 184), )
  ttnn_from_device_258 = ttnn.from_device(ttnn_reshape_169, )
  ttnn_to_layout_290 = ttnn.to_layout(ttnn_from_device_258, ttnn.TILE_LAYOUT, )
  ttnn_to_device_169 = ttnn.to_device(ttnn_to_layout_290, device = device)
  ttnn_permute_75 = ttnn.permute(ttnn_to_device_169, (0, 3, 1, 2), )
  ttnn_from_torch_138 = ttnn.from_torch(arg348_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_55 = ttnn.add(ttnn_from_torch_138, 0.001, )
  ttnn_rsqrt_25 = ttnn.rsqrt(ttnn_add_55, )
  ttnn_from_device_259 = ttnn.from_device(ttnn_rsqrt_25, )
  ttnn_to_layout_291 = ttnn.to_layout(ttnn_from_device_259, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_170 = ttnn.reshape(ttnn_to_layout_291, (1, 184, 1, 1), )
  ttnn_from_torch_139 = ttnn.from_torch(arg347_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_171 = ttnn.reshape(ttnn_from_torch_139, (1, 184, 1, 1), )
  ttnn_from_device_260 = ttnn.from_device(ttnn_reshape_171, )
  ttnn_to_layout_292 = ttnn.to_layout(ttnn_from_device_260, ttnn.TILE_LAYOUT, )
  ttnn_to_device_170 = ttnn.to_device(ttnn_to_layout_292, device = device)
  ttnn_subtract_25 = ttnn.subtract(ttnn_permute_75, ttnn_to_device_170, )
  ttnn_from_device_261 = ttnn.from_device(ttnn_reshape_170, )
  ttnn_to_layout_293 = ttnn.to_layout(ttnn_from_device_261, ttnn.TILE_LAYOUT, )
  ttnn_to_device_171 = ttnn.to_device(ttnn_to_layout_293, device = device)
  ttnn_multiply_53 = ttnn.multiply(ttnn_subtract_25, ttnn_to_device_171, )
  ttnn_from_torch_140 = ttnn.from_torch(arg88_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_172 = ttnn.reshape(ttnn_from_torch_140, (1, 184, 1, 1), )
  ttnn_from_device_262 = ttnn.from_device(ttnn_reshape_172, )
  ttnn_to_layout_294 = ttnn.to_layout(ttnn_from_device_262, ttnn.TILE_LAYOUT, )
  ttnn_to_device_172 = ttnn.to_device(ttnn_to_layout_294, device = device)
  ttnn_multiply_54 = ttnn.multiply(ttnn_multiply_53, ttnn_to_device_172, )
  ttnn_from_torch_141 = ttnn.from_torch(arg89_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_173 = ttnn.reshape(ttnn_from_torch_141, (1, 184, 1, 1), )
  ttnn_from_device_263 = ttnn.from_device(ttnn_reshape_173, )
  ttnn_to_layout_295 = ttnn.to_layout(ttnn_from_device_263, ttnn.TILE_LAYOUT, )
  ttnn_to_device_173 = ttnn.to_device(ttnn_to_layout_295, device = device)
  ttnn_add_56 = ttnn.add(ttnn_multiply_54, ttnn_to_device_173, )
  ttnn_prefix_pack_to_tuple_25 = pack_to_tuple_wrapper(ttnn_add_56, )
  ttnn_prefix_getitem_25 = ttnn_prefix_pack_to_tuple_25[0]
  ttnn_hardswish_6 = ttnn.hardswish(ttnn_prefix_getitem_25, )
  ttnn_permute_76 = ttnn.permute(ttnn_hardswish_6, (0, 2, 3, 1), )
  ttnn_from_device_264 = ttnn.from_device(ttnn_permute_76, )
  ttnn_to_layout_296 = ttnn.to_layout(ttnn_from_device_264, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_174 = ttnn.reshape(ttnn_to_layout_296, (1, 1, 400, 184), )
  ttnn_from_device_265 = ttnn.from_device(ttnn_reshape_174, )
  ttnn_to_layout_297 = ttnn.to_layout(ttnn_from_device_265, ttnn.TILE_LAYOUT, )
  ttnn_to_device_174 = ttnn.to_device(ttnn_to_layout_297, device = device)
  ttnn_from_torch_142 = ttnn.from_torch(arg90_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_32 = conv_wrapper(ttnn_to_device_174, ttnn_from_torch_142, None, 1, 184, 80, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_32 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_32, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_266 = ttnn.from_device(ttnn_sharded_to_interleaved_32, )
  ttnn_to_layout_298 = ttnn.to_layout(ttnn_from_device_266, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_175 = ttnn.reshape(ttnn_to_layout_298, (1, 20, 20, 80), )
  ttnn_from_device_267 = ttnn.from_device(ttnn_reshape_175, )
  ttnn_to_layout_299 = ttnn.to_layout(ttnn_from_device_267, ttnn.TILE_LAYOUT, )
  ttnn_to_device_175 = ttnn.to_device(ttnn_to_layout_299, device = device)
  ttnn_permute_77 = ttnn.permute(ttnn_to_device_175, (0, 3, 1, 2), )
  ttnn_from_torch_143 = ttnn.from_torch(arg351_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_57 = ttnn.add(ttnn_from_torch_143, 0.001, )
  ttnn_rsqrt_26 = ttnn.rsqrt(ttnn_add_57, )
  ttnn_from_device_268 = ttnn.from_device(ttnn_rsqrt_26, )
  ttnn_to_layout_300 = ttnn.to_layout(ttnn_from_device_268, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_176 = ttnn.reshape(ttnn_to_layout_300, (1, 80, 1, 1), )
  ttnn_from_torch_144 = ttnn.from_torch(arg350_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_177 = ttnn.reshape(ttnn_from_torch_144, (1, 80, 1, 1), )
  ttnn_from_device_269 = ttnn.from_device(ttnn_reshape_177, )
  ttnn_to_layout_301 = ttnn.to_layout(ttnn_from_device_269, ttnn.TILE_LAYOUT, )
  ttnn_to_device_176 = ttnn.to_device(ttnn_to_layout_301, device = device)
  ttnn_subtract_26 = ttnn.subtract(ttnn_permute_77, ttnn_to_device_176, )
  ttnn_from_device_270 = ttnn.from_device(ttnn_reshape_176, )
  ttnn_to_layout_302 = ttnn.to_layout(ttnn_from_device_270, ttnn.TILE_LAYOUT, )
  ttnn_to_device_177 = ttnn.to_device(ttnn_to_layout_302, device = device)
  ttnn_multiply_55 = ttnn.multiply(ttnn_subtract_26, ttnn_to_device_177, )
  ttnn_from_torch_145 = ttnn.from_torch(arg91_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_178 = ttnn.reshape(ttnn_from_torch_145, (1, 80, 1, 1), )
  ttnn_from_device_271 = ttnn.from_device(ttnn_reshape_178, )
  ttnn_to_layout_303 = ttnn.to_layout(ttnn_from_device_271, ttnn.TILE_LAYOUT, )
  ttnn_to_device_178 = ttnn.to_device(ttnn_to_layout_303, device = device)
  ttnn_multiply_56 = ttnn.multiply(ttnn_multiply_55, ttnn_to_device_178, )
  ttnn_from_torch_146 = ttnn.from_torch(arg92_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_179 = ttnn.reshape(ttnn_from_torch_146, (1, 80, 1, 1), )
  ttnn_from_device_272 = ttnn.from_device(ttnn_reshape_179, )
  ttnn_to_layout_304 = ttnn.to_layout(ttnn_from_device_272, ttnn.TILE_LAYOUT, )
  ttnn_to_device_179 = ttnn.to_device(ttnn_to_layout_304, device = device)
  ttnn_add_58 = ttnn.add(ttnn_multiply_56, ttnn_to_device_179, )
  ttnn_prefix_pack_to_tuple_26 = pack_to_tuple_wrapper(ttnn_add_58, )
  ttnn_prefix_getitem_26 = ttnn_prefix_pack_to_tuple_26[0]
  ttnn_add_59 = ttnn.add(ttnn_prefix_getitem_26, ttnn_add_52, )
  ttnn_permute_78 = ttnn.permute(ttnn_add_59, (0, 2, 3, 1), )
  ttnn_from_device_273 = ttnn.from_device(ttnn_permute_78, )
  ttnn_to_layout_305 = ttnn.to_layout(ttnn_from_device_273, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_180 = ttnn.reshape(ttnn_to_layout_305, (1, 1, 400, 80), )
  ttnn_from_device_274 = ttnn.from_device(ttnn_reshape_180, )
  ttnn_to_layout_306 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_180 = ttnn.to_device(ttnn_to_layout_306, device = device)
  ttnn_from_torch_147 = ttnn.from_torch(arg93_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_33 = conv_wrapper(ttnn_to_device_180, ttnn_from_torch_147, None, 1, 80, 184, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_33 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_33, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_275 = ttnn.from_device(ttnn_sharded_to_interleaved_33, )
  ttnn_to_layout_307 = ttnn.to_layout(ttnn_from_device_275, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_181 = ttnn.reshape(ttnn_to_layout_307, (1, 20, 20, 184), )
  ttnn_from_device_276 = ttnn.from_device(ttnn_reshape_181, )
  ttnn_to_layout_308 = ttnn.to_layout(ttnn_from_device_276, ttnn.TILE_LAYOUT, )
  ttnn_to_device_181 = ttnn.to_device(ttnn_to_layout_308, device = device)
  ttnn_permute_79 = ttnn.permute(ttnn_to_device_181, (0, 3, 1, 2), )
  ttnn_from_torch_148 = ttnn.from_torch(arg354_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_60 = ttnn.add(ttnn_from_torch_148, 0.001, )
  ttnn_rsqrt_27 = ttnn.rsqrt(ttnn_add_60, )
  ttnn_from_device_277 = ttnn.from_device(ttnn_rsqrt_27, )
  ttnn_to_layout_309 = ttnn.to_layout(ttnn_from_device_277, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_182 = ttnn.reshape(ttnn_to_layout_309, (1, 184, 1, 1), )
  ttnn_from_torch_149 = ttnn.from_torch(arg353_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_183 = ttnn.reshape(ttnn_from_torch_149, (1, 184, 1, 1), )
  ttnn_from_device_278 = ttnn.from_device(ttnn_reshape_183, )
  ttnn_to_layout_310 = ttnn.to_layout(ttnn_from_device_278, ttnn.TILE_LAYOUT, )
  ttnn_to_device_182 = ttnn.to_device(ttnn_to_layout_310, device = device)
  ttnn_subtract_27 = ttnn.subtract(ttnn_permute_79, ttnn_to_device_182, )
  ttnn_from_device_279 = ttnn.from_device(ttnn_reshape_182, )
  ttnn_to_layout_311 = ttnn.to_layout(ttnn_from_device_279, ttnn.TILE_LAYOUT, )
  ttnn_to_device_183 = ttnn.to_device(ttnn_to_layout_311, device = device)
  ttnn_multiply_57 = ttnn.multiply(ttnn_subtract_27, ttnn_to_device_183, )
  ttnn_from_torch_150 = ttnn.from_torch(arg94_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_184 = ttnn.reshape(ttnn_from_torch_150, (1, 184, 1, 1), )
  ttnn_from_device_280 = ttnn.from_device(ttnn_reshape_184, )
  ttnn_to_layout_312 = ttnn.to_layout(ttnn_from_device_280, ttnn.TILE_LAYOUT, )
  ttnn_to_device_184 = ttnn.to_device(ttnn_to_layout_312, device = device)
  ttnn_multiply_58 = ttnn.multiply(ttnn_multiply_57, ttnn_to_device_184, )
  ttnn_from_torch_151 = ttnn.from_torch(arg95_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_185 = ttnn.reshape(ttnn_from_torch_151, (1, 184, 1, 1), )
  ttnn_from_device_281 = ttnn.from_device(ttnn_reshape_185, )
  ttnn_to_layout_313 = ttnn.to_layout(ttnn_from_device_281, ttnn.TILE_LAYOUT, )
  ttnn_to_device_185 = ttnn.to_device(ttnn_to_layout_313, device = device)
  ttnn_add_61 = ttnn.add(ttnn_multiply_58, ttnn_to_device_185, )
  ttnn_prefix_pack_to_tuple_27 = pack_to_tuple_wrapper(ttnn_add_61, )
  ttnn_prefix_getitem_27 = ttnn_prefix_pack_to_tuple_27[0]
  ttnn_hardswish_7 = ttnn.hardswish(ttnn_prefix_getitem_27, )
  ttnn_permute_80 = ttnn.permute(ttnn_hardswish_7, (0, 2, 3, 1), )
  ttnn_from_device_282 = ttnn.from_device(ttnn_permute_80, )
  ttnn_to_layout_314 = ttnn.to_layout(ttnn_from_device_282, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_186 = ttnn.reshape(ttnn_to_layout_314, (1, 1, 400, 184), )
  ttnn_from_device_283 = ttnn.from_device(ttnn_reshape_186, )
  ttnn_to_layout_315 = ttnn.to_layout(ttnn_from_device_283, ttnn.TILE_LAYOUT, )
  ttnn_to_device_186 = ttnn.to_device(ttnn_to_layout_315, device = device)
  ttnn_from_torch_152 = ttnn.from_torch(arg96_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_34 = conv_wrapper(ttnn_to_device_186, ttnn_from_torch_152, None, 1, 184, 184, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 184, device, False, None, )
  ttnn_sharded_to_interleaved_34 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_34, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_284 = ttnn.from_device(ttnn_sharded_to_interleaved_34, )
  ttnn_to_layout_316 = ttnn.to_layout(ttnn_from_device_284, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_187 = ttnn.reshape(ttnn_to_layout_316, (1, 20, 20, 184), )
  ttnn_from_device_285 = ttnn.from_device(ttnn_reshape_187, )
  ttnn_to_layout_317 = ttnn.to_layout(ttnn_from_device_285, ttnn.TILE_LAYOUT, )
  ttnn_to_device_187 = ttnn.to_device(ttnn_to_layout_317, device = device)
  ttnn_permute_81 = ttnn.permute(ttnn_to_device_187, (0, 3, 1, 2), )
  test_accuracy(convolution_34, ttnn_permute_81)
  ttnn_from_torch_153 = ttnn.from_torch(arg357_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_62 = ttnn.add(ttnn_from_torch_153, 0.001, )
  ttnn_rsqrt_28 = ttnn.rsqrt(ttnn_add_62, )
  ttnn_from_device_286 = ttnn.from_device(ttnn_rsqrt_28, )
  ttnn_to_layout_318 = ttnn.to_layout(ttnn_from_device_286, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_188 = ttnn.reshape(ttnn_to_layout_318, (1, 184, 1, 1), )
  ttnn_from_torch_154 = ttnn.from_torch(arg356_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_189 = ttnn.reshape(ttnn_from_torch_154, (1, 184, 1, 1), )
  ttnn_from_device_287 = ttnn.from_device(ttnn_reshape_189, )
  ttnn_to_layout_319 = ttnn.to_layout(ttnn_from_device_287, ttnn.TILE_LAYOUT, )
  ttnn_to_device_188 = ttnn.to_device(ttnn_to_layout_319, device = device)
  ttnn_subtract_28 = ttnn.subtract(ttnn_permute_81, ttnn_to_device_188, )
  ttnn_from_device_288 = ttnn.from_device(ttnn_reshape_188, )
  ttnn_to_layout_320 = ttnn.to_layout(ttnn_from_device_288, ttnn.TILE_LAYOUT, )
  ttnn_to_device_189 = ttnn.to_device(ttnn_to_layout_320, device = device)
  ttnn_multiply_59 = ttnn.multiply(ttnn_subtract_28, ttnn_to_device_189, )
  ttnn_from_torch_155 = ttnn.from_torch(arg97_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_190 = ttnn.reshape(ttnn_from_torch_155, (1, 184, 1, 1), )
  ttnn_from_device_289 = ttnn.from_device(ttnn_reshape_190, )
  ttnn_to_layout_321 = ttnn.to_layout(ttnn_from_device_289, ttnn.TILE_LAYOUT, )
  ttnn_to_device_190 = ttnn.to_device(ttnn_to_layout_321, device = device)
  ttnn_multiply_60 = ttnn.multiply(ttnn_multiply_59, ttnn_to_device_190, )
  ttnn_from_torch_156 = ttnn.from_torch(arg98_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_191 = ttnn.reshape(ttnn_from_torch_156, (1, 184, 1, 1), )
  ttnn_from_device_290 = ttnn.from_device(ttnn_reshape_191, )
  ttnn_to_layout_322 = ttnn.to_layout(ttnn_from_device_290, ttnn.TILE_LAYOUT, )
  ttnn_to_device_191 = ttnn.to_device(ttnn_to_layout_322, device = device)
  ttnn_add_63 = ttnn.add(ttnn_multiply_60, ttnn_to_device_191, )
  ttnn_prefix_pack_to_tuple_28 = pack_to_tuple_wrapper(ttnn_add_63, )
  ttnn_prefix_getitem_28 = ttnn_prefix_pack_to_tuple_28[0]
  test_accuracy(getitem_84, ttnn_prefix_getitem_28)
  ttnn_hardswish_8 = ttnn.hardswish(ttnn_prefix_getitem_28, )
  test_accuracy(hardswish_8, ttnn_hardswish_8)
  ttnn_permute_82 = ttnn.permute(ttnn_hardswish_8, (0, 2, 3, 1), )
  ttnn_from_device_291 = ttnn.from_device(ttnn_permute_82, )
  ttnn_to_layout_323 = ttnn.to_layout(ttnn_from_device_291, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_192 = ttnn.reshape(ttnn_to_layout_323, (1, 1, 400, 184), )
  ttnn_from_device_292 = ttnn.from_device(ttnn_reshape_192, )
  ttnn_to_layout_324 = ttnn.to_layout(ttnn_from_device_292, ttnn.TILE_LAYOUT, )
  ttnn_to_device_192 = ttnn.to_device(ttnn_to_layout_324, device = device)
  ttnn_from_torch_157 = ttnn.from_torch(arg99_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_35 = conv_wrapper(ttnn_to_device_192, ttnn_from_torch_157, None, 1, 184, 80, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_35 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_35, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_293 = ttnn.from_device(ttnn_sharded_to_interleaved_35, )
  ttnn_to_layout_325 = ttnn.to_layout(ttnn_from_device_293, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_193 = ttnn.reshape(ttnn_to_layout_325, (1, 20, 20, 80), )
  ttnn_from_device_294 = ttnn.from_device(ttnn_reshape_193, )
  ttnn_to_layout_326 = ttnn.to_layout(ttnn_from_device_294, ttnn.TILE_LAYOUT, )
  ttnn_to_device_193 = ttnn.to_device(ttnn_to_layout_326, device = device)
  ttnn_permute_83 = ttnn.permute(ttnn_to_device_193, (0, 3, 1, 2), )
  test_accuracy(convolution_35, ttnn_permute_83)
  ttnn_from_torch_158 = ttnn.from_torch(arg360_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_64 = ttnn.add(ttnn_from_torch_158, 0.001, )
  ttnn_rsqrt_29 = ttnn.rsqrt(ttnn_add_64, )
  ttnn_from_device_295 = ttnn.from_device(ttnn_rsqrt_29, )
  ttnn_to_layout_327 = ttnn.to_layout(ttnn_from_device_295, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_194 = ttnn.reshape(ttnn_to_layout_327, (1, 80, 1, 1), )
  ttnn_from_torch_159 = ttnn.from_torch(arg359_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_195 = ttnn.reshape(ttnn_from_torch_159, (1, 80, 1, 1), )
  ttnn_from_device_296 = ttnn.from_device(ttnn_reshape_195, )
  ttnn_to_layout_328 = ttnn.to_layout(ttnn_from_device_296, ttnn.TILE_LAYOUT, )
  ttnn_to_device_194 = ttnn.to_device(ttnn_to_layout_328, device = device)
  ttnn_subtract_29 = ttnn.subtract(ttnn_permute_83, ttnn_to_device_194, )
  ttnn_from_device_297 = ttnn.from_device(ttnn_reshape_194, )
  ttnn_to_layout_329 = ttnn.to_layout(ttnn_from_device_297, ttnn.TILE_LAYOUT, )
  ttnn_to_device_195 = ttnn.to_device(ttnn_to_layout_329, device = device)
  ttnn_multiply_61 = ttnn.multiply(ttnn_subtract_29, ttnn_to_device_195, )
  ttnn_from_torch_160 = ttnn.from_torch(arg100_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_196 = ttnn.reshape(ttnn_from_torch_160, (1, 80, 1, 1), )
  ttnn_from_device_298 = ttnn.from_device(ttnn_reshape_196, )
  ttnn_to_layout_330 = ttnn.to_layout(ttnn_from_device_298, ttnn.TILE_LAYOUT, )
  ttnn_to_device_196 = ttnn.to_device(ttnn_to_layout_330, device = device)
  ttnn_multiply_62 = ttnn.multiply(ttnn_multiply_61, ttnn_to_device_196, )
  ttnn_from_torch_161 = ttnn.from_torch(arg101_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_197 = ttnn.reshape(ttnn_from_torch_161, (1, 80, 1, 1), )
  ttnn_from_device_299 = ttnn.from_device(ttnn_reshape_197, )
  ttnn_to_layout_331 = ttnn.to_layout(ttnn_from_device_299, ttnn.TILE_LAYOUT, )
  ttnn_to_device_197 = ttnn.to_device(ttnn_to_layout_331, device = device)
  ttnn_add_65 = ttnn.add(ttnn_multiply_62, ttnn_to_device_197, )
  ttnn_prefix_pack_to_tuple_29 = pack_to_tuple_wrapper(ttnn_add_65, )
  ttnn_prefix_getitem_29 = ttnn_prefix_pack_to_tuple_29[0]
  test_accuracy(getitem_87, ttnn_prefix_getitem_29)
  ttnn_add_66 = ttnn.add(ttnn_prefix_getitem_29, ttnn_add_59, )
  test_accuracy(add_11, ttnn_add_66)
  ttnn_permute_84 = ttnn.permute(ttnn_add_66, (0, 2, 3, 1), )
  ttnn_from_device_300 = ttnn.from_device(ttnn_permute_84, )
  ttnn_to_layout_332 = ttnn.to_layout(ttnn_from_device_300, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_198 = ttnn.reshape(ttnn_to_layout_332, (1, 1, 400, 80), )
  ttnn_from_device_301 = ttnn.from_device(ttnn_reshape_198, )
  ttnn_to_layout_333 = ttnn.to_layout(ttnn_from_device_301, ttnn.TILE_LAYOUT, )
  ttnn_to_device_198 = ttnn.to_device(ttnn_to_layout_333, device = device)
  ttnn_from_torch_162 = ttnn.from_torch(arg102_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_36 = conv_wrapper(ttnn_to_device_198, ttnn_from_torch_162, None, 1, 80, 480, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_36 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_36, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_302 = ttnn.from_device(ttnn_sharded_to_interleaved_36, )
  ttnn_to_layout_334 = ttnn.to_layout(ttnn_from_device_302, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_199 = ttnn.reshape(ttnn_to_layout_334, (1, 20, 20, 480), )
  ttnn_from_device_303 = ttnn.from_device(ttnn_reshape_199, )
  ttnn_to_layout_335 = ttnn.to_layout(ttnn_from_device_303, ttnn.TILE_LAYOUT, )
  ttnn_to_device_199 = ttnn.to_device(ttnn_to_layout_335, device = device)
  ttnn_permute_85 = ttnn.permute(ttnn_to_device_199, (0, 3, 1, 2), )
  ttnn_from_torch_163 = ttnn.from_torch(arg363_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_67 = ttnn.add(ttnn_from_torch_163, 0.001, )
  ttnn_rsqrt_30 = ttnn.rsqrt(ttnn_add_67, )
  ttnn_from_device_304 = ttnn.from_device(ttnn_rsqrt_30, )
  ttnn_to_layout_336 = ttnn.to_layout(ttnn_from_device_304, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_200 = ttnn.reshape(ttnn_to_layout_336, (1, 480, 1, 1), )
  ttnn_from_torch_164 = ttnn.from_torch(arg362_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_201 = ttnn.reshape(ttnn_from_torch_164, (1, 480, 1, 1), )
  ttnn_from_device_305 = ttnn.from_device(ttnn_reshape_201, )
  ttnn_to_layout_337 = ttnn.to_layout(ttnn_from_device_305, ttnn.TILE_LAYOUT, )
  ttnn_to_device_200 = ttnn.to_device(ttnn_to_layout_337, device = device)
  ttnn_subtract_30 = ttnn.subtract(ttnn_permute_85, ttnn_to_device_200, )
  ttnn_from_device_306 = ttnn.from_device(ttnn_reshape_200, )
  ttnn_to_layout_338 = ttnn.to_layout(ttnn_from_device_306, ttnn.TILE_LAYOUT, )
  ttnn_to_device_201 = ttnn.to_device(ttnn_to_layout_338, device = device)
  ttnn_multiply_63 = ttnn.multiply(ttnn_subtract_30, ttnn_to_device_201, )
  ttnn_from_torch_165 = ttnn.from_torch(arg103_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_202 = ttnn.reshape(ttnn_from_torch_165, (1, 480, 1, 1), )
  ttnn_from_device_307 = ttnn.from_device(ttnn_reshape_202, )
  ttnn_to_layout_339 = ttnn.to_layout(ttnn_from_device_307, ttnn.TILE_LAYOUT, )
  ttnn_to_device_202 = ttnn.to_device(ttnn_to_layout_339, device = device)
  ttnn_multiply_64 = ttnn.multiply(ttnn_multiply_63, ttnn_to_device_202, )
  ttnn_from_torch_166 = ttnn.from_torch(arg104_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_203 = ttnn.reshape(ttnn_from_torch_166, (1, 480, 1, 1), )
  ttnn_from_device_308 = ttnn.from_device(ttnn_reshape_203, )
  ttnn_to_layout_340 = ttnn.to_layout(ttnn_from_device_308, ttnn.TILE_LAYOUT, )
  ttnn_to_device_203 = ttnn.to_device(ttnn_to_layout_340, device = device)
  ttnn_add_68 = ttnn.add(ttnn_multiply_64, ttnn_to_device_203, )
  ttnn_prefix_pack_to_tuple_30 = pack_to_tuple_wrapper(ttnn_add_68, )
  ttnn_prefix_getitem_30 = ttnn_prefix_pack_to_tuple_30[0]
  ttnn_hardswish_9 = ttnn.hardswish(ttnn_prefix_getitem_30, )
  ttnn_permute_86 = ttnn.permute(ttnn_hardswish_9, (0, 2, 3, 1), )
  ttnn_from_device_309 = ttnn.from_device(ttnn_permute_86, )
  ttnn_to_layout_341 = ttnn.to_layout(ttnn_from_device_309, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_204 = ttnn.reshape(ttnn_to_layout_341, (1, 1, 400, 480), )
  ttnn_from_device_310 = ttnn.from_device(ttnn_reshape_204, )
  ttnn_to_layout_342 = ttnn.to_layout(ttnn_from_device_310, ttnn.TILE_LAYOUT, )
  ttnn_to_device_204 = ttnn.to_device(ttnn_to_layout_342, device = device)
  ttnn_from_torch_167 = ttnn.from_torch(arg105_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_37 = conv_wrapper(ttnn_to_device_204, ttnn_from_torch_167, None, 1, 480, 480, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 480, device, False, None, )
  ttnn_sharded_to_interleaved_37 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_37, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_311 = ttnn.from_device(ttnn_sharded_to_interleaved_37, )
  ttnn_to_layout_343 = ttnn.to_layout(ttnn_from_device_311, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_205 = ttnn.reshape(ttnn_to_layout_343, (1, 20, 20, 480), )
  ttnn_from_device_312 = ttnn.from_device(ttnn_reshape_205, )
  ttnn_to_layout_344 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_205 = ttnn.to_device(ttnn_to_layout_344, device = device)
  ttnn_permute_87 = ttnn.permute(ttnn_to_device_205, (0, 3, 1, 2), )
  test_accuracy(convolution_37, ttnn_permute_87)
  ttnn_from_torch_168 = ttnn.from_torch(arg366_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_69 = ttnn.add(ttnn_from_torch_168, 0.001, )
  ttnn_rsqrt_31 = ttnn.rsqrt(ttnn_add_69, )
  ttnn_from_device_313 = ttnn.from_device(ttnn_rsqrt_31, )
  ttnn_to_layout_345 = ttnn.to_layout(ttnn_from_device_313, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_206 = ttnn.reshape(ttnn_to_layout_345, (1, 480, 1, 1), )
  ttnn_from_torch_169 = ttnn.from_torch(arg365_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_207 = ttnn.reshape(ttnn_from_torch_169, (1, 480, 1, 1), )
  ttnn_from_device_314 = ttnn.from_device(ttnn_reshape_207, )
  ttnn_to_layout_346 = ttnn.to_layout(ttnn_from_device_314, ttnn.TILE_LAYOUT, )
  ttnn_to_device_206 = ttnn.to_device(ttnn_to_layout_346, device = device)
  ttnn_subtract_31 = ttnn.subtract(ttnn_permute_87, ttnn_to_device_206, )
  ttnn_from_device_315 = ttnn.from_device(ttnn_reshape_206, )
  ttnn_to_layout_347 = ttnn.to_layout(ttnn_from_device_315, ttnn.TILE_LAYOUT, )
  ttnn_to_device_207 = ttnn.to_device(ttnn_to_layout_347, device = device)
  ttnn_multiply_65 = ttnn.multiply(ttnn_subtract_31, ttnn_to_device_207, )
  ttnn_from_torch_170 = ttnn.from_torch(arg106_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_208 = ttnn.reshape(ttnn_from_torch_170, (1, 480, 1, 1), )
  ttnn_from_device_316 = ttnn.from_device(ttnn_reshape_208, )
  ttnn_to_layout_348 = ttnn.to_layout(ttnn_from_device_316, ttnn.TILE_LAYOUT, )
  ttnn_to_device_208 = ttnn.to_device(ttnn_to_layout_348, device = device)
  ttnn_multiply_66 = ttnn.multiply(ttnn_multiply_65, ttnn_to_device_208, )
  ttnn_from_torch_171 = ttnn.from_torch(arg107_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_209 = ttnn.reshape(ttnn_from_torch_171, (1, 480, 1, 1), )
  ttnn_from_device_317 = ttnn.from_device(ttnn_reshape_209, )
  ttnn_to_layout_349 = ttnn.to_layout(ttnn_from_device_317, ttnn.TILE_LAYOUT, )
  ttnn_to_device_209 = ttnn.to_device(ttnn_to_layout_349, device = device)
  ttnn_add_70 = ttnn.add(ttnn_multiply_66, ttnn_to_device_209, )
  ttnn_prefix_pack_to_tuple_31 = pack_to_tuple_wrapper(ttnn_add_70, )
  ttnn_prefix_getitem_31 = ttnn_prefix_pack_to_tuple_31[0]
  test_accuracy(getitem_93, ttnn_prefix_getitem_31)
  ttnn_hardswish_10 = ttnn.hardswish(ttnn_prefix_getitem_31, )
  test_accuracy(hardswish_10, ttnn_hardswish_10)
  ttnn_mean_3 = ttnn.mean(ttnn_hardswish_10, (-1, -2), )
  ttnn_permute_88 = ttnn.permute(ttnn_mean_3, (0, 2, 3, 1), )
  ttnn_from_device_318 = ttnn.from_device(ttnn_permute_88, )
  ttnn_to_layout_350 = ttnn.to_layout(ttnn_from_device_318, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_210 = ttnn.reshape(ttnn_to_layout_350, (1, 1, 1, 480), )
  ttnn_from_torch_172 = ttnn.from_torch(arg109_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_211 = ttnn.reshape(ttnn_from_torch_172, (1, 1, 1, 120), )
  ttnn_from_device_319 = ttnn.from_device(ttnn_reshape_211, )
  ttnn_to_layout_351 = ttnn.to_layout(ttnn_from_device_319, ttnn.TILE_LAYOUT, )
  ttnn_to_device_210 = ttnn.to_device(ttnn_to_layout_351, device = device)
  ttnn_prefix_move_to_host_6 = move_to_host_wrapper(ttnn_to_device_210, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_320 = ttnn.from_device(ttnn_reshape_210, )
  ttnn_to_layout_352 = ttnn.to_layout(ttnn_from_device_320, ttnn.TILE_LAYOUT, )
  ttnn_to_device_211 = ttnn.to_device(ttnn_to_layout_352, device = device)
  ttnn_from_torch_173 = ttnn.from_torch(arg108_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_38 = conv_wrapper(ttnn_to_device_211, ttnn_from_torch_173, ttnn_prefix_move_to_host_6, 1, 480, 120, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_38 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_38, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_321 = ttnn.from_device(ttnn_sharded_to_interleaved_38, )
  ttnn_to_layout_353 = ttnn.to_layout(ttnn_from_device_321, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_212 = ttnn.reshape(ttnn_to_layout_353, (1, 1, 1, 120), )
  ttnn_from_device_322 = ttnn.from_device(ttnn_reshape_212, )
  ttnn_to_layout_354 = ttnn.to_layout(ttnn_from_device_322, ttnn.TILE_LAYOUT, )
  ttnn_to_device_212 = ttnn.to_device(ttnn_to_layout_354, device = device)
  ttnn_permute_89 = ttnn.permute(ttnn_to_device_212, (0, 3, 1, 2), )
  ttnn_relu_14 = ttnn.relu(ttnn_permute_89, )
  ttnn_permute_90 = ttnn.permute(ttnn_relu_14, (0, 2, 3, 1), )
  ttnn_from_device_323 = ttnn.from_device(ttnn_permute_90, )
  ttnn_to_layout_355 = ttnn.to_layout(ttnn_from_device_323, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_213 = ttnn.reshape(ttnn_to_layout_355, (1, 1, 1, 120), )
  ttnn_from_torch_174 = ttnn.from_torch(arg111_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_214 = ttnn.reshape(ttnn_from_torch_174, (1, 1, 1, 480), )
  ttnn_from_device_324 = ttnn.from_device(ttnn_reshape_214, )
  ttnn_to_layout_356 = ttnn.to_layout(ttnn_from_device_324, ttnn.TILE_LAYOUT, )
  ttnn_to_device_213 = ttnn.to_device(ttnn_to_layout_356, device = device)
  ttnn_prefix_move_to_host_7 = move_to_host_wrapper(ttnn_to_device_213, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_325 = ttnn.from_device(ttnn_reshape_213, )
  ttnn_to_layout_357 = ttnn.to_layout(ttnn_from_device_325, ttnn.TILE_LAYOUT, )
  ttnn_to_device_214 = ttnn.to_device(ttnn_to_layout_357, device = device)
  ttnn_from_torch_175 = ttnn.from_torch(arg110_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_39 = conv_wrapper(ttnn_to_device_214, ttnn_from_torch_175, ttnn_prefix_move_to_host_7, 1, 120, 480, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_39 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_39, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_326 = ttnn.from_device(ttnn_sharded_to_interleaved_39, )
  ttnn_to_layout_358 = ttnn.to_layout(ttnn_from_device_326, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_215 = ttnn.reshape(ttnn_to_layout_358, (1, 1, 1, 480), )
  ttnn_from_device_327 = ttnn.from_device(ttnn_reshape_215, )
  ttnn_to_layout_359 = ttnn.to_layout(ttnn_from_device_327, ttnn.TILE_LAYOUT, )
  ttnn_to_device_215 = ttnn.to_device(ttnn_to_layout_359, device = device)
  ttnn_permute_91 = ttnn.permute(ttnn_to_device_215, (0, 3, 1, 2), )
  ttnn_hardsigmoid_3 = ttnn.hardsigmoid(ttnn_permute_91, )
  ttnn_multiply_67 = ttnn.multiply(ttnn_hardsigmoid_3, ttnn_hardswish_10, )
  test_accuracy(mul_11, ttnn_multiply_67)
  ttnn_permute_92 = ttnn.permute(ttnn_multiply_67, (0, 2, 3, 1), )
  ttnn_from_device_328 = ttnn.from_device(ttnn_permute_92, )
  ttnn_to_layout_360 = ttnn.to_layout(ttnn_from_device_328, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_216 = ttnn.reshape(ttnn_to_layout_360, (1, 1, 400, 480), )
  ttnn_from_device_329 = ttnn.from_device(ttnn_reshape_216, )
  ttnn_to_layout_361 = ttnn.to_layout(ttnn_from_device_329, ttnn.TILE_LAYOUT, )
  ttnn_to_device_216 = ttnn.to_device(ttnn_to_layout_361, device = device)
  ttnn_from_torch_176 = ttnn.from_torch(arg112_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_40 = conv_wrapper(ttnn_to_device_216, ttnn_from_torch_176, None, 1, 480, 112, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_40 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_40, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_330 = ttnn.from_device(ttnn_sharded_to_interleaved_40, )
  ttnn_to_layout_362 = ttnn.to_layout(ttnn_from_device_330, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_217 = ttnn.reshape(ttnn_to_layout_362, (1, 20, 20, 112), )
  ttnn_from_device_331 = ttnn.from_device(ttnn_reshape_217, )
  ttnn_to_layout_363 = ttnn.to_layout(ttnn_from_device_331, ttnn.TILE_LAYOUT, )
  ttnn_to_device_217 = ttnn.to_device(ttnn_to_layout_363, device = device)
  ttnn_permute_93 = ttnn.permute(ttnn_to_device_217, (0, 3, 1, 2), )
  ttnn_from_torch_177 = ttnn.from_torch(arg369_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_71 = ttnn.add(ttnn_from_torch_177, 0.001, )
  ttnn_rsqrt_32 = ttnn.rsqrt(ttnn_add_71, )
  ttnn_from_device_332 = ttnn.from_device(ttnn_rsqrt_32, )
  ttnn_to_layout_364 = ttnn.to_layout(ttnn_from_device_332, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_218 = ttnn.reshape(ttnn_to_layout_364, (1, 112, 1, 1), )
  ttnn_from_torch_178 = ttnn.from_torch(arg368_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_219 = ttnn.reshape(ttnn_from_torch_178, (1, 112, 1, 1), )
  ttnn_from_device_333 = ttnn.from_device(ttnn_reshape_219, )
  ttnn_to_layout_365 = ttnn.to_layout(ttnn_from_device_333, ttnn.TILE_LAYOUT, )
  ttnn_to_device_218 = ttnn.to_device(ttnn_to_layout_365, device = device)
  ttnn_subtract_32 = ttnn.subtract(ttnn_permute_93, ttnn_to_device_218, )
  ttnn_from_device_334 = ttnn.from_device(ttnn_reshape_218, )
  ttnn_to_layout_366 = ttnn.to_layout(ttnn_from_device_334, ttnn.TILE_LAYOUT, )
  ttnn_to_device_219 = ttnn.to_device(ttnn_to_layout_366, device = device)
  ttnn_multiply_68 = ttnn.multiply(ttnn_subtract_32, ttnn_to_device_219, )
  ttnn_from_torch_179 = ttnn.from_torch(arg113_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_220 = ttnn.reshape(ttnn_from_torch_179, (1, 112, 1, 1), )
  ttnn_from_device_335 = ttnn.from_device(ttnn_reshape_220, )
  ttnn_to_layout_367 = ttnn.to_layout(ttnn_from_device_335, ttnn.TILE_LAYOUT, )
  ttnn_to_device_220 = ttnn.to_device(ttnn_to_layout_367, device = device)
  ttnn_multiply_69 = ttnn.multiply(ttnn_multiply_68, ttnn_to_device_220, )
  ttnn_from_torch_180 = ttnn.from_torch(arg114_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_221 = ttnn.reshape(ttnn_from_torch_180, (1, 112, 1, 1), )
  ttnn_from_device_336 = ttnn.from_device(ttnn_reshape_221, )
  ttnn_to_layout_368 = ttnn.to_layout(ttnn_from_device_336, ttnn.TILE_LAYOUT, )
  ttnn_to_device_221 = ttnn.to_device(ttnn_to_layout_368, device = device)
  ttnn_add_72 = ttnn.add(ttnn_multiply_69, ttnn_to_device_221, )
  ttnn_prefix_pack_to_tuple_32 = pack_to_tuple_wrapper(ttnn_add_72, )
  ttnn_prefix_getitem_32 = ttnn_prefix_pack_to_tuple_32[0]
  ttnn_permute_94 = ttnn.permute(ttnn_prefix_getitem_32, (0, 2, 3, 1), )
  ttnn_from_device_337 = ttnn.from_device(ttnn_permute_94, )
  ttnn_to_layout_369 = ttnn.to_layout(ttnn_from_device_337, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_222 = ttnn.reshape(ttnn_to_layout_369, (1, 1, 400, 112), )
  ttnn_from_device_338 = ttnn.from_device(ttnn_reshape_222, )
  ttnn_to_layout_370 = ttnn.to_layout(ttnn_from_device_338, ttnn.TILE_LAYOUT, )
  ttnn_to_device_222 = ttnn.to_device(ttnn_to_layout_370, device = device)
  ttnn_from_torch_181 = ttnn.from_torch(arg115_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_41 = conv_wrapper(ttnn_to_device_222, ttnn_from_torch_181, None, 1, 112, 672, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_41 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_41, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_339 = ttnn.from_device(ttnn_sharded_to_interleaved_41, )
  ttnn_to_layout_371 = ttnn.to_layout(ttnn_from_device_339, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_223 = ttnn.reshape(ttnn_to_layout_371, (1, 20, 20, 672), )
  ttnn_from_device_340 = ttnn.from_device(ttnn_reshape_223, )
  ttnn_to_layout_372 = ttnn.to_layout(ttnn_from_device_340, ttnn.TILE_LAYOUT, )
  ttnn_to_device_223 = ttnn.to_device(ttnn_to_layout_372, device = device)
  ttnn_permute_95 = ttnn.permute(ttnn_to_device_223, (0, 3, 1, 2), )
  ttnn_from_torch_182 = ttnn.from_torch(arg372_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_73 = ttnn.add(ttnn_from_torch_182, 0.001, )
  ttnn_rsqrt_33 = ttnn.rsqrt(ttnn_add_73, )
  ttnn_from_device_341 = ttnn.from_device(ttnn_rsqrt_33, )
  ttnn_to_layout_373 = ttnn.to_layout(ttnn_from_device_341, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_224 = ttnn.reshape(ttnn_to_layout_373, (1, 672, 1, 1), )
  ttnn_from_torch_183 = ttnn.from_torch(arg371_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_225 = ttnn.reshape(ttnn_from_torch_183, (1, 672, 1, 1), )
  ttnn_from_device_342 = ttnn.from_device(ttnn_reshape_225, )
  ttnn_to_layout_374 = ttnn.to_layout(ttnn_from_device_342, ttnn.TILE_LAYOUT, )
  ttnn_to_device_224 = ttnn.to_device(ttnn_to_layout_374, device = device)
  ttnn_subtract_33 = ttnn.subtract(ttnn_permute_95, ttnn_to_device_224, )
  ttnn_from_device_343 = ttnn.from_device(ttnn_reshape_224, )
  ttnn_to_layout_375 = ttnn.to_layout(ttnn_from_device_343, ttnn.TILE_LAYOUT, )
  ttnn_to_device_225 = ttnn.to_device(ttnn_to_layout_375, device = device)
  ttnn_multiply_70 = ttnn.multiply(ttnn_subtract_33, ttnn_to_device_225, )
  ttnn_from_torch_184 = ttnn.from_torch(arg116_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_226 = ttnn.reshape(ttnn_from_torch_184, (1, 672, 1, 1), )
  ttnn_from_device_344 = ttnn.from_device(ttnn_reshape_226, )
  ttnn_to_layout_376 = ttnn.to_layout(ttnn_from_device_344, ttnn.TILE_LAYOUT, )
  ttnn_to_device_226 = ttnn.to_device(ttnn_to_layout_376, device = device)
  ttnn_multiply_71 = ttnn.multiply(ttnn_multiply_70, ttnn_to_device_226, )
  ttnn_from_torch_185 = ttnn.from_torch(arg117_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_227 = ttnn.reshape(ttnn_from_torch_185, (1, 672, 1, 1), )
  ttnn_from_device_345 = ttnn.from_device(ttnn_reshape_227, )
  ttnn_to_layout_377 = ttnn.to_layout(ttnn_from_device_345, ttnn.TILE_LAYOUT, )
  ttnn_to_device_227 = ttnn.to_device(ttnn_to_layout_377, device = device)
  ttnn_add_74 = ttnn.add(ttnn_multiply_71, ttnn_to_device_227, )
  ttnn_prefix_pack_to_tuple_33 = pack_to_tuple_wrapper(ttnn_add_74, )
  ttnn_prefix_getitem_33 = ttnn_prefix_pack_to_tuple_33[0]
  ttnn_hardswish_11 = ttnn.hardswish(ttnn_prefix_getitem_33, )
  ttnn_permute_96 = ttnn.permute(ttnn_hardswish_11, (0, 2, 3, 1), )
  ttnn_from_device_346 = ttnn.from_device(ttnn_permute_96, )
  ttnn_to_layout_378 = ttnn.to_layout(ttnn_from_device_346, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_228 = ttnn.reshape(ttnn_to_layout_378, (1, 1, 400, 672), )
  ttnn_from_device_347 = ttnn.from_device(ttnn_reshape_228, )
  ttnn_to_layout_379 = ttnn.to_layout(ttnn_from_device_347, ttnn.TILE_LAYOUT, )
  ttnn_to_device_228 = ttnn.to_device(ttnn_to_layout_379, device = device)
  ttnn_from_torch_186 = ttnn.from_torch(arg118_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_42 = conv_wrapper(ttnn_to_device_228, ttnn_from_torch_186, None, 1, 672, 672, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 672, device, False, None, )
  ttnn_sharded_to_interleaved_42 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_42, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_348 = ttnn.from_device(ttnn_sharded_to_interleaved_42, )
  ttnn_to_layout_380 = ttnn.to_layout(ttnn_from_device_348, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_229 = ttnn.reshape(ttnn_to_layout_380, (1, 20, 20, 672), )
  ttnn_from_device_349 = ttnn.from_device(ttnn_reshape_229, )
  ttnn_to_layout_381 = ttnn.to_layout(ttnn_from_device_349, ttnn.TILE_LAYOUT, )
  ttnn_to_device_229 = ttnn.to_device(ttnn_to_layout_381, device = device)
  ttnn_permute_97 = ttnn.permute(ttnn_to_device_229, (0, 3, 1, 2), )
  ttnn_from_torch_187 = ttnn.from_torch(arg375_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_75 = ttnn.add(ttnn_from_torch_187, 0.001, )
  ttnn_rsqrt_34 = ttnn.rsqrt(ttnn_add_75, )
  ttnn_from_device_350 = ttnn.from_device(ttnn_rsqrt_34, )
  ttnn_to_layout_382 = ttnn.to_layout(ttnn_from_device_350, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_230 = ttnn.reshape(ttnn_to_layout_382, (1, 672, 1, 1), )
  ttnn_from_torch_188 = ttnn.from_torch(arg374_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_231 = ttnn.reshape(ttnn_from_torch_188, (1, 672, 1, 1), )
  ttnn_from_device_351 = ttnn.from_device(ttnn_reshape_231, )
  ttnn_to_layout_383 = ttnn.to_layout(ttnn_from_device_351, ttnn.TILE_LAYOUT, )
  ttnn_to_device_230 = ttnn.to_device(ttnn_to_layout_383, device = device)
  ttnn_subtract_34 = ttnn.subtract(ttnn_permute_97, ttnn_to_device_230, )
  ttnn_from_device_352 = ttnn.from_device(ttnn_reshape_230, )
  ttnn_to_layout_384 = ttnn.to_layout(ttnn_from_device_352, ttnn.TILE_LAYOUT, )
  ttnn_to_device_231 = ttnn.to_device(ttnn_to_layout_384, device = device)
  ttnn_multiply_72 = ttnn.multiply(ttnn_subtract_34, ttnn_to_device_231, )
  ttnn_from_torch_189 = ttnn.from_torch(arg119_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_232 = ttnn.reshape(ttnn_from_torch_189, (1, 672, 1, 1), )
  ttnn_from_device_353 = ttnn.from_device(ttnn_reshape_232, )
  ttnn_to_layout_385 = ttnn.to_layout(ttnn_from_device_353, ttnn.TILE_LAYOUT, )
  ttnn_to_device_232 = ttnn.to_device(ttnn_to_layout_385, device = device)
  ttnn_multiply_73 = ttnn.multiply(ttnn_multiply_72, ttnn_to_device_232, )
  ttnn_from_torch_190 = ttnn.from_torch(arg120_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_233 = ttnn.reshape(ttnn_from_torch_190, (1, 672, 1, 1), )
  ttnn_from_device_354 = ttnn.from_device(ttnn_reshape_233, )
  ttnn_to_layout_386 = ttnn.to_layout(ttnn_from_device_354, ttnn.TILE_LAYOUT, )
  ttnn_to_device_233 = ttnn.to_device(ttnn_to_layout_386, device = device)
  ttnn_add_76 = ttnn.add(ttnn_multiply_73, ttnn_to_device_233, )
  ttnn_prefix_pack_to_tuple_34 = pack_to_tuple_wrapper(ttnn_add_76, )
  ttnn_prefix_getitem_34 = ttnn_prefix_pack_to_tuple_34[0]
  ttnn_hardswish_12 = ttnn.hardswish(ttnn_prefix_getitem_34, )
  ttnn_mean_4 = ttnn.mean(ttnn_hardswish_12, (-1, -2), )
  ttnn_permute_98 = ttnn.permute(ttnn_mean_4, (0, 2, 3, 1), )
  ttnn_from_device_355 = ttnn.from_device(ttnn_permute_98, )
  ttnn_to_layout_387 = ttnn.to_layout(ttnn_from_device_355, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_234 = ttnn.reshape(ttnn_to_layout_387, (1, 1, 1, 672), )
  ttnn_from_torch_191 = ttnn.from_torch(arg122_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_235 = ttnn.reshape(ttnn_from_torch_191, (1, 1, 1, 168), )
  ttnn_from_device_356 = ttnn.from_device(ttnn_reshape_235, )
  ttnn_to_layout_388 = ttnn.to_layout(ttnn_from_device_356, ttnn.TILE_LAYOUT, )
  ttnn_to_device_234 = ttnn.to_device(ttnn_to_layout_388, device = device)
  ttnn_prefix_move_to_host_8 = move_to_host_wrapper(ttnn_to_device_234, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_357 = ttnn.from_device(ttnn_reshape_234, )
  ttnn_to_layout_389 = ttnn.to_layout(ttnn_from_device_357, ttnn.TILE_LAYOUT, )
  ttnn_to_device_235 = ttnn.to_device(ttnn_to_layout_389, device = device)
  ttnn_from_torch_192 = ttnn.from_torch(arg121_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_43 = conv_wrapper(ttnn_to_device_235, ttnn_from_torch_192, ttnn_prefix_move_to_host_8, 1, 672, 168, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_43 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_43, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_358 = ttnn.from_device(ttnn_sharded_to_interleaved_43, )
  ttnn_to_layout_390 = ttnn.to_layout(ttnn_from_device_358, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_236 = ttnn.reshape(ttnn_to_layout_390, (1, 1, 1, 168), )
  ttnn_from_device_359 = ttnn.from_device(ttnn_reshape_236, )
  ttnn_to_layout_391 = ttnn.to_layout(ttnn_from_device_359, ttnn.TILE_LAYOUT, )
  ttnn_to_device_236 = ttnn.to_device(ttnn_to_layout_391, device = device)
  ttnn_permute_99 = ttnn.permute(ttnn_to_device_236, (0, 3, 1, 2), )
  ttnn_relu_15 = ttnn.relu(ttnn_permute_99, )
  ttnn_permute_100 = ttnn.permute(ttnn_relu_15, (0, 2, 3, 1), )
  ttnn_from_device_360 = ttnn.from_device(ttnn_permute_100, )
  ttnn_to_layout_392 = ttnn.to_layout(ttnn_from_device_360, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_237 = ttnn.reshape(ttnn_to_layout_392, (1, 1, 1, 168), )
  ttnn_from_torch_193 = ttnn.from_torch(arg124_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_238 = ttnn.reshape(ttnn_from_torch_193, (1, 1, 1, 672), )
  ttnn_from_device_361 = ttnn.from_device(ttnn_reshape_238, )
  ttnn_to_layout_393 = ttnn.to_layout(ttnn_from_device_361, ttnn.TILE_LAYOUT, )
  ttnn_to_device_237 = ttnn.to_device(ttnn_to_layout_393, device = device)
  ttnn_prefix_move_to_host_9 = move_to_host_wrapper(ttnn_to_device_237, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_362 = ttnn.from_device(ttnn_reshape_237, )
  ttnn_to_layout_394 = ttnn.to_layout(ttnn_from_device_362, ttnn.TILE_LAYOUT, )
  ttnn_to_device_238 = ttnn.to_device(ttnn_to_layout_394, device = device)
  ttnn_from_torch_194 = ttnn.from_torch(arg123_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_44 = conv_wrapper(ttnn_to_device_238, ttnn_from_torch_194, ttnn_prefix_move_to_host_9, 1, 168, 672, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_44 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_44, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_363 = ttnn.from_device(ttnn_sharded_to_interleaved_44, )
  ttnn_to_layout_395 = ttnn.to_layout(ttnn_from_device_363, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_239 = ttnn.reshape(ttnn_to_layout_395, (1, 1, 1, 672), )
  ttnn_from_device_364 = ttnn.from_device(ttnn_reshape_239, )
  ttnn_to_layout_396 = ttnn.to_layout(ttnn_from_device_364, ttnn.TILE_LAYOUT, )
  ttnn_to_device_239 = ttnn.to_device(ttnn_to_layout_396, device = device)
  ttnn_permute_101 = ttnn.permute(ttnn_to_device_239, (0, 3, 1, 2), )
  ttnn_hardsigmoid_4 = ttnn.hardsigmoid(ttnn_permute_101, )
  ttnn_multiply_74 = ttnn.multiply(ttnn_hardsigmoid_4, ttnn_hardswish_12, )
  test_accuracy(mul_12, ttnn_multiply_74)
  ttnn_permute_102 = ttnn.permute(ttnn_multiply_74, (0, 2, 3, 1), )
  ttnn_from_device_365 = ttnn.from_device(ttnn_permute_102, )
  ttnn_to_layout_397 = ttnn.to_layout(ttnn_from_device_365, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_240 = ttnn.reshape(ttnn_to_layout_397, (1, 1, 400, 672), )
  ttnn_from_device_366 = ttnn.from_device(ttnn_reshape_240, )
  ttnn_to_layout_398 = ttnn.to_layout(ttnn_from_device_366, ttnn.TILE_LAYOUT, )
  ttnn_to_device_240 = ttnn.to_device(ttnn_to_layout_398, device = device)
  ttnn_from_torch_195 = ttnn.from_torch(arg125_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_45 = conv_wrapper(ttnn_to_device_240, ttnn_from_torch_195, None, 1, 672, 112, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_45 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_45, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_367 = ttnn.from_device(ttnn_sharded_to_interleaved_45, )
  ttnn_to_layout_399 = ttnn.to_layout(ttnn_from_device_367, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_241 = ttnn.reshape(ttnn_to_layout_399, (1, 20, 20, 112), )
  ttnn_from_device_368 = ttnn.from_device(ttnn_reshape_241, )
  ttnn_to_layout_400 = ttnn.to_layout(ttnn_from_device_368, ttnn.TILE_LAYOUT, )
  ttnn_to_device_241 = ttnn.to_device(ttnn_to_layout_400, device = device)
  ttnn_permute_103 = ttnn.permute(ttnn_to_device_241, (0, 3, 1, 2), )
  test_accuracy(convolution_45, ttnn_permute_103)
  ttnn_from_torch_196 = ttnn.from_torch(arg378_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_77 = ttnn.add(ttnn_from_torch_196, 0.001, )
  ttnn_rsqrt_35 = ttnn.rsqrt(ttnn_add_77, )
  ttnn_from_device_369 = ttnn.from_device(ttnn_rsqrt_35, )
  ttnn_to_layout_401 = ttnn.to_layout(ttnn_from_device_369, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_242 = ttnn.reshape(ttnn_to_layout_401, (1, 112, 1, 1), )
  ttnn_from_torch_197 = ttnn.from_torch(arg377_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_243 = ttnn.reshape(ttnn_from_torch_197, (1, 112, 1, 1), )
  ttnn_from_device_370 = ttnn.from_device(ttnn_reshape_243, )
  ttnn_to_layout_402 = ttnn.to_layout(ttnn_from_device_370, ttnn.TILE_LAYOUT, )
  ttnn_to_device_242 = ttnn.to_device(ttnn_to_layout_402, device = device)
  ttnn_subtract_35 = ttnn.subtract(ttnn_permute_103, ttnn_to_device_242, )
  ttnn_from_device_371 = ttnn.from_device(ttnn_reshape_242, )
  ttnn_to_layout_403 = ttnn.to_layout(ttnn_from_device_371, ttnn.TILE_LAYOUT, )
  ttnn_to_device_243 = ttnn.to_device(ttnn_to_layout_403, device = device)
  ttnn_multiply_75 = ttnn.multiply(ttnn_subtract_35, ttnn_to_device_243, )
  ttnn_from_torch_198 = ttnn.from_torch(arg126_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_244 = ttnn.reshape(ttnn_from_torch_198, (1, 112, 1, 1), )
  ttnn_from_device_372 = ttnn.from_device(ttnn_reshape_244, )
  ttnn_to_layout_404 = ttnn.to_layout(ttnn_from_device_372, ttnn.TILE_LAYOUT, )
  ttnn_to_device_244 = ttnn.to_device(ttnn_to_layout_404, device = device)
  ttnn_multiply_76 = ttnn.multiply(ttnn_multiply_75, ttnn_to_device_244, )
  ttnn_from_torch_199 = ttnn.from_torch(arg127_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_245 = ttnn.reshape(ttnn_from_torch_199, (1, 112, 1, 1), )
  ttnn_from_device_373 = ttnn.from_device(ttnn_reshape_245, )
  ttnn_to_layout_405 = ttnn.to_layout(ttnn_from_device_373, ttnn.TILE_LAYOUT, )
  ttnn_to_device_245 = ttnn.to_device(ttnn_to_layout_405, device = device)
  ttnn_add_78 = ttnn.add(ttnn_multiply_76, ttnn_to_device_245, )
  ttnn_prefix_pack_to_tuple_35 = pack_to_tuple_wrapper(ttnn_add_78, )
  ttnn_prefix_getitem_35 = ttnn_prefix_pack_to_tuple_35[0]
  test_accuracy(getitem_105, ttnn_prefix_getitem_35)
  ttnn_add_79 = ttnn.add(ttnn_prefix_getitem_35, ttnn_prefix_getitem_32, )
  test_accuracy(add_12, ttnn_add_79)
  ttnn_permute_104 = ttnn.permute(ttnn_add_79, (0, 2, 3, 1), )
  ttnn_from_device_374 = ttnn.from_device(ttnn_permute_104, )
  ttnn_to_layout_406 = ttnn.to_layout(ttnn_from_device_374, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_246 = ttnn.reshape(ttnn_to_layout_406, (1, 1, 400, 112), )
  ttnn_from_device_375 = ttnn.from_device(ttnn_reshape_246, )
  ttnn_to_layout_407 = ttnn.to_layout(ttnn_from_device_375, ttnn.TILE_LAYOUT, )
  ttnn_to_device_246 = ttnn.to_device(ttnn_to_layout_407, device = device)
  ttnn_from_torch_200 = ttnn.from_torch(arg128_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_46 = conv_wrapper(ttnn_to_device_246, ttnn_from_torch_200, None, 1, 112, 672, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_46 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_46, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_376 = ttnn.from_device(ttnn_sharded_to_interleaved_46, )
  ttnn_to_layout_408 = ttnn.to_layout(ttnn_from_device_376, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_247 = ttnn.reshape(ttnn_to_layout_408, (1, 20, 20, 672), )
  ttnn_from_device_377 = ttnn.from_device(ttnn_reshape_247, )
  ttnn_to_layout_409 = ttnn.to_layout(ttnn_from_device_377, ttnn.TILE_LAYOUT, )
  ttnn_to_device_247 = ttnn.to_device(ttnn_to_layout_409, device = device)
  ttnn_permute_105 = ttnn.permute(ttnn_to_device_247, (0, 3, 1, 2), )
  ttnn_from_torch_201 = ttnn.from_torch(arg381_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_80 = ttnn.add(ttnn_from_torch_201, 0.001, )
  ttnn_rsqrt_36 = ttnn.rsqrt(ttnn_add_80, )
  ttnn_from_device_378 = ttnn.from_device(ttnn_rsqrt_36, )
  ttnn_to_layout_410 = ttnn.to_layout(ttnn_from_device_378, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_248 = ttnn.reshape(ttnn_to_layout_410, (1, 672, 1, 1), )
  ttnn_from_torch_202 = ttnn.from_torch(arg380_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_249 = ttnn.reshape(ttnn_from_torch_202, (1, 672, 1, 1), )
  ttnn_from_device_379 = ttnn.from_device(ttnn_reshape_249, )
  ttnn_to_layout_411 = ttnn.to_layout(ttnn_from_device_379, ttnn.TILE_LAYOUT, )
  ttnn_to_device_248 = ttnn.to_device(ttnn_to_layout_411, device = device)
  ttnn_subtract_36 = ttnn.subtract(ttnn_permute_105, ttnn_to_device_248, )
  ttnn_from_device_380 = ttnn.from_device(ttnn_reshape_248, )
  ttnn_to_layout_412 = ttnn.to_layout(ttnn_from_device_380, ttnn.TILE_LAYOUT, )
  ttnn_to_device_249 = ttnn.to_device(ttnn_to_layout_412, device = device)
  ttnn_multiply_77 = ttnn.multiply(ttnn_subtract_36, ttnn_to_device_249, )
  ttnn_from_torch_203 = ttnn.from_torch(arg129_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_250 = ttnn.reshape(ttnn_from_torch_203, (1, 672, 1, 1), )
  ttnn_from_device_381 = ttnn.from_device(ttnn_reshape_250, )
  ttnn_to_layout_413 = ttnn.to_layout(ttnn_from_device_381, ttnn.TILE_LAYOUT, )
  ttnn_to_device_250 = ttnn.to_device(ttnn_to_layout_413, device = device)
  ttnn_multiply_78 = ttnn.multiply(ttnn_multiply_77, ttnn_to_device_250, )
  ttnn_from_torch_204 = ttnn.from_torch(arg130_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_251 = ttnn.reshape(ttnn_from_torch_204, (1, 672, 1, 1), )
  ttnn_from_device_382 = ttnn.from_device(ttnn_reshape_251, )
  ttnn_to_layout_414 = ttnn.to_layout(ttnn_from_device_382, ttnn.TILE_LAYOUT, )
  ttnn_to_device_251 = ttnn.to_device(ttnn_to_layout_414, device = device)
  ttnn_add_81 = ttnn.add(ttnn_multiply_78, ttnn_to_device_251, )
  ttnn_prefix_pack_to_tuple_36 = pack_to_tuple_wrapper(ttnn_add_81, )
  ttnn_prefix_getitem_36 = ttnn_prefix_pack_to_tuple_36[0]
  ttnn_hardswish_13 = ttnn.hardswish(ttnn_prefix_getitem_36, )
  test_accuracy(hardswish_13, ttnn_hardswish_13)
  ttnn_permute_106 = ttnn.permute(ttnn_hardswish_13, (0, 2, 3, 1), )
  ttnn_from_device_383 = ttnn.from_device(ttnn_permute_106, )
  ttnn_to_layout_415 = ttnn.to_layout(ttnn_from_device_383, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_252 = ttnn.reshape(ttnn_to_layout_415, (1, 1, 400, 672), )
  ttnn_from_device_384 = ttnn.from_device(ttnn_reshape_252, )
  ttnn_to_layout_416 = ttnn.to_layout(ttnn_from_device_384, ttnn.TILE_LAYOUT, )
  ttnn_to_device_252 = ttnn.to_device(ttnn_to_layout_416, device = device)
  ttnn_from_torch_205 = ttnn.from_torch(arg131_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_47 = conv_wrapper(ttnn_to_device_252, ttnn_from_torch_205, None, 1, 672, 672, [20, 20], [5, 5], [2, 2], [2, 2], [1, 1], 672, device, False, None, )
  ttnn_sharded_to_interleaved_47 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_47, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_385 = ttnn.from_device(ttnn_sharded_to_interleaved_47, )
  ttnn_to_layout_417 = ttnn.to_layout(ttnn_from_device_385, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_253 = ttnn.reshape(ttnn_to_layout_417, (1, 10, 10, 672), )
  ttnn_from_device_386 = ttnn.from_device(ttnn_reshape_253, )
  ttnn_to_layout_418 = ttnn.to_layout(ttnn_from_device_386, ttnn.TILE_LAYOUT, )
  ttnn_to_device_253 = ttnn.to_device(ttnn_to_layout_418, device = device)
  ttnn_permute_107 = ttnn.permute(ttnn_to_device_253, (0, 3, 1, 2), )
  test_accuracy(convolution_47, ttnn_permute_107)
  ttnn_from_torch_206 = ttnn.from_torch(arg384_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_82 = ttnn.add(ttnn_from_torch_206, 0.001, )
  ttnn_rsqrt_37 = ttnn.rsqrt(ttnn_add_82, )
  ttnn_from_device_387 = ttnn.from_device(ttnn_rsqrt_37, )
  ttnn_to_layout_419 = ttnn.to_layout(ttnn_from_device_387, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_254 = ttnn.reshape(ttnn_to_layout_419, (1, 672, 1, 1), )
  ttnn_from_torch_207 = ttnn.from_torch(arg383_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_255 = ttnn.reshape(ttnn_from_torch_207, (1, 672, 1, 1), )
  ttnn_from_device_388 = ttnn.from_device(ttnn_reshape_255, )
  ttnn_to_layout_420 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_254 = ttnn.to_device(ttnn_to_layout_420, device = device)
  ttnn_subtract_37 = ttnn.subtract(ttnn_permute_107, ttnn_to_device_254, )
  ttnn_from_device_389 = ttnn.from_device(ttnn_reshape_254, )
  ttnn_to_layout_421 = ttnn.to_layout(ttnn_from_device_389, ttnn.TILE_LAYOUT, )
  ttnn_to_device_255 = ttnn.to_device(ttnn_to_layout_421, device = device)
  ttnn_multiply_79 = ttnn.multiply(ttnn_subtract_37, ttnn_to_device_255, )
  ttnn_from_torch_208 = ttnn.from_torch(arg132_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_256 = ttnn.reshape(ttnn_from_torch_208, (1, 672, 1, 1), )
  ttnn_from_device_390 = ttnn.from_device(ttnn_reshape_256, )
  ttnn_to_layout_422 = ttnn.to_layout(ttnn_from_device_390, ttnn.TILE_LAYOUT, )
  ttnn_to_device_256 = ttnn.to_device(ttnn_to_layout_422, device = device)
  ttnn_multiply_80 = ttnn.multiply(ttnn_multiply_79, ttnn_to_device_256, )
  ttnn_from_torch_209 = ttnn.from_torch(arg133_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_257 = ttnn.reshape(ttnn_from_torch_209, (1, 672, 1, 1), )
  ttnn_from_device_391 = ttnn.from_device(ttnn_reshape_257, )
  ttnn_to_layout_423 = ttnn.to_layout(ttnn_from_device_391, ttnn.TILE_LAYOUT, )
  ttnn_to_device_257 = ttnn.to_device(ttnn_to_layout_423, device = device)
  ttnn_add_83 = ttnn.add(ttnn_multiply_80, ttnn_to_device_257, )
  ttnn_prefix_pack_to_tuple_37 = pack_to_tuple_wrapper(ttnn_add_83, )
  ttnn_prefix_getitem_37 = ttnn_prefix_pack_to_tuple_37[0]
  test_accuracy(getitem_111, ttnn_prefix_getitem_37)
  ttnn_hardswish_14 = ttnn.hardswish(ttnn_prefix_getitem_37, )
  test_accuracy(hardswish_14, ttnn_hardswish_14)
  ttnn_mean_5 = ttnn.mean(ttnn_hardswish_14, (-1, -2), )
  test_accuracy(mean_5, ttnn_mean_5)
  ttnn_permute_108 = ttnn.permute(ttnn_mean_5, (0, 2, 3, 1), )
  ttnn_from_device_392 = ttnn.from_device(ttnn_permute_108, )
  ttnn_to_layout_424 = ttnn.to_layout(ttnn_from_device_392, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_258 = ttnn.reshape(ttnn_to_layout_424, (1, 1, 1, 672), )
  ttnn_from_torch_210 = ttnn.from_torch(arg135_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_259 = ttnn.reshape(ttnn_from_torch_210, (1, 1, 1, 168), )
  ttnn_from_device_393 = ttnn.from_device(ttnn_reshape_259, )
  ttnn_to_layout_425 = ttnn.to_layout(ttnn_from_device_393, ttnn.TILE_LAYOUT, )
  ttnn_to_device_258 = ttnn.to_device(ttnn_to_layout_425, device = device)
  ttnn_prefix_move_to_host_10 = move_to_host_wrapper(ttnn_to_device_258, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_394 = ttnn.from_device(ttnn_reshape_258, )
  ttnn_to_layout_426 = ttnn.to_layout(ttnn_from_device_394, ttnn.TILE_LAYOUT, )
  ttnn_to_device_259 = ttnn.to_device(ttnn_to_layout_426, device = device)
  ttnn_from_torch_211 = ttnn.from_torch(arg134_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_48 = conv_wrapper(ttnn_to_device_259, ttnn_from_torch_211, ttnn_prefix_move_to_host_10, 1, 672, 168, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_48 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_48, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_395 = ttnn.from_device(ttnn_sharded_to_interleaved_48, )
  ttnn_to_layout_427 = ttnn.to_layout(ttnn_from_device_395, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_260 = ttnn.reshape(ttnn_to_layout_427, (1, 1, 1, 168), )
  ttnn_from_device_396 = ttnn.from_device(ttnn_reshape_260, )
  ttnn_to_layout_428 = ttnn.to_layout(ttnn_from_device_396, ttnn.TILE_LAYOUT, )
  ttnn_to_device_260 = ttnn.to_device(ttnn_to_layout_428, device = device)
  ttnn_permute_109 = ttnn.permute(ttnn_to_device_260, (0, 3, 1, 2), )
  test_accuracy(convolution_48, ttnn_permute_109)
  ttnn_relu_16 = ttnn.relu(ttnn_permute_109, )
  test_accuracy(relu_16, ttnn_relu_16)
  ttnn_permute_110 = ttnn.permute(ttnn_relu_16, (0, 2, 3, 1), )
  ttnn_from_device_397 = ttnn.from_device(ttnn_permute_110, )
  ttnn_to_layout_429 = ttnn.to_layout(ttnn_from_device_397, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_261 = ttnn.reshape(ttnn_to_layout_429, (1, 1, 1, 168), )
  ttnn_from_torch_212 = ttnn.from_torch(arg137_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_262 = ttnn.reshape(ttnn_from_torch_212, (1, 1, 1, 672), )
  ttnn_from_device_398 = ttnn.from_device(ttnn_reshape_262, )
  ttnn_to_layout_430 = ttnn.to_layout(ttnn_from_device_398, ttnn.TILE_LAYOUT, )
  ttnn_to_device_261 = ttnn.to_device(ttnn_to_layout_430, device = device)
  ttnn_prefix_move_to_host_11 = move_to_host_wrapper(ttnn_to_device_261, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_399 = ttnn.from_device(ttnn_reshape_261, )
  ttnn_to_layout_431 = ttnn.to_layout(ttnn_from_device_399, ttnn.TILE_LAYOUT, )
  ttnn_to_device_262 = ttnn.to_device(ttnn_to_layout_431, device = device)
  ttnn_from_torch_213 = ttnn.from_torch(arg136_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_49 = conv_wrapper(ttnn_to_device_262, ttnn_from_torch_213, ttnn_prefix_move_to_host_11, 1, 168, 672, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_49 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_49, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_400 = ttnn.from_device(ttnn_sharded_to_interleaved_49, )
  ttnn_to_layout_432 = ttnn.to_layout(ttnn_from_device_400, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_263 = ttnn.reshape(ttnn_to_layout_432, (1, 1, 1, 672), )
  ttnn_from_device_401 = ttnn.from_device(ttnn_reshape_263, )
  ttnn_to_layout_433 = ttnn.to_layout(ttnn_from_device_401, ttnn.TILE_LAYOUT, )
  ttnn_to_device_263 = ttnn.to_device(ttnn_to_layout_433, device = device)
  ttnn_permute_111 = ttnn.permute(ttnn_to_device_263, (0, 3, 1, 2), )
  test_accuracy(convolution_49, ttnn_permute_111)
  ttnn_hardsigmoid_5 = ttnn.hardsigmoid(ttnn_permute_111, )
  test_accuracy(hardsigmoid_5, ttnn_hardsigmoid_5)
  ttnn_multiply_81 = ttnn.multiply(ttnn_hardsigmoid_5, ttnn_hardswish_14, )
  test_accuracy(mul_13, ttnn_multiply_81)
  ttnn_permute_112 = ttnn.permute(ttnn_multiply_81, (0, 2, 3, 1), )
  ttnn_from_device_402 = ttnn.from_device(ttnn_permute_112, )
  ttnn_to_layout_434 = ttnn.to_layout(ttnn_from_device_402, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_264 = ttnn.reshape(ttnn_to_layout_434, (1, 1, 100, 672), )
  ttnn_from_device_403 = ttnn.from_device(ttnn_reshape_264, )
  ttnn_to_layout_435 = ttnn.to_layout(ttnn_from_device_403, ttnn.TILE_LAYOUT, )
  ttnn_to_device_264 = ttnn.to_device(ttnn_to_layout_435, device = device)
  ttnn_from_torch_214 = ttnn.from_torch(arg138_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_50 = conv_wrapper(ttnn_to_device_264, ttnn_from_torch_214, None, 1, 672, 80, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_50 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_50, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_404 = ttnn.from_device(ttnn_sharded_to_interleaved_50, )
  ttnn_to_layout_436 = ttnn.to_layout(ttnn_from_device_404, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_265 = ttnn.reshape(ttnn_to_layout_436, (1, 10, 10, 80), )
  ttnn_from_device_405 = ttnn.from_device(ttnn_reshape_265, )
  ttnn_to_layout_437 = ttnn.to_layout(ttnn_from_device_405, ttnn.TILE_LAYOUT, )
  ttnn_to_device_265 = ttnn.to_device(ttnn_to_layout_437, device = device)
  ttnn_permute_113 = ttnn.permute(ttnn_to_device_265, (0, 3, 1, 2), )
  ttnn_from_torch_215 = ttnn.from_torch(arg387_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_84 = ttnn.add(ttnn_from_torch_215, 0.001, )
  ttnn_rsqrt_38 = ttnn.rsqrt(ttnn_add_84, )
  ttnn_from_device_406 = ttnn.from_device(ttnn_rsqrt_38, )
  ttnn_to_layout_438 = ttnn.to_layout(ttnn_from_device_406, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_266 = ttnn.reshape(ttnn_to_layout_438, (1, 80, 1, 1), )
  ttnn_from_torch_216 = ttnn.from_torch(arg386_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_267 = ttnn.reshape(ttnn_from_torch_216, (1, 80, 1, 1), )
  ttnn_from_device_407 = ttnn.from_device(ttnn_reshape_267, )
  ttnn_to_layout_439 = ttnn.to_layout(ttnn_from_device_407, ttnn.TILE_LAYOUT, )
  ttnn_to_device_266 = ttnn.to_device(ttnn_to_layout_439, device = device)
  ttnn_subtract_38 = ttnn.subtract(ttnn_permute_113, ttnn_to_device_266, )
  ttnn_from_device_408 = ttnn.from_device(ttnn_reshape_266, )
  ttnn_to_layout_440 = ttnn.to_layout(ttnn_from_device_408, ttnn.TILE_LAYOUT, )
  ttnn_to_device_267 = ttnn.to_device(ttnn_to_layout_440, device = device)
  ttnn_multiply_82 = ttnn.multiply(ttnn_subtract_38, ttnn_to_device_267, )
  ttnn_from_torch_217 = ttnn.from_torch(arg139_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_268 = ttnn.reshape(ttnn_from_torch_217, (1, 80, 1, 1), )
  ttnn_from_device_409 = ttnn.from_device(ttnn_reshape_268, )
  ttnn_to_layout_441 = ttnn.to_layout(ttnn_from_device_409, ttnn.TILE_LAYOUT, )
  ttnn_to_device_268 = ttnn.to_device(ttnn_to_layout_441, device = device)
  ttnn_multiply_83 = ttnn.multiply(ttnn_multiply_82, ttnn_to_device_268, )
  ttnn_from_torch_218 = ttnn.from_torch(arg140_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_269 = ttnn.reshape(ttnn_from_torch_218, (1, 80, 1, 1), )
  ttnn_from_device_410 = ttnn.from_device(ttnn_reshape_269, )
  ttnn_to_layout_442 = ttnn.to_layout(ttnn_from_device_410, ttnn.TILE_LAYOUT, )
  ttnn_to_device_269 = ttnn.to_device(ttnn_to_layout_442, device = device)
  ttnn_add_85 = ttnn.add(ttnn_multiply_83, ttnn_to_device_269, )
  ttnn_prefix_pack_to_tuple_38 = pack_to_tuple_wrapper(ttnn_add_85, )
  ttnn_prefix_getitem_38 = ttnn_prefix_pack_to_tuple_38[0]
  ttnn_permute_114 = ttnn.permute(ttnn_prefix_getitem_38, (0, 2, 3, 1), )
  ttnn_from_device_411 = ttnn.from_device(ttnn_permute_114, )
  ttnn_to_layout_443 = ttnn.to_layout(ttnn_from_device_411, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_270 = ttnn.reshape(ttnn_to_layout_443, (1, 1, 100, 80), )
  ttnn_from_device_412 = ttnn.from_device(ttnn_reshape_270, )
  ttnn_to_layout_444 = ttnn.to_layout(ttnn_from_device_412, ttnn.TILE_LAYOUT, )
  ttnn_to_device_270 = ttnn.to_device(ttnn_to_layout_444, device = device)
  ttnn_from_torch_219 = ttnn.from_torch(arg141_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_51 = conv_wrapper(ttnn_to_device_270, ttnn_from_torch_219, None, 1, 80, 480, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_51 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_51, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_413 = ttnn.from_device(ttnn_sharded_to_interleaved_51, )
  ttnn_to_layout_445 = ttnn.to_layout(ttnn_from_device_413, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_271 = ttnn.reshape(ttnn_to_layout_445, (1, 10, 10, 480), )
  ttnn_from_device_414 = ttnn.from_device(ttnn_reshape_271, )
  ttnn_to_layout_446 = ttnn.to_layout(ttnn_from_device_414, ttnn.TILE_LAYOUT, )
  ttnn_to_device_271 = ttnn.to_device(ttnn_to_layout_446, device = device)
  ttnn_permute_115 = ttnn.permute(ttnn_to_device_271, (0, 3, 1, 2), )
  ttnn_from_torch_220 = ttnn.from_torch(arg390_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_86 = ttnn.add(ttnn_from_torch_220, 0.001, )
  ttnn_rsqrt_39 = ttnn.rsqrt(ttnn_add_86, )
  ttnn_from_device_415 = ttnn.from_device(ttnn_rsqrt_39, )
  ttnn_to_layout_447 = ttnn.to_layout(ttnn_from_device_415, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_272 = ttnn.reshape(ttnn_to_layout_447, (1, 480, 1, 1), )
  ttnn_from_torch_221 = ttnn.from_torch(arg389_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_273 = ttnn.reshape(ttnn_from_torch_221, (1, 480, 1, 1), )
  ttnn_from_device_416 = ttnn.from_device(ttnn_reshape_273, )
  ttnn_to_layout_448 = ttnn.to_layout(ttnn_from_device_416, ttnn.TILE_LAYOUT, )
  ttnn_to_device_272 = ttnn.to_device(ttnn_to_layout_448, device = device)
  ttnn_subtract_39 = ttnn.subtract(ttnn_permute_115, ttnn_to_device_272, )
  ttnn_from_device_417 = ttnn.from_device(ttnn_reshape_272, )
  ttnn_to_layout_449 = ttnn.to_layout(ttnn_from_device_417, ttnn.TILE_LAYOUT, )
  ttnn_to_device_273 = ttnn.to_device(ttnn_to_layout_449, device = device)
  ttnn_multiply_84 = ttnn.multiply(ttnn_subtract_39, ttnn_to_device_273, )
  ttnn_from_torch_222 = ttnn.from_torch(arg142_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_274 = ttnn.reshape(ttnn_from_torch_222, (1, 480, 1, 1), )
  ttnn_from_device_418 = ttnn.from_device(ttnn_reshape_274, )
  ttnn_to_layout_450 = ttnn.to_layout(ttnn_from_device_418, ttnn.TILE_LAYOUT, )
  ttnn_to_device_274 = ttnn.to_device(ttnn_to_layout_450, device = device)
  ttnn_multiply_85 = ttnn.multiply(ttnn_multiply_84, ttnn_to_device_274, )
  ttnn_from_torch_223 = ttnn.from_torch(arg143_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_275 = ttnn.reshape(ttnn_from_torch_223, (1, 480, 1, 1), )
  ttnn_from_device_419 = ttnn.from_device(ttnn_reshape_275, )
  ttnn_to_layout_451 = ttnn.to_layout(ttnn_from_device_419, ttnn.TILE_LAYOUT, )
  ttnn_to_device_275 = ttnn.to_device(ttnn_to_layout_451, device = device)
  ttnn_add_87 = ttnn.add(ttnn_multiply_85, ttnn_to_device_275, )
  ttnn_prefix_pack_to_tuple_39 = pack_to_tuple_wrapper(ttnn_add_87, )
  ttnn_prefix_getitem_39 = ttnn_prefix_pack_to_tuple_39[0]
  ttnn_hardswish_15 = ttnn.hardswish(ttnn_prefix_getitem_39, )
  ttnn_permute_116 = ttnn.permute(ttnn_hardswish_15, (0, 2, 3, 1), )
  ttnn_from_device_420 = ttnn.from_device(ttnn_permute_116, )
  ttnn_to_layout_452 = ttnn.to_layout(ttnn_from_device_420, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_276 = ttnn.reshape(ttnn_to_layout_452, (1, 1, 100, 480), )
  ttnn_from_device_421 = ttnn.from_device(ttnn_reshape_276, )
  ttnn_to_layout_453 = ttnn.to_layout(ttnn_from_device_421, ttnn.TILE_LAYOUT, )
  ttnn_to_device_276 = ttnn.to_device(ttnn_to_layout_453, device = device)
  ttnn_from_torch_224 = ttnn.from_torch(arg144_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_52 = conv_wrapper(ttnn_to_device_276, ttnn_from_torch_224, None, 1, 480, 480, [10, 10], [5, 5], [1, 1], [2, 2], [1, 1], 480, device, False, None, )
  ttnn_sharded_to_interleaved_52 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_52, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_422 = ttnn.from_device(ttnn_sharded_to_interleaved_52, )
  ttnn_to_layout_454 = ttnn.to_layout(ttnn_from_device_422, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_277 = ttnn.reshape(ttnn_to_layout_454, (1, 10, 10, 480), )
  ttnn_from_device_423 = ttnn.from_device(ttnn_reshape_277, )
  ttnn_to_layout_455 = ttnn.to_layout(ttnn_from_device_423, ttnn.TILE_LAYOUT, )
  ttnn_to_device_277 = ttnn.to_device(ttnn_to_layout_455, device = device)
  ttnn_permute_117 = ttnn.permute(ttnn_to_device_277, (0, 3, 1, 2), )
  ttnn_from_torch_225 = ttnn.from_torch(arg393_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_88 = ttnn.add(ttnn_from_torch_225, 0.001, )
  ttnn_rsqrt_40 = ttnn.rsqrt(ttnn_add_88, )
  ttnn_from_device_424 = ttnn.from_device(ttnn_rsqrt_40, )
  ttnn_to_layout_456 = ttnn.to_layout(ttnn_from_device_424, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_278 = ttnn.reshape(ttnn_to_layout_456, (1, 480, 1, 1), )
  ttnn_from_torch_226 = ttnn.from_torch(arg392_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_279 = ttnn.reshape(ttnn_from_torch_226, (1, 480, 1, 1), )
  ttnn_from_device_425 = ttnn.from_device(ttnn_reshape_279, )
  ttnn_to_layout_457 = ttnn.to_layout(ttnn_from_device_425, ttnn.TILE_LAYOUT, )
  ttnn_to_device_278 = ttnn.to_device(ttnn_to_layout_457, device = device)
  ttnn_subtract_40 = ttnn.subtract(ttnn_permute_117, ttnn_to_device_278, )
  ttnn_from_device_426 = ttnn.from_device(ttnn_reshape_278, )
  ttnn_to_layout_458 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_279 = ttnn.to_device(ttnn_to_layout_458, device = device)
  ttnn_multiply_86 = ttnn.multiply(ttnn_subtract_40, ttnn_to_device_279, )
  ttnn_from_torch_227 = ttnn.from_torch(arg145_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_280 = ttnn.reshape(ttnn_from_torch_227, (1, 480, 1, 1), )
  ttnn_from_device_427 = ttnn.from_device(ttnn_reshape_280, )
  ttnn_to_layout_459 = ttnn.to_layout(ttnn_from_device_427, ttnn.TILE_LAYOUT, )
  ttnn_to_device_280 = ttnn.to_device(ttnn_to_layout_459, device = device)
  ttnn_multiply_87 = ttnn.multiply(ttnn_multiply_86, ttnn_to_device_280, )
  ttnn_from_torch_228 = ttnn.from_torch(arg146_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_281 = ttnn.reshape(ttnn_from_torch_228, (1, 480, 1, 1), )
  ttnn_from_device_428 = ttnn.from_device(ttnn_reshape_281, )
  ttnn_to_layout_460 = ttnn.to_layout(ttnn_from_device_428, ttnn.TILE_LAYOUT, )
  ttnn_to_device_281 = ttnn.to_device(ttnn_to_layout_460, device = device)
  ttnn_add_89 = ttnn.add(ttnn_multiply_87, ttnn_to_device_281, )
  ttnn_prefix_pack_to_tuple_40 = pack_to_tuple_wrapper(ttnn_add_89, )
  ttnn_prefix_getitem_40 = ttnn_prefix_pack_to_tuple_40[0]
  ttnn_hardswish_16 = ttnn.hardswish(ttnn_prefix_getitem_40, )
  ttnn_mean_6 = ttnn.mean(ttnn_hardswish_16, (-1, -2), )
  ttnn_permute_118 = ttnn.permute(ttnn_mean_6, (0, 2, 3, 1), )
  ttnn_from_device_429 = ttnn.from_device(ttnn_permute_118, )
  ttnn_to_layout_461 = ttnn.to_layout(ttnn_from_device_429, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_282 = ttnn.reshape(ttnn_to_layout_461, (1, 1, 1, 480), )
  ttnn_from_torch_229 = ttnn.from_torch(arg148_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_283 = ttnn.reshape(ttnn_from_torch_229, (1, 1, 1, 120), )
  ttnn_from_device_430 = ttnn.from_device(ttnn_reshape_283, )
  ttnn_to_layout_462 = ttnn.to_layout(ttnn_from_device_430, ttnn.TILE_LAYOUT, )
  ttnn_to_device_282 = ttnn.to_device(ttnn_to_layout_462, device = device)
  ttnn_prefix_move_to_host_12 = move_to_host_wrapper(ttnn_to_device_282, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_431 = ttnn.from_device(ttnn_reshape_282, )
  ttnn_to_layout_463 = ttnn.to_layout(ttnn_from_device_431, ttnn.TILE_LAYOUT, )
  ttnn_to_device_283 = ttnn.to_device(ttnn_to_layout_463, device = device)
  ttnn_from_torch_230 = ttnn.from_torch(arg147_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_53 = conv_wrapper(ttnn_to_device_283, ttnn_from_torch_230, ttnn_prefix_move_to_host_12, 1, 480, 120, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_53 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_53, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_432 = ttnn.from_device(ttnn_sharded_to_interleaved_53, )
  ttnn_to_layout_464 = ttnn.to_layout(ttnn_from_device_432, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_284 = ttnn.reshape(ttnn_to_layout_464, (1, 1, 1, 120), )
  ttnn_from_device_433 = ttnn.from_device(ttnn_reshape_284, )
  ttnn_to_layout_465 = ttnn.to_layout(ttnn_from_device_433, ttnn.TILE_LAYOUT, )
  ttnn_to_device_284 = ttnn.to_device(ttnn_to_layout_465, device = device)
  ttnn_permute_119 = ttnn.permute(ttnn_to_device_284, (0, 3, 1, 2), )
  ttnn_relu_17 = ttnn.relu(ttnn_permute_119, )
  ttnn_permute_120 = ttnn.permute(ttnn_relu_17, (0, 2, 3, 1), )
  ttnn_from_device_434 = ttnn.from_device(ttnn_permute_120, )
  ttnn_to_layout_466 = ttnn.to_layout(ttnn_from_device_434, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_285 = ttnn.reshape(ttnn_to_layout_466, (1, 1, 1, 120), )
  ttnn_from_torch_231 = ttnn.from_torch(arg150_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_286 = ttnn.reshape(ttnn_from_torch_231, (1, 1, 1, 480), )
  ttnn_from_device_435 = ttnn.from_device(ttnn_reshape_286, )
  ttnn_to_layout_467 = ttnn.to_layout(ttnn_from_device_435, ttnn.TILE_LAYOUT, )
  ttnn_to_device_285 = ttnn.to_device(ttnn_to_layout_467, device = device)
  ttnn_prefix_move_to_host_13 = move_to_host_wrapper(ttnn_to_device_285, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_436 = ttnn.from_device(ttnn_reshape_285, )
  ttnn_to_layout_468 = ttnn.to_layout(ttnn_from_device_436, ttnn.TILE_LAYOUT, )
  ttnn_to_device_286 = ttnn.to_device(ttnn_to_layout_468, device = device)
  ttnn_from_torch_232 = ttnn.from_torch(arg149_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_54 = conv_wrapper(ttnn_to_device_286, ttnn_from_torch_232, ttnn_prefix_move_to_host_13, 1, 120, 480, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_54 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_54, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_437 = ttnn.from_device(ttnn_sharded_to_interleaved_54, )
  ttnn_to_layout_469 = ttnn.to_layout(ttnn_from_device_437, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_287 = ttnn.reshape(ttnn_to_layout_469, (1, 1, 1, 480), )
  ttnn_from_device_438 = ttnn.from_device(ttnn_reshape_287, )
  ttnn_to_layout_470 = ttnn.to_layout(ttnn_from_device_438, ttnn.TILE_LAYOUT, )
  ttnn_to_device_287 = ttnn.to_device(ttnn_to_layout_470, device = device)
  ttnn_permute_121 = ttnn.permute(ttnn_to_device_287, (0, 3, 1, 2), )
  ttnn_hardsigmoid_6 = ttnn.hardsigmoid(ttnn_permute_121, )
  ttnn_multiply_88 = ttnn.multiply(ttnn_hardsigmoid_6, ttnn_hardswish_16, )
  ttnn_permute_122 = ttnn.permute(ttnn_multiply_88, (0, 2, 3, 1), )
  ttnn_from_device_439 = ttnn.from_device(ttnn_permute_122, )
  ttnn_to_layout_471 = ttnn.to_layout(ttnn_from_device_439, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_288 = ttnn.reshape(ttnn_to_layout_471, (1, 1, 100, 480), )
  ttnn_from_device_440 = ttnn.from_device(ttnn_reshape_288, )
  ttnn_to_layout_472 = ttnn.to_layout(ttnn_from_device_440, ttnn.TILE_LAYOUT, )
  ttnn_to_device_288 = ttnn.to_device(ttnn_to_layout_472, device = device)
  ttnn_from_torch_233 = ttnn.from_torch(arg151_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_55 = conv_wrapper(ttnn_to_device_288, ttnn_from_torch_233, None, 1, 480, 80, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_55 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_55, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_441 = ttnn.from_device(ttnn_sharded_to_interleaved_55, )
  ttnn_to_layout_473 = ttnn.to_layout(ttnn_from_device_441, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_289 = ttnn.reshape(ttnn_to_layout_473, (1, 10, 10, 80), )
  ttnn_from_device_442 = ttnn.from_device(ttnn_reshape_289, )
  ttnn_to_layout_474 = ttnn.to_layout(ttnn_from_device_442, ttnn.TILE_LAYOUT, )
  ttnn_to_device_289 = ttnn.to_device(ttnn_to_layout_474, device = device)
  ttnn_permute_123 = ttnn.permute(ttnn_to_device_289, (0, 3, 1, 2), )
  ttnn_from_torch_234 = ttnn.from_torch(arg396_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_90 = ttnn.add(ttnn_from_torch_234, 0.001, )
  ttnn_rsqrt_41 = ttnn.rsqrt(ttnn_add_90, )
  ttnn_from_device_443 = ttnn.from_device(ttnn_rsqrt_41, )
  ttnn_to_layout_475 = ttnn.to_layout(ttnn_from_device_443, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_290 = ttnn.reshape(ttnn_to_layout_475, (1, 80, 1, 1), )
  ttnn_from_torch_235 = ttnn.from_torch(arg395_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_291 = ttnn.reshape(ttnn_from_torch_235, (1, 80, 1, 1), )
  ttnn_from_device_444 = ttnn.from_device(ttnn_reshape_291, )
  ttnn_to_layout_476 = ttnn.to_layout(ttnn_from_device_444, ttnn.TILE_LAYOUT, )
  ttnn_to_device_290 = ttnn.to_device(ttnn_to_layout_476, device = device)
  ttnn_subtract_41 = ttnn.subtract(ttnn_permute_123, ttnn_to_device_290, )
  ttnn_from_device_445 = ttnn.from_device(ttnn_reshape_290, )
  ttnn_to_layout_477 = ttnn.to_layout(ttnn_from_device_445, ttnn.TILE_LAYOUT, )
  ttnn_to_device_291 = ttnn.to_device(ttnn_to_layout_477, device = device)
  ttnn_multiply_89 = ttnn.multiply(ttnn_subtract_41, ttnn_to_device_291, )
  ttnn_from_torch_236 = ttnn.from_torch(arg152_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_292 = ttnn.reshape(ttnn_from_torch_236, (1, 80, 1, 1), )
  ttnn_from_device_446 = ttnn.from_device(ttnn_reshape_292, )
  ttnn_to_layout_478 = ttnn.to_layout(ttnn_from_device_446, ttnn.TILE_LAYOUT, )
  ttnn_to_device_292 = ttnn.to_device(ttnn_to_layout_478, device = device)
  ttnn_multiply_90 = ttnn.multiply(ttnn_multiply_89, ttnn_to_device_292, )
  ttnn_from_torch_237 = ttnn.from_torch(arg153_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_293 = ttnn.reshape(ttnn_from_torch_237, (1, 80, 1, 1), )
  ttnn_from_device_447 = ttnn.from_device(ttnn_reshape_293, )
  ttnn_to_layout_479 = ttnn.to_layout(ttnn_from_device_447, ttnn.TILE_LAYOUT, )
  ttnn_to_device_293 = ttnn.to_device(ttnn_to_layout_479, device = device)
  ttnn_add_91 = ttnn.add(ttnn_multiply_90, ttnn_to_device_293, )
  ttnn_prefix_pack_to_tuple_41 = pack_to_tuple_wrapper(ttnn_add_91, )
  ttnn_prefix_getitem_41 = ttnn_prefix_pack_to_tuple_41[0]
  ttnn_add_92 = ttnn.add(ttnn_prefix_getitem_41, ttnn_prefix_getitem_38, )
  ttnn_permute_124 = ttnn.permute(ttnn_add_92, (0, 2, 3, 1), )
  ttnn_from_device_448 = ttnn.from_device(ttnn_permute_124, )
  ttnn_to_layout_480 = ttnn.to_layout(ttnn_from_device_448, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_294 = ttnn.reshape(ttnn_to_layout_480, (1, 1, 100, 80), )
  ttnn_from_device_449 = ttnn.from_device(ttnn_reshape_294, )
  ttnn_to_layout_481 = ttnn.to_layout(ttnn_from_device_449, ttnn.TILE_LAYOUT, )
  ttnn_to_device_294 = ttnn.to_device(ttnn_to_layout_481, device = device)
  ttnn_from_torch_238 = ttnn.from_torch(arg154_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_56 = conv_wrapper(ttnn_to_device_294, ttnn_from_torch_238, None, 1, 80, 480, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_56 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_56, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_450 = ttnn.from_device(ttnn_sharded_to_interleaved_56, )
  ttnn_to_layout_482 = ttnn.to_layout(ttnn_from_device_450, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_295 = ttnn.reshape(ttnn_to_layout_482, (1, 10, 10, 480), )
  ttnn_from_device_451 = ttnn.from_device(ttnn_reshape_295, )
  ttnn_to_layout_483 = ttnn.to_layout(ttnn_from_device_451, ttnn.TILE_LAYOUT, )
  ttnn_to_device_295 = ttnn.to_device(ttnn_to_layout_483, device = device)
  ttnn_permute_125 = ttnn.permute(ttnn_to_device_295, (0, 3, 1, 2), )
  ttnn_from_torch_239 = ttnn.from_torch(arg399_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_93 = ttnn.add(ttnn_from_torch_239, 0.001, )
  ttnn_rsqrt_42 = ttnn.rsqrt(ttnn_add_93, )
  ttnn_from_device_452 = ttnn.from_device(ttnn_rsqrt_42, )
  ttnn_to_layout_484 = ttnn.to_layout(ttnn_from_device_452, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_296 = ttnn.reshape(ttnn_to_layout_484, (1, 480, 1, 1), )
  ttnn_from_torch_240 = ttnn.from_torch(arg398_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_297 = ttnn.reshape(ttnn_from_torch_240, (1, 480, 1, 1), )
  ttnn_from_device_453 = ttnn.from_device(ttnn_reshape_297, )
  ttnn_to_layout_485 = ttnn.to_layout(ttnn_from_device_453, ttnn.TILE_LAYOUT, )
  ttnn_to_device_296 = ttnn.to_device(ttnn_to_layout_485, device = device)
  ttnn_subtract_42 = ttnn.subtract(ttnn_permute_125, ttnn_to_device_296, )
  ttnn_from_device_454 = ttnn.from_device(ttnn_reshape_296, )
  ttnn_to_layout_486 = ttnn.to_layout(ttnn_from_device_454, ttnn.TILE_LAYOUT, )
  ttnn_to_device_297 = ttnn.to_device(ttnn_to_layout_486, device = device)
  ttnn_multiply_91 = ttnn.multiply(ttnn_subtract_42, ttnn_to_device_297, )
  ttnn_from_torch_241 = ttnn.from_torch(arg155_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_298 = ttnn.reshape(ttnn_from_torch_241, (1, 480, 1, 1), )
  ttnn_from_device_455 = ttnn.from_device(ttnn_reshape_298, )
  ttnn_to_layout_487 = ttnn.to_layout(ttnn_from_device_455, ttnn.TILE_LAYOUT, )
  ttnn_to_device_298 = ttnn.to_device(ttnn_to_layout_487, device = device)
  ttnn_multiply_92 = ttnn.multiply(ttnn_multiply_91, ttnn_to_device_298, )
  ttnn_from_torch_242 = ttnn.from_torch(arg156_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_299 = ttnn.reshape(ttnn_from_torch_242, (1, 480, 1, 1), )
  ttnn_from_device_456 = ttnn.from_device(ttnn_reshape_299, )
  ttnn_to_layout_488 = ttnn.to_layout(ttnn_from_device_456, ttnn.TILE_LAYOUT, )
  ttnn_to_device_299 = ttnn.to_device(ttnn_to_layout_488, device = device)
  ttnn_add_94 = ttnn.add(ttnn_multiply_92, ttnn_to_device_299, )
  ttnn_prefix_pack_to_tuple_42 = pack_to_tuple_wrapper(ttnn_add_94, )
  ttnn_prefix_getitem_42 = ttnn_prefix_pack_to_tuple_42[0]
  ttnn_hardswish_17 = ttnn.hardswish(ttnn_prefix_getitem_42, )
  ttnn_permute_126 = ttnn.permute(ttnn_hardswish_17, (0, 2, 3, 1), )
  ttnn_from_device_457 = ttnn.from_device(ttnn_permute_126, )
  ttnn_to_layout_489 = ttnn.to_layout(ttnn_from_device_457, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_300 = ttnn.reshape(ttnn_to_layout_489, (1, 1, 100, 480), )
  ttnn_from_device_458 = ttnn.from_device(ttnn_reshape_300, )
  ttnn_to_layout_490 = ttnn.to_layout(ttnn_from_device_458, ttnn.TILE_LAYOUT, )
  ttnn_to_device_300 = ttnn.to_device(ttnn_to_layout_490, device = device)
  ttnn_from_torch_243 = ttnn.from_torch(arg157_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_57 = conv_wrapper(ttnn_to_device_300, ttnn_from_torch_243, None, 1, 480, 480, [10, 10], [5, 5], [1, 1], [2, 2], [1, 1], 480, device, False, None, )
  ttnn_sharded_to_interleaved_57 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_57, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_459 = ttnn.from_device(ttnn_sharded_to_interleaved_57, )
  ttnn_to_layout_491 = ttnn.to_layout(ttnn_from_device_459, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_301 = ttnn.reshape(ttnn_to_layout_491, (1, 10, 10, 480), )
  ttnn_from_device_460 = ttnn.from_device(ttnn_reshape_301, )
  ttnn_to_layout_492 = ttnn.to_layout(ttnn_from_device_460, ttnn.TILE_LAYOUT, )
  ttnn_to_device_301 = ttnn.to_device(ttnn_to_layout_492, device = device)
  ttnn_permute_127 = ttnn.permute(ttnn_to_device_301, (0, 3, 1, 2), )
  ttnn_from_torch_244 = ttnn.from_torch(arg402_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_95 = ttnn.add(ttnn_from_torch_244, 0.001, )
  ttnn_rsqrt_43 = ttnn.rsqrt(ttnn_add_95, )
  ttnn_from_device_461 = ttnn.from_device(ttnn_rsqrt_43, )
  ttnn_to_layout_493 = ttnn.to_layout(ttnn_from_device_461, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_302 = ttnn.reshape(ttnn_to_layout_493, (1, 480, 1, 1), )
  ttnn_from_torch_245 = ttnn.from_torch(arg401_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_303 = ttnn.reshape(ttnn_from_torch_245, (1, 480, 1, 1), )
  ttnn_from_device_462 = ttnn.from_device(ttnn_reshape_303, )
  ttnn_to_layout_494 = ttnn.to_layout(ttnn_from_device_462, ttnn.TILE_LAYOUT, )
  ttnn_to_device_302 = ttnn.to_device(ttnn_to_layout_494, device = device)
  ttnn_subtract_43 = ttnn.subtract(ttnn_permute_127, ttnn_to_device_302, )
  ttnn_from_device_463 = ttnn.from_device(ttnn_reshape_302, )
  ttnn_to_layout_495 = ttnn.to_layout(ttnn_from_device_463, ttnn.TILE_LAYOUT, )
  ttnn_to_device_303 = ttnn.to_device(ttnn_to_layout_495, device = device)
  ttnn_multiply_93 = ttnn.multiply(ttnn_subtract_43, ttnn_to_device_303, )
  ttnn_from_torch_246 = ttnn.from_torch(arg158_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_304 = ttnn.reshape(ttnn_from_torch_246, (1, 480, 1, 1), )
  ttnn_from_device_464 = ttnn.from_device(ttnn_reshape_304, )
  ttnn_to_layout_496 = ttnn.to_layout(ttnn_from_device_464, ttnn.TILE_LAYOUT, )
  ttnn_to_device_304 = ttnn.to_device(ttnn_to_layout_496, device = device)
  ttnn_multiply_94 = ttnn.multiply(ttnn_multiply_93, ttnn_to_device_304, )
  ttnn_from_torch_247 = ttnn.from_torch(arg159_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_305 = ttnn.reshape(ttnn_from_torch_247, (1, 480, 1, 1), )
  ttnn_from_device_465 = ttnn.from_device(ttnn_reshape_305, )
  ttnn_to_layout_497 = ttnn.to_layout(ttnn_from_device_465, ttnn.TILE_LAYOUT, )
  ttnn_to_device_305 = ttnn.to_device(ttnn_to_layout_497, device = device)
  ttnn_add_96 = ttnn.add(ttnn_multiply_94, ttnn_to_device_305, )
  ttnn_prefix_pack_to_tuple_43 = pack_to_tuple_wrapper(ttnn_add_96, )
  ttnn_prefix_getitem_43 = ttnn_prefix_pack_to_tuple_43[0]
  ttnn_hardswish_18 = ttnn.hardswish(ttnn_prefix_getitem_43, )
  ttnn_mean_7 = ttnn.mean(ttnn_hardswish_18, (-1, -2), )
  test_accuracy(mean_7, ttnn_mean_7)
  ttnn_permute_128 = ttnn.permute(ttnn_mean_7, (0, 2, 3, 1), )
  ttnn_from_device_466 = ttnn.from_device(ttnn_permute_128, )
  ttnn_to_layout_498 = ttnn.to_layout(ttnn_from_device_466, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_306 = ttnn.reshape(ttnn_to_layout_498, (1, 1, 1, 480), )
  ttnn_from_torch_248 = ttnn.from_torch(arg161_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_307 = ttnn.reshape(ttnn_from_torch_248, (1, 1, 1, 120), )
  ttnn_from_device_467 = ttnn.from_device(ttnn_reshape_307, )
  ttnn_to_layout_499 = ttnn.to_layout(ttnn_from_device_467, ttnn.TILE_LAYOUT, )
  ttnn_to_device_306 = ttnn.to_device(ttnn_to_layout_499, device = device)
  ttnn_prefix_move_to_host_14 = move_to_host_wrapper(ttnn_to_device_306, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_468 = ttnn.from_device(ttnn_reshape_306, )
  ttnn_to_layout_500 = ttnn.to_layout(ttnn_from_device_468, ttnn.TILE_LAYOUT, )
  ttnn_to_device_307 = ttnn.to_device(ttnn_to_layout_500, device = device)
  ttnn_from_torch_249 = ttnn.from_torch(arg160_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_58 = conv_wrapper(ttnn_to_device_307, ttnn_from_torch_249, ttnn_prefix_move_to_host_14, 1, 480, 120, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_58 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_58, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_469 = ttnn.from_device(ttnn_sharded_to_interleaved_58, )
  ttnn_to_layout_501 = ttnn.to_layout(ttnn_from_device_469, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_308 = ttnn.reshape(ttnn_to_layout_501, (1, 1, 1, 120), )
  ttnn_from_device_470 = ttnn.from_device(ttnn_reshape_308, )
  ttnn_to_layout_502 = ttnn.to_layout(ttnn_from_device_470, ttnn.TILE_LAYOUT, )
  ttnn_to_device_308 = ttnn.to_device(ttnn_to_layout_502, device = device)
  ttnn_permute_129 = ttnn.permute(ttnn_to_device_308, (0, 3, 1, 2), )
  test_accuracy(convolution_58, ttnn_permute_129)
  ttnn_relu_18 = ttnn.relu(ttnn_permute_129, )
  test_accuracy(relu_18, ttnn_relu_18)
  ttnn_permute_130 = ttnn.permute(ttnn_relu_18, (0, 2, 3, 1), )
  ttnn_from_device_471 = ttnn.from_device(ttnn_permute_130, )
  ttnn_to_layout_503 = ttnn.to_layout(ttnn_from_device_471, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_309 = ttnn.reshape(ttnn_to_layout_503, (1, 1, 1, 120), )
  ttnn_from_torch_250 = ttnn.from_torch(arg163_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_310 = ttnn.reshape(ttnn_from_torch_250, (1, 1, 1, 480), )
  ttnn_from_device_472 = ttnn.from_device(ttnn_reshape_310, )
  ttnn_to_layout_504 = ttnn.to_layout(ttnn_from_device_472, ttnn.TILE_LAYOUT, )
  ttnn_to_device_309 = ttnn.to_device(ttnn_to_layout_504, device = device)
  ttnn_prefix_move_to_host_15 = move_to_host_wrapper(ttnn_to_device_309, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_473 = ttnn.from_device(ttnn_reshape_309, )
  ttnn_to_layout_505 = ttnn.to_layout(ttnn_from_device_473, ttnn.TILE_LAYOUT, )
  ttnn_to_device_310 = ttnn.to_device(ttnn_to_layout_505, device = device)
  ttnn_from_torch_251 = ttnn.from_torch(arg162_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_59 = conv_wrapper(ttnn_to_device_310, ttnn_from_torch_251, ttnn_prefix_move_to_host_15, 1, 120, 480, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_59 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_59, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_474 = ttnn.from_device(ttnn_sharded_to_interleaved_59, )
  ttnn_to_layout_506 = ttnn.to_layout(ttnn_from_device_474, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_311 = ttnn.reshape(ttnn_to_layout_506, (1, 1, 1, 480), )
  ttnn_from_device_475 = ttnn.from_device(ttnn_reshape_311, )
  ttnn_to_layout_507 = ttnn.to_layout(ttnn_from_device_475, ttnn.TILE_LAYOUT, )
  ttnn_to_device_311 = ttnn.to_device(ttnn_to_layout_507, device = device)
  ttnn_permute_131 = ttnn.permute(ttnn_to_device_311, (0, 3, 1, 2), )
  test_accuracy(convolution_59, ttnn_permute_131)
  ttnn_hardsigmoid_7 = ttnn.hardsigmoid(ttnn_permute_131, )
  test_accuracy(hardsigmoid_7, ttnn_hardsigmoid_7)
  ttnn_multiply_95 = ttnn.multiply(ttnn_hardsigmoid_7, ttnn_hardswish_18, )
  test_accuracy(mul_15, ttnn_multiply_95)
  ttnn_permute_132 = ttnn.permute(ttnn_multiply_95, (0, 2, 3, 1), )
  ttnn_from_device_476 = ttnn.from_device(ttnn_permute_132, )
  ttnn_to_layout_508 = ttnn.to_layout(ttnn_from_device_476, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_312 = ttnn.reshape(ttnn_to_layout_508, (1, 1, 100, 480), )
  ttnn_from_device_477 = ttnn.from_device(ttnn_reshape_312, )
  ttnn_to_layout_509 = ttnn.to_layout(ttnn_from_device_477, ttnn.TILE_LAYOUT, )
  ttnn_to_device_312 = ttnn.to_device(ttnn_to_layout_509, device = device)
  ttnn_from_torch_252 = ttnn.from_torch(arg164_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_60 = conv_wrapper(ttnn_to_device_312, ttnn_from_torch_252, None, 1, 480, 80, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_60 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_60, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_478 = ttnn.from_device(ttnn_sharded_to_interleaved_60, )
  ttnn_to_layout_510 = ttnn.to_layout(ttnn_from_device_478, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_313 = ttnn.reshape(ttnn_to_layout_510, (1, 10, 10, 80), )
  ttnn_from_device_479 = ttnn.from_device(ttnn_reshape_313, )
  ttnn_to_layout_511 = ttnn.to_layout(ttnn_from_device_479, ttnn.TILE_LAYOUT, )
  ttnn_to_device_313 = ttnn.to_device(ttnn_to_layout_511, device = device)
  ttnn_permute_133 = ttnn.permute(ttnn_to_device_313, (0, 3, 1, 2), )
  test_accuracy(convolution_60, ttnn_permute_133)
  ttnn_from_torch_253 = ttnn.from_torch(arg405_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_97 = ttnn.add(ttnn_from_torch_253, 0.001, )
  ttnn_rsqrt_44 = ttnn.rsqrt(ttnn_add_97, )
  ttnn_from_device_480 = ttnn.from_device(ttnn_rsqrt_44, )
  ttnn_to_layout_512 = ttnn.to_layout(ttnn_from_device_480, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_314 = ttnn.reshape(ttnn_to_layout_512, (1, 80, 1, 1), )
  ttnn_from_torch_254 = ttnn.from_torch(arg404_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_315 = ttnn.reshape(ttnn_from_torch_254, (1, 80, 1, 1), )
  ttnn_from_device_481 = ttnn.from_device(ttnn_reshape_315, )
  ttnn_to_layout_513 = ttnn.to_layout(ttnn_from_device_481, ttnn.TILE_LAYOUT, )
  ttnn_to_device_314 = ttnn.to_device(ttnn_to_layout_513, device = device)
  ttnn_subtract_44 = ttnn.subtract(ttnn_permute_133, ttnn_to_device_314, )
  ttnn_from_device_482 = ttnn.from_device(ttnn_reshape_314, )
  ttnn_to_layout_514 = ttnn.to_layout(ttnn_from_device_482, ttnn.TILE_LAYOUT, )
  ttnn_to_device_315 = ttnn.to_device(ttnn_to_layout_514, device = device)
  ttnn_multiply_96 = ttnn.multiply(ttnn_subtract_44, ttnn_to_device_315, )
  ttnn_from_torch_255 = ttnn.from_torch(arg165_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_316 = ttnn.reshape(ttnn_from_torch_255, (1, 80, 1, 1), )
  ttnn_from_device_483 = ttnn.from_device(ttnn_reshape_316, )
  ttnn_to_layout_515 = ttnn.to_layout(ttnn_from_device_483, ttnn.TILE_LAYOUT, )
  ttnn_to_device_316 = ttnn.to_device(ttnn_to_layout_515, device = device)
  ttnn_multiply_97 = ttnn.multiply(ttnn_multiply_96, ttnn_to_device_316, )
  ttnn_from_torch_256 = ttnn.from_torch(arg166_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_317 = ttnn.reshape(ttnn_from_torch_256, (1, 80, 1, 1), )
  ttnn_from_device_484 = ttnn.from_device(ttnn_reshape_317, )
  ttnn_to_layout_516 = ttnn.to_layout(ttnn_from_device_484, ttnn.TILE_LAYOUT, )
  ttnn_to_device_317 = ttnn.to_device(ttnn_to_layout_516, device = device)
  ttnn_add_98 = ttnn.add(ttnn_multiply_97, ttnn_to_device_317, )
  ttnn_prefix_pack_to_tuple_44 = pack_to_tuple_wrapper(ttnn_add_98, )
  ttnn_prefix_getitem_44 = ttnn_prefix_pack_to_tuple_44[0]
  test_accuracy(getitem_132, ttnn_prefix_getitem_44)
  ttnn_add_99 = ttnn.add(ttnn_prefix_getitem_44, ttnn_add_92, )
  test_accuracy(add_14, ttnn_add_99)
  ttnn_permute_134 = ttnn.permute(ttnn_add_99, (0, 2, 3, 1), )
  ttnn_from_device_485 = ttnn.from_device(ttnn_permute_134, )
  ttnn_to_layout_517 = ttnn.to_layout(ttnn_from_device_485, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_318 = ttnn.reshape(ttnn_to_layout_517, (1, 1, 100, 80), )
  ttnn_from_device_486 = ttnn.from_device(ttnn_reshape_318, )
  ttnn_to_layout_518 = ttnn.to_layout(ttnn_from_device_486, ttnn.TILE_LAYOUT, )
  ttnn_to_device_318 = ttnn.to_device(ttnn_to_layout_518, device = device)
  ttnn_from_torch_257 = ttnn.from_torch(arg167_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_61 = conv_wrapper(ttnn_to_device_318, ttnn_from_torch_257, None, 1, 80, 480, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_61 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_61, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_487 = ttnn.from_device(ttnn_sharded_to_interleaved_61, )
  ttnn_to_layout_519 = ttnn.to_layout(ttnn_from_device_487, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_319 = ttnn.reshape(ttnn_to_layout_519, (1, 10, 10, 480), )
  ttnn_from_device_488 = ttnn.from_device(ttnn_reshape_319, )
  ttnn_to_layout_520 = ttnn.to_layout(ttnn_from_device_488, ttnn.TILE_LAYOUT, )
  ttnn_to_device_319 = ttnn.to_device(ttnn_to_layout_520, device = device)
  ttnn_permute_135 = ttnn.permute(ttnn_to_device_319, (0, 3, 1, 2), )
  ttnn_from_torch_258 = ttnn.from_torch(arg408_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_100 = ttnn.add(ttnn_from_torch_258, 0.001, )
  ttnn_rsqrt_45 = ttnn.rsqrt(ttnn_add_100, )
  ttnn_from_device_489 = ttnn.from_device(ttnn_rsqrt_45, )
  ttnn_to_layout_521 = ttnn.to_layout(ttnn_from_device_489, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_320 = ttnn.reshape(ttnn_to_layout_521, (1, 480, 1, 1), )
  ttnn_from_torch_259 = ttnn.from_torch(arg407_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_321 = ttnn.reshape(ttnn_from_torch_259, (1, 480, 1, 1), )
  ttnn_from_device_490 = ttnn.from_device(ttnn_reshape_321, )
  ttnn_to_layout_522 = ttnn.to_layout(ttnn_from_device_490, ttnn.TILE_LAYOUT, )
  ttnn_to_device_320 = ttnn.to_device(ttnn_to_layout_522, device = device)
  ttnn_subtract_45 = ttnn.subtract(ttnn_permute_135, ttnn_to_device_320, )
  ttnn_from_device_491 = ttnn.from_device(ttnn_reshape_320, )
  ttnn_to_layout_523 = ttnn.to_layout(ttnn_from_device_491, ttnn.TILE_LAYOUT, )
  ttnn_to_device_321 = ttnn.to_device(ttnn_to_layout_523, device = device)
  ttnn_multiply_98 = ttnn.multiply(ttnn_subtract_45, ttnn_to_device_321, )
  ttnn_from_torch_260 = ttnn.from_torch(arg168_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_322 = ttnn.reshape(ttnn_from_torch_260, (1, 480, 1, 1), )
  ttnn_from_device_492 = ttnn.from_device(ttnn_reshape_322, )
  ttnn_to_layout_524 = ttnn.to_layout(ttnn_from_device_492, ttnn.TILE_LAYOUT, )
  ttnn_to_device_322 = ttnn.to_device(ttnn_to_layout_524, device = device)
  ttnn_multiply_99 = ttnn.multiply(ttnn_multiply_98, ttnn_to_device_322, )
  ttnn_from_torch_261 = ttnn.from_torch(arg169_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_323 = ttnn.reshape(ttnn_from_torch_261, (1, 480, 1, 1), )
  ttnn_from_device_493 = ttnn.from_device(ttnn_reshape_323, )
  ttnn_to_layout_525 = ttnn.to_layout(ttnn_from_device_493, ttnn.TILE_LAYOUT, )
  ttnn_to_device_323 = ttnn.to_device(ttnn_to_layout_525, device = device)
  ttnn_add_101 = ttnn.add(ttnn_multiply_99, ttnn_to_device_323, )
  ttnn_prefix_pack_to_tuple_45 = pack_to_tuple_wrapper(ttnn_add_101, )
  ttnn_prefix_getitem_45 = ttnn_prefix_pack_to_tuple_45[0]
  ttnn_hardswish_19 = ttnn.hardswish(ttnn_prefix_getitem_45, )
  test_accuracy(hardswish_19, ttnn_hardswish_19)
  ttnn_permute_136 = ttnn.permute(ttnn_hardswish_19, (0, 2, 3, 1), )
  ttnn_from_device_494 = ttnn.from_device(ttnn_permute_136, )
  ttnn_to_layout_526 = ttnn.to_layout(ttnn_from_device_494, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_324 = ttnn.reshape(ttnn_to_layout_526, (1, 1, 100, 480), )
  ttnn_from_device_495 = ttnn.from_device(ttnn_reshape_324, )
  ttnn_to_layout_527 = ttnn.to_layout(ttnn_from_device_495, ttnn.TILE_LAYOUT, )
  ttnn_to_device_324 = ttnn.to_device(ttnn_to_layout_527, device = device)
  ttnn_from_torch_262 = ttnn.from_torch(arg170_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_62 = conv_wrapper(ttnn_to_device_324, ttnn_from_torch_262, None, 1, 480, 256, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_62 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_62, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_496 = ttnn.from_device(ttnn_sharded_to_interleaved_62, )
  ttnn_to_layout_528 = ttnn.to_layout(ttnn_from_device_496, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_325 = ttnn.reshape(ttnn_to_layout_528, (1, 10, 10, 256), )
  ttnn_from_device_497 = ttnn.from_device(ttnn_reshape_325, )
  ttnn_to_layout_529 = ttnn.to_layout(ttnn_from_device_497, ttnn.TILE_LAYOUT, )
  ttnn_to_device_325 = ttnn.to_device(ttnn_to_layout_529, device = device)
  ttnn_permute_137 = ttnn.permute(ttnn_to_device_325, (0, 3, 1, 2), )
  test_accuracy(convolution_62, ttnn_permute_137)
  ttnn_from_torch_263 = ttnn.from_torch(arg411_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_102 = ttnn.add(ttnn_from_torch_263, 0.001, )
  ttnn_rsqrt_46 = ttnn.rsqrt(ttnn_add_102, )
  ttnn_from_device_498 = ttnn.from_device(ttnn_rsqrt_46, )
  ttnn_to_layout_530 = ttnn.to_layout(ttnn_from_device_498, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_326 = ttnn.reshape(ttnn_to_layout_530, (1, 256, 1, 1), )
  ttnn_from_torch_264 = ttnn.from_torch(arg410_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_327 = ttnn.reshape(ttnn_from_torch_264, (1, 256, 1, 1), )
  ttnn_from_device_499 = ttnn.from_device(ttnn_reshape_327, )
  ttnn_to_layout_531 = ttnn.to_layout(ttnn_from_device_499, ttnn.TILE_LAYOUT, )
  ttnn_to_device_326 = ttnn.to_device(ttnn_to_layout_531, device = device)
  ttnn_subtract_46 = ttnn.subtract(ttnn_permute_137, ttnn_to_device_326, )
  ttnn_from_device_500 = ttnn.from_device(ttnn_reshape_326, )
  ttnn_to_layout_532 = ttnn.to_layout(ttnn_from_device_500, ttnn.TILE_LAYOUT, )
  ttnn_to_device_327 = ttnn.to_device(ttnn_to_layout_532, device = device)
  ttnn_multiply_100 = ttnn.multiply(ttnn_subtract_46, ttnn_to_device_327, )
  ttnn_from_torch_265 = ttnn.from_torch(arg171_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_328 = ttnn.reshape(ttnn_from_torch_265, (1, 256, 1, 1), )
  ttnn_from_device_501 = ttnn.from_device(ttnn_reshape_328, )
  ttnn_to_layout_533 = ttnn.to_layout(ttnn_from_device_501, ttnn.TILE_LAYOUT, )
  ttnn_to_device_328 = ttnn.to_device(ttnn_to_layout_533, device = device)
  ttnn_multiply_101 = ttnn.multiply(ttnn_multiply_100, ttnn_to_device_328, )
  ttnn_from_torch_266 = ttnn.from_torch(arg172_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_329 = ttnn.reshape(ttnn_from_torch_266, (1, 256, 1, 1), )
  ttnn_from_device_502 = ttnn.from_device(ttnn_reshape_329, )
  ttnn_to_layout_534 = ttnn.to_layout(ttnn_from_device_502, ttnn.TILE_LAYOUT, )
  ttnn_to_device_329 = ttnn.to_device(ttnn_to_layout_534, device = device)
  ttnn_add_103 = ttnn.add(ttnn_multiply_101, ttnn_to_device_329, )
  ttnn_prefix_pack_to_tuple_46 = pack_to_tuple_wrapper(ttnn_add_103, )
  ttnn_prefix_getitem_46 = ttnn_prefix_pack_to_tuple_46[0]
  test_accuracy(getitem_138, ttnn_prefix_getitem_46)
  ttnn_hardtanh = ttnn.hardtanh(ttnn_prefix_getitem_46, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh, ttnn_hardtanh)
  ttnn_permute_138 = ttnn.permute(ttnn_hardtanh, (0, 2, 3, 1), )
  ttnn_from_device_503 = ttnn.from_device(ttnn_permute_138, )
  ttnn_to_layout_535 = ttnn.to_layout(ttnn_from_device_503, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_330 = ttnn.reshape(ttnn_to_layout_535, (1, 1, 100, 256), )
  ttnn_from_device_504 = ttnn.from_device(ttnn_reshape_330, )
  ttnn_to_layout_536 = ttnn.to_layout(ttnn_from_device_504, ttnn.TILE_LAYOUT, )
  ttnn_to_device_330 = ttnn.to_device(ttnn_to_layout_536, device = device)
  ttnn_from_torch_267 = ttnn.from_torch(arg173_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_63 = conv_wrapper(ttnn_to_device_330, ttnn_from_torch_267, None, 1, 256, 256, [10, 10], [3, 3], [2, 2], [1, 1], [1, 1], 256, device, False, None, )
  ttnn_sharded_to_interleaved_63 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_63, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_505 = ttnn.from_device(ttnn_sharded_to_interleaved_63, )
  ttnn_to_layout_537 = ttnn.to_layout(ttnn_from_device_505, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_331 = ttnn.reshape(ttnn_to_layout_537, (1, 5, 5, 256), )
  ttnn_from_device_506 = ttnn.from_device(ttnn_reshape_331, )
  ttnn_to_layout_538 = ttnn.to_layout(ttnn_from_device_506, ttnn.TILE_LAYOUT, )
  ttnn_to_device_331 = ttnn.to_device(ttnn_to_layout_538, device = device)
  ttnn_permute_139 = ttnn.permute(ttnn_to_device_331, (0, 3, 1, 2), )
  test_accuracy(convolution_63, ttnn_permute_139)
  ttnn_from_torch_268 = ttnn.from_torch(arg414_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_104 = ttnn.add(ttnn_from_torch_268, 0.001, )
  ttnn_rsqrt_47 = ttnn.rsqrt(ttnn_add_104, )
  ttnn_from_device_507 = ttnn.from_device(ttnn_rsqrt_47, )
  ttnn_to_layout_539 = ttnn.to_layout(ttnn_from_device_507, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_332 = ttnn.reshape(ttnn_to_layout_539, (1, 256, 1, 1), )
  ttnn_from_torch_269 = ttnn.from_torch(arg413_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_333 = ttnn.reshape(ttnn_from_torch_269, (1, 256, 1, 1), )
  ttnn_from_device_508 = ttnn.from_device(ttnn_reshape_333, )
  ttnn_to_layout_540 = ttnn.to_layout(ttnn_from_device_508, ttnn.TILE_LAYOUT, )
  ttnn_to_device_332 = ttnn.to_device(ttnn_to_layout_540, device = device)
  ttnn_subtract_47 = ttnn.subtract(ttnn_permute_139, ttnn_to_device_332, )
  ttnn_from_device_509 = ttnn.from_device(ttnn_reshape_332, )
  ttnn_to_layout_541 = ttnn.to_layout(ttnn_from_device_509, ttnn.TILE_LAYOUT, )
  ttnn_to_device_333 = ttnn.to_device(ttnn_to_layout_541, device = device)
  ttnn_multiply_102 = ttnn.multiply(ttnn_subtract_47, ttnn_to_device_333, )
  ttnn_from_torch_270 = ttnn.from_torch(arg174_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_334 = ttnn.reshape(ttnn_from_torch_270, (1, 256, 1, 1), )
  ttnn_from_device_510 = ttnn.from_device(ttnn_reshape_334, )
  ttnn_to_layout_542 = ttnn.to_layout(ttnn_from_device_510, ttnn.TILE_LAYOUT, )
  ttnn_to_device_334 = ttnn.to_device(ttnn_to_layout_542, device = device)
  ttnn_multiply_103 = ttnn.multiply(ttnn_multiply_102, ttnn_to_device_334, )
  ttnn_from_torch_271 = ttnn.from_torch(arg175_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_335 = ttnn.reshape(ttnn_from_torch_271, (1, 256, 1, 1), )
  ttnn_from_device_511 = ttnn.from_device(ttnn_reshape_335, )
  ttnn_to_layout_543 = ttnn.to_layout(ttnn_from_device_511, ttnn.TILE_LAYOUT, )
  ttnn_to_device_335 = ttnn.to_device(ttnn_to_layout_543, device = device)
  ttnn_add_105 = ttnn.add(ttnn_multiply_103, ttnn_to_device_335, )
  ttnn_prefix_pack_to_tuple_47 = pack_to_tuple_wrapper(ttnn_add_105, )
  ttnn_prefix_getitem_47 = ttnn_prefix_pack_to_tuple_47[0]
  test_accuracy(getitem_141, ttnn_prefix_getitem_47)
  ttnn_hardtanh_1 = ttnn.hardtanh(ttnn_prefix_getitem_47, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_1, ttnn_hardtanh_1)
  ttnn_permute_140 = ttnn.permute(ttnn_hardtanh_1, (0, 2, 3, 1), )
  ttnn_from_device_512 = ttnn.from_device(ttnn_permute_140, )
  ttnn_to_layout_544 = ttnn.to_layout(ttnn_from_device_512, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_336 = ttnn.reshape(ttnn_to_layout_544, (1, 1, 25, 256), )
  ttnn_from_device_513 = ttnn.from_device(ttnn_reshape_336, )
  ttnn_to_layout_545 = ttnn.to_layout(ttnn_from_device_513, ttnn.TILE_LAYOUT, )
  ttnn_to_device_336 = ttnn.to_device(ttnn_to_layout_545, device = device)
  ttnn_from_torch_272 = ttnn.from_torch(arg176_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_64 = conv_wrapper(ttnn_to_device_336, ttnn_from_torch_272, None, 1, 256, 512, [5, 5], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_64 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_64, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_514 = ttnn.from_device(ttnn_sharded_to_interleaved_64, )
  ttnn_to_layout_546 = ttnn.to_layout(ttnn_from_device_514, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_337 = ttnn.reshape(ttnn_to_layout_546, (1, 5, 5, 512), )
  ttnn_from_device_515 = ttnn.from_device(ttnn_reshape_337, )
  ttnn_to_layout_547 = ttnn.to_layout(ttnn_from_device_515, ttnn.TILE_LAYOUT, )
  ttnn_to_device_337 = ttnn.to_device(ttnn_to_layout_547, device = device)
  ttnn_permute_141 = ttnn.permute(ttnn_to_device_337, (0, 3, 1, 2), )
  ttnn_from_torch_273 = ttnn.from_torch(arg417_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_106 = ttnn.add(ttnn_from_torch_273, 0.001, )
  ttnn_rsqrt_48 = ttnn.rsqrt(ttnn_add_106, )
  ttnn_from_device_516 = ttnn.from_device(ttnn_rsqrt_48, )
  ttnn_to_layout_548 = ttnn.to_layout(ttnn_from_device_516, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_338 = ttnn.reshape(ttnn_to_layout_548, (1, 512, 1, 1), )
  ttnn_from_torch_274 = ttnn.from_torch(arg416_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_339 = ttnn.reshape(ttnn_from_torch_274, (1, 512, 1, 1), )
  ttnn_from_device_517 = ttnn.from_device(ttnn_reshape_339, )
  ttnn_to_layout_549 = ttnn.to_layout(ttnn_from_device_517, ttnn.TILE_LAYOUT, )
  ttnn_to_device_338 = ttnn.to_device(ttnn_to_layout_549, device = device)
  ttnn_subtract_48 = ttnn.subtract(ttnn_permute_141, ttnn_to_device_338, )
  ttnn_from_device_518 = ttnn.from_device(ttnn_reshape_338, )
  ttnn_to_layout_550 = ttnn.to_layout(ttnn_from_device_518, ttnn.TILE_LAYOUT, )
  ttnn_to_device_339 = ttnn.to_device(ttnn_to_layout_550, device = device)
  ttnn_multiply_104 = ttnn.multiply(ttnn_subtract_48, ttnn_to_device_339, )
  ttnn_from_torch_275 = ttnn.from_torch(arg177_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_340 = ttnn.reshape(ttnn_from_torch_275, (1, 512, 1, 1), )
  ttnn_from_device_519 = ttnn.from_device(ttnn_reshape_340, )
  ttnn_to_layout_551 = ttnn.to_layout(ttnn_from_device_519, ttnn.TILE_LAYOUT, )
  ttnn_to_device_340 = ttnn.to_device(ttnn_to_layout_551, device = device)
  ttnn_multiply_105 = ttnn.multiply(ttnn_multiply_104, ttnn_to_device_340, )
  ttnn_from_torch_276 = ttnn.from_torch(arg178_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_341 = ttnn.reshape(ttnn_from_torch_276, (1, 512, 1, 1), )
  ttnn_from_device_520 = ttnn.from_device(ttnn_reshape_341, )
  ttnn_to_layout_552 = ttnn.to_layout(ttnn_from_device_520, ttnn.TILE_LAYOUT, )
  ttnn_to_device_341 = ttnn.to_device(ttnn_to_layout_552, device = device)
  ttnn_add_107 = ttnn.add(ttnn_multiply_105, ttnn_to_device_341, )
  ttnn_prefix_pack_to_tuple_48 = pack_to_tuple_wrapper(ttnn_add_107, )
  ttnn_prefix_getitem_48 = ttnn_prefix_pack_to_tuple_48[0]
  ttnn_hardtanh_2 = ttnn.hardtanh(ttnn_prefix_getitem_48, min_val = 0.0, max_val = 6.0)
  ttnn_permute_142 = ttnn.permute(ttnn_hardtanh_2, (0, 2, 3, 1), )
  ttnn_from_device_521 = ttnn.from_device(ttnn_permute_142, )
  ttnn_to_layout_553 = ttnn.to_layout(ttnn_from_device_521, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_342 = ttnn.reshape(ttnn_to_layout_553, (1, 1, 25, 512), )
  ttnn_from_device_522 = ttnn.from_device(ttnn_reshape_342, )
  ttnn_to_layout_554 = ttnn.to_layout(ttnn_from_device_522, ttnn.TILE_LAYOUT, )
  ttnn_to_device_342 = ttnn.to_device(ttnn_to_layout_554, device = device)
  ttnn_from_torch_277 = ttnn.from_torch(arg179_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_65 = conv_wrapper(ttnn_to_device_342, ttnn_from_torch_277, None, 1, 512, 128, [5, 5], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_65 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_65, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_523 = ttnn.from_device(ttnn_sharded_to_interleaved_65, )
  ttnn_to_layout_555 = ttnn.to_layout(ttnn_from_device_523, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_343 = ttnn.reshape(ttnn_to_layout_555, (1, 5, 5, 128), )
  ttnn_from_device_524 = ttnn.from_device(ttnn_reshape_343, )
  ttnn_to_layout_556 = ttnn.to_layout(ttnn_from_device_524, ttnn.TILE_LAYOUT, )
  ttnn_to_device_343 = ttnn.to_device(ttnn_to_layout_556, device = device)
  ttnn_permute_143 = ttnn.permute(ttnn_to_device_343, (0, 3, 1, 2), )
  test_accuracy(convolution_65, ttnn_permute_143)
  ttnn_from_torch_278 = ttnn.from_torch(arg420_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_108 = ttnn.add(ttnn_from_torch_278, 0.001, )
  ttnn_rsqrt_49 = ttnn.rsqrt(ttnn_add_108, )
  ttnn_from_device_525 = ttnn.from_device(ttnn_rsqrt_49, )
  ttnn_to_layout_557 = ttnn.to_layout(ttnn_from_device_525, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_344 = ttnn.reshape(ttnn_to_layout_557, (1, 128, 1, 1), )
  ttnn_from_torch_279 = ttnn.from_torch(arg419_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_345 = ttnn.reshape(ttnn_from_torch_279, (1, 128, 1, 1), )
  ttnn_from_device_526 = ttnn.from_device(ttnn_reshape_345, )
  ttnn_to_layout_558 = ttnn.to_layout(ttnn_from_device_526, ttnn.TILE_LAYOUT, )
  ttnn_to_device_344 = ttnn.to_device(ttnn_to_layout_558, device = device)
  ttnn_subtract_49 = ttnn.subtract(ttnn_permute_143, ttnn_to_device_344, )
  ttnn_from_device_527 = ttnn.from_device(ttnn_reshape_344, )
  ttnn_to_layout_559 = ttnn.to_layout(ttnn_from_device_527, ttnn.TILE_LAYOUT, )
  ttnn_to_device_345 = ttnn.to_device(ttnn_to_layout_559, device = device)
  ttnn_multiply_106 = ttnn.multiply(ttnn_subtract_49, ttnn_to_device_345, )
  ttnn_from_torch_280 = ttnn.from_torch(arg180_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_346 = ttnn.reshape(ttnn_from_torch_280, (1, 128, 1, 1), )
  ttnn_from_device_528 = ttnn.from_device(ttnn_reshape_346, )
  ttnn_to_layout_560 = ttnn.to_layout(ttnn_from_device_528, ttnn.TILE_LAYOUT, )
  ttnn_to_device_346 = ttnn.to_device(ttnn_to_layout_560, device = device)
  ttnn_multiply_107 = ttnn.multiply(ttnn_multiply_106, ttnn_to_device_346, )
  ttnn_from_torch_281 = ttnn.from_torch(arg181_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_347 = ttnn.reshape(ttnn_from_torch_281, (1, 128, 1, 1), )
  ttnn_from_device_529 = ttnn.from_device(ttnn_reshape_347, )
  ttnn_to_layout_561 = ttnn.to_layout(ttnn_from_device_529, ttnn.TILE_LAYOUT, )
  ttnn_to_device_347 = ttnn.to_device(ttnn_to_layout_561, device = device)
  ttnn_add_109 = ttnn.add(ttnn_multiply_107, ttnn_to_device_347, )
  ttnn_prefix_pack_to_tuple_49 = pack_to_tuple_wrapper(ttnn_add_109, )
  ttnn_prefix_getitem_49 = ttnn_prefix_pack_to_tuple_49[0]
  test_accuracy(getitem_147, ttnn_prefix_getitem_49)
  ttnn_hardtanh_3 = ttnn.hardtanh(ttnn_prefix_getitem_49, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_3, ttnn_hardtanh_3)
  ttnn_permute_144 = ttnn.permute(ttnn_hardtanh_3, (0, 2, 3, 1), )
  ttnn_from_device_530 = ttnn.from_device(ttnn_permute_144, )
  ttnn_to_layout_562 = ttnn.to_layout(ttnn_from_device_530, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_348 = ttnn.reshape(ttnn_to_layout_562, (1, 1, 25, 128), )
  ttnn_from_device_531 = ttnn.from_device(ttnn_reshape_348, )
  ttnn_to_layout_563 = ttnn.to_layout(ttnn_from_device_531, ttnn.TILE_LAYOUT, )
  ttnn_to_device_348 = ttnn.to_device(ttnn_to_layout_563, device = device)
  ttnn_from_torch_282 = ttnn.from_torch(arg182_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_66 = conv_wrapper(ttnn_to_device_348, ttnn_from_torch_282, None, 1, 128, 128, [5, 5], [3, 3], [2, 2], [1, 1], [1, 1], 128, device, False, None, )
  ttnn_sharded_to_interleaved_66 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_66, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_532 = ttnn.from_device(ttnn_sharded_to_interleaved_66, )
  ttnn_to_layout_564 = ttnn.to_layout(ttnn_from_device_532, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_349 = ttnn.reshape(ttnn_to_layout_564, (1, 3, 3, 128), )
  ttnn_from_device_533 = ttnn.from_device(ttnn_reshape_349, )
  ttnn_to_layout_565 = ttnn.to_layout(ttnn_from_device_533, ttnn.TILE_LAYOUT, )
  ttnn_to_device_349 = ttnn.to_device(ttnn_to_layout_565, device = device)
  ttnn_permute_145 = ttnn.permute(ttnn_to_device_349, (0, 3, 1, 2), )
  ttnn_from_torch_283 = ttnn.from_torch(arg423_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_110 = ttnn.add(ttnn_from_torch_283, 0.001, )
  ttnn_rsqrt_50 = ttnn.rsqrt(ttnn_add_110, )
  ttnn_from_device_534 = ttnn.from_device(ttnn_rsqrt_50, )
  ttnn_to_layout_566 = ttnn.to_layout(ttnn_from_device_534, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_350 = ttnn.reshape(ttnn_to_layout_566, (1, 128, 1, 1), )
  ttnn_from_torch_284 = ttnn.from_torch(arg422_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_351 = ttnn.reshape(ttnn_from_torch_284, (1, 128, 1, 1), )
  ttnn_from_device_535 = ttnn.from_device(ttnn_reshape_351, )
  ttnn_to_layout_567 = ttnn.to_layout(ttnn_from_device_535, ttnn.TILE_LAYOUT, )
  ttnn_to_device_350 = ttnn.to_device(ttnn_to_layout_567, device = device)
  ttnn_subtract_50 = ttnn.subtract(ttnn_permute_145, ttnn_to_device_350, )
  ttnn_from_device_536 = ttnn.from_device(ttnn_reshape_350, )
  ttnn_to_layout_568 = ttnn.to_layout(ttnn_from_device_536, ttnn.TILE_LAYOUT, )
  ttnn_to_device_351 = ttnn.to_device(ttnn_to_layout_568, device = device)
  ttnn_multiply_108 = ttnn.multiply(ttnn_subtract_50, ttnn_to_device_351, )
  ttnn_from_torch_285 = ttnn.from_torch(arg183_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_352 = ttnn.reshape(ttnn_from_torch_285, (1, 128, 1, 1), )
  ttnn_from_device_537 = ttnn.from_device(ttnn_reshape_352, )
  ttnn_to_layout_569 = ttnn.to_layout(ttnn_from_device_537, ttnn.TILE_LAYOUT, )
  ttnn_to_device_352 = ttnn.to_device(ttnn_to_layout_569, device = device)
  ttnn_multiply_109 = ttnn.multiply(ttnn_multiply_108, ttnn_to_device_352, )
  ttnn_from_torch_286 = ttnn.from_torch(arg184_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_353 = ttnn.reshape(ttnn_from_torch_286, (1, 128, 1, 1), )
  ttnn_from_device_538 = ttnn.from_device(ttnn_reshape_353, )
  ttnn_to_layout_570 = ttnn.to_layout(ttnn_from_device_538, ttnn.TILE_LAYOUT, )
  ttnn_to_device_353 = ttnn.to_device(ttnn_to_layout_570, device = device)
  ttnn_add_111 = ttnn.add(ttnn_multiply_109, ttnn_to_device_353, )
  ttnn_prefix_pack_to_tuple_50 = pack_to_tuple_wrapper(ttnn_add_111, )
  ttnn_prefix_getitem_50 = ttnn_prefix_pack_to_tuple_50[0]
  ttnn_hardtanh_4 = ttnn.hardtanh(ttnn_prefix_getitem_50, min_val = 0.0, max_val = 6.0)
  ttnn_permute_146 = ttnn.permute(ttnn_hardtanh_4, (0, 2, 3, 1), )
  ttnn_from_device_539 = ttnn.from_device(ttnn_permute_146, )
  ttnn_to_layout_571 = ttnn.to_layout(ttnn_from_device_539, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_354 = ttnn.reshape(ttnn_to_layout_571, (1, 1, 9, 128), )
  ttnn_from_device_540 = ttnn.from_device(ttnn_reshape_354, )
  ttnn_to_layout_572 = ttnn.to_layout(ttnn_from_device_540, ttnn.TILE_LAYOUT, )
  ttnn_to_device_354 = ttnn.to_device(ttnn_to_layout_572, device = device)
  ttnn_from_torch_287 = ttnn.from_torch(arg185_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_67 = conv_wrapper(ttnn_to_device_354, ttnn_from_torch_287, None, 1, 128, 256, [3, 3], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_67 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_67, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_541 = ttnn.from_device(ttnn_sharded_to_interleaved_67, )
  ttnn_to_layout_573 = ttnn.to_layout(ttnn_from_device_541, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_355 = ttnn.reshape(ttnn_to_layout_573, (1, 3, 3, 256), )
  ttnn_from_device_542 = ttnn.from_device(ttnn_reshape_355, )
  ttnn_to_layout_574 = ttnn.to_layout(ttnn_from_device_542, ttnn.TILE_LAYOUT, )
  ttnn_to_device_355 = ttnn.to_device(ttnn_to_layout_574, device = device)
  ttnn_permute_147 = ttnn.permute(ttnn_to_device_355, (0, 3, 1, 2), )
  ttnn_from_torch_288 = ttnn.from_torch(arg426_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_112 = ttnn.add(ttnn_from_torch_288, 0.001, )
  ttnn_rsqrt_51 = ttnn.rsqrt(ttnn_add_112, )
  ttnn_from_device_543 = ttnn.from_device(ttnn_rsqrt_51, )
  ttnn_to_layout_575 = ttnn.to_layout(ttnn_from_device_543, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_356 = ttnn.reshape(ttnn_to_layout_575, (1, 256, 1, 1), )
  ttnn_from_torch_289 = ttnn.from_torch(arg425_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_357 = ttnn.reshape(ttnn_from_torch_289, (1, 256, 1, 1), )
  ttnn_from_device_544 = ttnn.from_device(ttnn_reshape_357, )
  ttnn_to_layout_576 = ttnn.to_layout(ttnn_from_device_544, ttnn.TILE_LAYOUT, )
  ttnn_to_device_356 = ttnn.to_device(ttnn_to_layout_576, device = device)
  ttnn_subtract_51 = ttnn.subtract(ttnn_permute_147, ttnn_to_device_356, )
  ttnn_from_device_545 = ttnn.from_device(ttnn_reshape_356, )
  ttnn_to_layout_577 = ttnn.to_layout(ttnn_from_device_545, ttnn.TILE_LAYOUT, )
  ttnn_to_device_357 = ttnn.to_device(ttnn_to_layout_577, device = device)
  ttnn_multiply_110 = ttnn.multiply(ttnn_subtract_51, ttnn_to_device_357, )
  ttnn_from_torch_290 = ttnn.from_torch(arg186_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_358 = ttnn.reshape(ttnn_from_torch_290, (1, 256, 1, 1), )
  ttnn_from_device_546 = ttnn.from_device(ttnn_reshape_358, )
  ttnn_to_layout_578 = ttnn.to_layout(ttnn_from_device_546, ttnn.TILE_LAYOUT, )
  ttnn_to_device_358 = ttnn.to_device(ttnn_to_layout_578, device = device)
  ttnn_multiply_111 = ttnn.multiply(ttnn_multiply_110, ttnn_to_device_358, )
  ttnn_from_torch_291 = ttnn.from_torch(arg187_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_359 = ttnn.reshape(ttnn_from_torch_291, (1, 256, 1, 1), )
  ttnn_from_device_547 = ttnn.from_device(ttnn_reshape_359, )
  ttnn_to_layout_579 = ttnn.to_layout(ttnn_from_device_547, ttnn.TILE_LAYOUT, )
  ttnn_to_device_359 = ttnn.to_device(ttnn_to_layout_579, device = device)
  ttnn_add_113 = ttnn.add(ttnn_multiply_111, ttnn_to_device_359, )
  ttnn_prefix_pack_to_tuple_51 = pack_to_tuple_wrapper(ttnn_add_113, )
  ttnn_prefix_getitem_51 = ttnn_prefix_pack_to_tuple_51[0]
  ttnn_hardtanh_5 = ttnn.hardtanh(ttnn_prefix_getitem_51, min_val = 0.0, max_val = 6.0)
  ttnn_permute_148 = ttnn.permute(ttnn_hardtanh_5, (0, 2, 3, 1), )
  ttnn_from_device_548 = ttnn.from_device(ttnn_permute_148, )
  ttnn_to_layout_580 = ttnn.to_layout(ttnn_from_device_548, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_360 = ttnn.reshape(ttnn_to_layout_580, (1, 1, 9, 256), )
  ttnn_from_device_549 = ttnn.from_device(ttnn_reshape_360, )
  ttnn_to_layout_581 = ttnn.to_layout(ttnn_from_device_549, ttnn.TILE_LAYOUT, )
  ttnn_to_device_360 = ttnn.to_device(ttnn_to_layout_581, device = device)
  ttnn_from_torch_292 = ttnn.from_torch(arg188_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_68 = conv_wrapper(ttnn_to_device_360, ttnn_from_torch_292, None, 1, 256, 128, [3, 3], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_68 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_68, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_550 = ttnn.from_device(ttnn_sharded_to_interleaved_68, )
  ttnn_to_layout_582 = ttnn.to_layout(ttnn_from_device_550, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_361 = ttnn.reshape(ttnn_to_layout_582, (1, 3, 3, 128), )
  ttnn_from_device_551 = ttnn.from_device(ttnn_reshape_361, )
  ttnn_to_layout_583 = ttnn.to_layout(ttnn_from_device_551, ttnn.TILE_LAYOUT, )
  ttnn_to_device_361 = ttnn.to_device(ttnn_to_layout_583, device = device)
  ttnn_permute_149 = ttnn.permute(ttnn_to_device_361, (0, 3, 1, 2), )
  test_accuracy(convolution_68, ttnn_permute_149)
  ttnn_from_torch_293 = ttnn.from_torch(arg429_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_114 = ttnn.add(ttnn_from_torch_293, 0.001, )
  ttnn_rsqrt_52 = ttnn.rsqrt(ttnn_add_114, )
  ttnn_from_device_552 = ttnn.from_device(ttnn_rsqrt_52, )
  ttnn_to_layout_584 = ttnn.to_layout(ttnn_from_device_552, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_362 = ttnn.reshape(ttnn_to_layout_584, (1, 128, 1, 1), )
  ttnn_from_torch_294 = ttnn.from_torch(arg428_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_363 = ttnn.reshape(ttnn_from_torch_294, (1, 128, 1, 1), )
  ttnn_from_device_553 = ttnn.from_device(ttnn_reshape_363, )
  ttnn_to_layout_585 = ttnn.to_layout(ttnn_from_device_553, ttnn.TILE_LAYOUT, )
  ttnn_to_device_362 = ttnn.to_device(ttnn_to_layout_585, device = device)
  ttnn_subtract_52 = ttnn.subtract(ttnn_permute_149, ttnn_to_device_362, )
  ttnn_from_device_554 = ttnn.from_device(ttnn_reshape_362, )
  ttnn_to_layout_586 = ttnn.to_layout(ttnn_from_device_554, ttnn.TILE_LAYOUT, )
  ttnn_to_device_363 = ttnn.to_device(ttnn_to_layout_586, device = device)
  ttnn_multiply_112 = ttnn.multiply(ttnn_subtract_52, ttnn_to_device_363, )
  ttnn_from_torch_295 = ttnn.from_torch(arg189_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_364 = ttnn.reshape(ttnn_from_torch_295, (1, 128, 1, 1), )
  ttnn_from_device_555 = ttnn.from_device(ttnn_reshape_364, )
  ttnn_to_layout_587 = ttnn.to_layout(ttnn_from_device_555, ttnn.TILE_LAYOUT, )
  ttnn_to_device_364 = ttnn.to_device(ttnn_to_layout_587, device = device)
  ttnn_multiply_113 = ttnn.multiply(ttnn_multiply_112, ttnn_to_device_364, )
  ttnn_from_torch_296 = ttnn.from_torch(arg190_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_365 = ttnn.reshape(ttnn_from_torch_296, (1, 128, 1, 1), )
  ttnn_from_device_556 = ttnn.from_device(ttnn_reshape_365, )
  ttnn_to_layout_588 = ttnn.to_layout(ttnn_from_device_556, ttnn.TILE_LAYOUT, )
  ttnn_to_device_365 = ttnn.to_device(ttnn_to_layout_588, device = device)
  ttnn_add_115 = ttnn.add(ttnn_multiply_113, ttnn_to_device_365, )
  ttnn_prefix_pack_to_tuple_52 = pack_to_tuple_wrapper(ttnn_add_115, )
  ttnn_prefix_getitem_52 = ttnn_prefix_pack_to_tuple_52[0]
  test_accuracy(getitem_156, ttnn_prefix_getitem_52)
  ttnn_hardtanh_6 = ttnn.hardtanh(ttnn_prefix_getitem_52, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_6, ttnn_hardtanh_6)
  ttnn_permute_150 = ttnn.permute(ttnn_hardtanh_6, (0, 2, 3, 1), )
  ttnn_from_device_557 = ttnn.from_device(ttnn_permute_150, )
  ttnn_to_layout_589 = ttnn.to_layout(ttnn_from_device_557, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_366 = ttnn.reshape(ttnn_to_layout_589, (1, 1, 9, 128), )
  ttnn_from_device_558 = ttnn.from_device(ttnn_reshape_366, )
  ttnn_to_layout_590 = ttnn.to_layout(ttnn_from_device_558, ttnn.TILE_LAYOUT, )
  ttnn_to_device_366 = ttnn.to_device(ttnn_to_layout_590, device = device)
  ttnn_from_torch_297 = ttnn.from_torch(arg191_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_69 = conv_wrapper(ttnn_to_device_366, ttnn_from_torch_297, None, 1, 128, 128, [3, 3], [3, 3], [2, 2], [1, 1], [1, 1], 128, device, False, None, )
  ttnn_sharded_to_interleaved_69 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_69, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_559 = ttnn.from_device(ttnn_sharded_to_interleaved_69, )
  ttnn_to_layout_591 = ttnn.to_layout(ttnn_from_device_559, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_367 = ttnn.reshape(ttnn_to_layout_591, (1, 2, 2, 128), )
  ttnn_from_device_560 = ttnn.from_device(ttnn_reshape_367, )
  ttnn_to_layout_592 = ttnn.to_layout(ttnn_from_device_560, ttnn.TILE_LAYOUT, )
  ttnn_to_device_367 = ttnn.to_device(ttnn_to_layout_592, device = device)
  ttnn_permute_151 = ttnn.permute(ttnn_to_device_367, (0, 3, 1, 2), )
  test_accuracy(convolution_69, ttnn_permute_151)
  ttnn_from_torch_298 = ttnn.from_torch(arg432_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_116 = ttnn.add(ttnn_from_torch_298, 0.001, )
  ttnn_rsqrt_53 = ttnn.rsqrt(ttnn_add_116, )
  ttnn_from_device_561 = ttnn.from_device(ttnn_rsqrt_53, )
  ttnn_to_layout_593 = ttnn.to_layout(ttnn_from_device_561, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_368 = ttnn.reshape(ttnn_to_layout_593, (1, 128, 1, 1), )
  ttnn_from_torch_299 = ttnn.from_torch(arg431_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_369 = ttnn.reshape(ttnn_from_torch_299, (1, 128, 1, 1), )
  ttnn_from_device_562 = ttnn.from_device(ttnn_reshape_369, )
  ttnn_to_layout_594 = ttnn.to_layout(ttnn_from_device_562, ttnn.TILE_LAYOUT, )
  ttnn_to_device_368 = ttnn.to_device(ttnn_to_layout_594, device = device)
  ttnn_subtract_53 = ttnn.subtract(ttnn_permute_151, ttnn_to_device_368, )
  ttnn_from_device_563 = ttnn.from_device(ttnn_reshape_368, )
  ttnn_to_layout_595 = ttnn.to_layout(ttnn_from_device_563, ttnn.TILE_LAYOUT, )
  ttnn_to_device_369 = ttnn.to_device(ttnn_to_layout_595, device = device)
  ttnn_multiply_114 = ttnn.multiply(ttnn_subtract_53, ttnn_to_device_369, )
  ttnn_from_torch_300 = ttnn.from_torch(arg192_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_370 = ttnn.reshape(ttnn_from_torch_300, (1, 128, 1, 1), )
  ttnn_from_device_564 = ttnn.from_device(ttnn_reshape_370, )
  ttnn_to_layout_596 = ttnn.to_layout(ttnn_from_device_564, ttnn.TILE_LAYOUT, )
  ttnn_to_device_370 = ttnn.to_device(ttnn_to_layout_596, device = device)
  ttnn_multiply_115 = ttnn.multiply(ttnn_multiply_114, ttnn_to_device_370, )
  ttnn_from_torch_301 = ttnn.from_torch(arg193_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_371 = ttnn.reshape(ttnn_from_torch_301, (1, 128, 1, 1), )
  ttnn_from_device_565 = ttnn.from_device(ttnn_reshape_371, )
  ttnn_to_layout_597 = ttnn.to_layout(ttnn_from_device_565, ttnn.TILE_LAYOUT, )
  ttnn_to_device_371 = ttnn.to_device(ttnn_to_layout_597, device = device)
  ttnn_add_117 = ttnn.add(ttnn_multiply_115, ttnn_to_device_371, )
  ttnn_prefix_pack_to_tuple_53 = pack_to_tuple_wrapper(ttnn_add_117, )
  ttnn_prefix_getitem_53 = ttnn_prefix_pack_to_tuple_53[0]
  test_accuracy(getitem_159, ttnn_prefix_getitem_53)
  ttnn_hardtanh_7 = ttnn.hardtanh(ttnn_prefix_getitem_53, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_7, ttnn_hardtanh_7)
  ttnn_permute_152 = ttnn.permute(ttnn_hardtanh_7, (0, 2, 3, 1), )
  ttnn_from_device_566 = ttnn.from_device(ttnn_permute_152, )
  ttnn_to_layout_598 = ttnn.to_layout(ttnn_from_device_566, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_372 = ttnn.reshape(ttnn_to_layout_598, (1, 1, 4, 128), )
  ttnn_from_device_567 = ttnn.from_device(ttnn_reshape_372, )
  ttnn_to_layout_599 = ttnn.to_layout(ttnn_from_device_567, ttnn.TILE_LAYOUT, )
  ttnn_to_device_372 = ttnn.to_device(ttnn_to_layout_599, device = device)
  ttnn_from_torch_302 = ttnn.from_torch(arg194_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_70 = conv_wrapper(ttnn_to_device_372, ttnn_from_torch_302, None, 1, 128, 256, [2, 2], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_70 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_70, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_568 = ttnn.from_device(ttnn_sharded_to_interleaved_70, )
  ttnn_to_layout_600 = ttnn.to_layout(ttnn_from_device_568, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_373 = ttnn.reshape(ttnn_to_layout_600, (1, 2, 2, 256), )
  ttnn_from_device_569 = ttnn.from_device(ttnn_reshape_373, )
  ttnn_to_layout_601 = ttnn.to_layout(ttnn_from_device_569, ttnn.TILE_LAYOUT, )
  ttnn_to_device_373 = ttnn.to_device(ttnn_to_layout_601, device = device)
  ttnn_permute_153 = ttnn.permute(ttnn_to_device_373, (0, 3, 1, 2), )
  ttnn_from_torch_303 = ttnn.from_torch(arg435_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_118 = ttnn.add(ttnn_from_torch_303, 0.001, )
  ttnn_rsqrt_54 = ttnn.rsqrt(ttnn_add_118, )
  ttnn_from_device_570 = ttnn.from_device(ttnn_rsqrt_54, )
  ttnn_to_layout_602 = ttnn.to_layout(ttnn_from_device_570, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_374 = ttnn.reshape(ttnn_to_layout_602, (1, 256, 1, 1), )
  ttnn_from_torch_304 = ttnn.from_torch(arg434_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_375 = ttnn.reshape(ttnn_from_torch_304, (1, 256, 1, 1), )
  ttnn_from_device_571 = ttnn.from_device(ttnn_reshape_375, )
  ttnn_to_layout_603 = ttnn.to_layout(ttnn_from_device_571, ttnn.TILE_LAYOUT, )
  ttnn_to_device_374 = ttnn.to_device(ttnn_to_layout_603, device = device)
  ttnn_subtract_54 = ttnn.subtract(ttnn_permute_153, ttnn_to_device_374, )
  ttnn_from_device_572 = ttnn.from_device(ttnn_reshape_374, )
  ttnn_to_layout_604 = ttnn.to_layout(ttnn_from_device_572, ttnn.TILE_LAYOUT, )
  ttnn_to_device_375 = ttnn.to_device(ttnn_to_layout_604, device = device)
  ttnn_multiply_116 = ttnn.multiply(ttnn_subtract_54, ttnn_to_device_375, )
  ttnn_from_torch_305 = ttnn.from_torch(arg195_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_376 = ttnn.reshape(ttnn_from_torch_305, (1, 256, 1, 1), )
  ttnn_from_device_573 = ttnn.from_device(ttnn_reshape_376, )
  ttnn_to_layout_605 = ttnn.to_layout(ttnn_from_device_573, ttnn.TILE_LAYOUT, )
  ttnn_to_device_376 = ttnn.to_device(ttnn_to_layout_605, device = device)
  ttnn_multiply_117 = ttnn.multiply(ttnn_multiply_116, ttnn_to_device_376, )
  ttnn_from_torch_306 = ttnn.from_torch(arg196_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_377 = ttnn.reshape(ttnn_from_torch_306, (1, 256, 1, 1), )
  ttnn_from_device_574 = ttnn.from_device(ttnn_reshape_377, )
  ttnn_to_layout_606 = ttnn.to_layout(ttnn_from_device_574, ttnn.TILE_LAYOUT, )
  ttnn_to_device_377 = ttnn.to_device(ttnn_to_layout_606, device = device)
  ttnn_add_119 = ttnn.add(ttnn_multiply_117, ttnn_to_device_377, )
  ttnn_prefix_pack_to_tuple_54 = pack_to_tuple_wrapper(ttnn_add_119, )
  ttnn_prefix_getitem_54 = ttnn_prefix_pack_to_tuple_54[0]
  ttnn_hardtanh_8 = ttnn.hardtanh(ttnn_prefix_getitem_54, min_val = 0.0, max_val = 6.0)
  ttnn_permute_154 = ttnn.permute(ttnn_hardtanh_8, (0, 2, 3, 1), )
  ttnn_from_device_575 = ttnn.from_device(ttnn_permute_154, )
  ttnn_to_layout_607 = ttnn.to_layout(ttnn_from_device_575, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_378 = ttnn.reshape(ttnn_to_layout_607, (1, 1, 4, 256), )
  ttnn_from_device_576 = ttnn.from_device(ttnn_reshape_378, )
  ttnn_to_layout_608 = ttnn.to_layout(ttnn_from_device_576, ttnn.TILE_LAYOUT, )
  ttnn_to_device_378 = ttnn.to_device(ttnn_to_layout_608, device = device)
  ttnn_from_torch_307 = ttnn.from_torch(arg197_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_71 = conv_wrapper(ttnn_to_device_378, ttnn_from_torch_307, None, 1, 256, 64, [2, 2], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_71 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_71, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_577 = ttnn.from_device(ttnn_sharded_to_interleaved_71, )
  ttnn_to_layout_609 = ttnn.to_layout(ttnn_from_device_577, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_379 = ttnn.reshape(ttnn_to_layout_609, (1, 2, 2, 64), )
  ttnn_from_device_578 = ttnn.from_device(ttnn_reshape_379, )
  ttnn_to_layout_610 = ttnn.to_layout(ttnn_from_device_578, ttnn.TILE_LAYOUT, )
  ttnn_to_device_379 = ttnn.to_device(ttnn_to_layout_610, device = device)
  ttnn_permute_155 = ttnn.permute(ttnn_to_device_379, (0, 3, 1, 2), )
  test_accuracy(convolution_71, ttnn_permute_155)
  ttnn_from_torch_308 = ttnn.from_torch(arg438_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_120 = ttnn.add(ttnn_from_torch_308, 0.001, )
  ttnn_rsqrt_55 = ttnn.rsqrt(ttnn_add_120, )
  ttnn_from_device_579 = ttnn.from_device(ttnn_rsqrt_55, )
  ttnn_to_layout_611 = ttnn.to_layout(ttnn_from_device_579, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_380 = ttnn.reshape(ttnn_to_layout_611, (1, 64, 1, 1), )
  ttnn_from_torch_309 = ttnn.from_torch(arg437_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_381 = ttnn.reshape(ttnn_from_torch_309, (1, 64, 1, 1), )
  ttnn_from_device_580 = ttnn.from_device(ttnn_reshape_381, )
  ttnn_to_layout_612 = ttnn.to_layout(ttnn_from_device_580, ttnn.TILE_LAYOUT, )
  ttnn_to_device_380 = ttnn.to_device(ttnn_to_layout_612, device = device)
  ttnn_subtract_55 = ttnn.subtract(ttnn_permute_155, ttnn_to_device_380, )
  ttnn_from_device_581 = ttnn.from_device(ttnn_reshape_380, )
  ttnn_to_layout_613 = ttnn.to_layout(ttnn_from_device_581, ttnn.TILE_LAYOUT, )
  ttnn_to_device_381 = ttnn.to_device(ttnn_to_layout_613, device = device)
  ttnn_multiply_118 = ttnn.multiply(ttnn_subtract_55, ttnn_to_device_381, )
  ttnn_from_torch_310 = ttnn.from_torch(arg198_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_382 = ttnn.reshape(ttnn_from_torch_310, (1, 64, 1, 1), )
  ttnn_from_device_582 = ttnn.from_device(ttnn_reshape_382, )
  ttnn_to_layout_614 = ttnn.to_layout(ttnn_from_device_582, ttnn.TILE_LAYOUT, )
  ttnn_to_device_382 = ttnn.to_device(ttnn_to_layout_614, device = device)
  ttnn_multiply_119 = ttnn.multiply(ttnn_multiply_118, ttnn_to_device_382, )
  ttnn_from_torch_311 = ttnn.from_torch(arg199_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_383 = ttnn.reshape(ttnn_from_torch_311, (1, 64, 1, 1), )
  ttnn_from_device_583 = ttnn.from_device(ttnn_reshape_383, )
  ttnn_to_layout_615 = ttnn.to_layout(ttnn_from_device_583, ttnn.TILE_LAYOUT, )
  ttnn_to_device_383 = ttnn.to_device(ttnn_to_layout_615, device = device)
  ttnn_add_121 = ttnn.add(ttnn_multiply_119, ttnn_to_device_383, )
  ttnn_prefix_pack_to_tuple_55 = pack_to_tuple_wrapper(ttnn_add_121, )
  ttnn_prefix_getitem_55 = ttnn_prefix_pack_to_tuple_55[0]
  test_accuracy(getitem_165, ttnn_prefix_getitem_55)
  ttnn_hardtanh_9 = ttnn.hardtanh(ttnn_prefix_getitem_55, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_9, ttnn_hardtanh_9)
  ttnn_permute_156 = ttnn.permute(ttnn_hardtanh_9, (0, 2, 3, 1), )
  ttnn_from_device_584 = ttnn.from_device(ttnn_permute_156, )
  ttnn_to_layout_616 = ttnn.to_layout(ttnn_from_device_584, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_384 = ttnn.reshape(ttnn_to_layout_616, (1, 1, 4, 64), )
  ttnn_from_device_585 = ttnn.from_device(ttnn_reshape_384, )
  ttnn_to_layout_617 = ttnn.to_layout(ttnn_from_device_585, ttnn.TILE_LAYOUT, )
  ttnn_to_device_384 = ttnn.to_device(ttnn_to_layout_617, device = device)
  ttnn_from_torch_312 = ttnn.from_torch(arg200_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_72 = conv_wrapper(ttnn_to_device_384, ttnn_from_torch_312, None, 1, 64, 64, [2, 2], [3, 3], [2, 2], [1, 1], [1, 1], 64, device, False, None, )
  ttnn_sharded_to_interleaved_72 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_72, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_586 = ttnn.from_device(ttnn_sharded_to_interleaved_72, )
  ttnn_to_layout_618 = ttnn.to_layout(ttnn_from_device_586, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_385 = ttnn.reshape(ttnn_to_layout_618, (1, 1, 1, 64), )
  ttnn_from_device_587 = ttnn.from_device(ttnn_reshape_385, )
  ttnn_to_layout_619 = ttnn.to_layout(ttnn_from_device_587, ttnn.TILE_LAYOUT, )
  ttnn_to_device_385 = ttnn.to_device(ttnn_to_layout_619, device = device)
  ttnn_permute_157 = ttnn.permute(ttnn_to_device_385, (0, 3, 1, 2), )
  test_accuracy(convolution_72, ttnn_permute_157)
  ttnn_from_torch_313 = ttnn.from_torch(arg441_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_122 = ttnn.add(ttnn_from_torch_313, 0.001, )
  ttnn_rsqrt_56 = ttnn.rsqrt(ttnn_add_122, )
  ttnn_from_device_588 = ttnn.from_device(ttnn_rsqrt_56, )
  ttnn_to_layout_620 = ttnn.to_layout(ttnn_from_device_588, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_386 = ttnn.reshape(ttnn_to_layout_620, (1, 64, 1, 1), )
  ttnn_from_torch_314 = ttnn.from_torch(arg440_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_387 = ttnn.reshape(ttnn_from_torch_314, (1, 64, 1, 1), )
  ttnn_from_device_589 = ttnn.from_device(ttnn_reshape_387, )
  ttnn_to_layout_621 = ttnn.to_layout(ttnn_from_device_589, ttnn.TILE_LAYOUT, )
  ttnn_to_device_386 = ttnn.to_device(ttnn_to_layout_621, device = device)
  ttnn_subtract_56 = ttnn.subtract(ttnn_permute_157, ttnn_to_device_386, )
  ttnn_from_device_590 = ttnn.from_device(ttnn_reshape_386, )
  ttnn_to_layout_622 = ttnn.to_layout(ttnn_from_device_590, ttnn.TILE_LAYOUT, )
  ttnn_to_device_387 = ttnn.to_device(ttnn_to_layout_622, device = device)
  ttnn_multiply_120 = ttnn.multiply(ttnn_subtract_56, ttnn_to_device_387, )
  ttnn_from_torch_315 = ttnn.from_torch(arg201_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_388 = ttnn.reshape(ttnn_from_torch_315, (1, 64, 1, 1), )
  ttnn_from_device_591 = ttnn.from_device(ttnn_reshape_388, )
  ttnn_to_layout_623 = ttnn.to_layout(ttnn_from_device_591, ttnn.TILE_LAYOUT, )
  ttnn_to_device_388 = ttnn.to_device(ttnn_to_layout_623, device = device)
  ttnn_multiply_121 = ttnn.multiply(ttnn_multiply_120, ttnn_to_device_388, )
  ttnn_from_torch_316 = ttnn.from_torch(arg202_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_389 = ttnn.reshape(ttnn_from_torch_316, (1, 64, 1, 1), )
  ttnn_from_device_592 = ttnn.from_device(ttnn_reshape_389, )
  ttnn_to_layout_624 = ttnn.to_layout(ttnn_from_device_592, ttnn.TILE_LAYOUT, )
  ttnn_to_device_389 = ttnn.to_device(ttnn_to_layout_624, device = device)
  ttnn_add_123 = ttnn.add(ttnn_multiply_121, ttnn_to_device_389, )
  ttnn_prefix_pack_to_tuple_56 = pack_to_tuple_wrapper(ttnn_add_123, )
  ttnn_prefix_getitem_56 = ttnn_prefix_pack_to_tuple_56[0]
  test_accuracy(getitem_168, ttnn_prefix_getitem_56)
  ttnn_hardtanh_10 = ttnn.hardtanh(ttnn_prefix_getitem_56, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_10, ttnn_hardtanh_10)
  ttnn_permute_158 = ttnn.permute(ttnn_hardtanh_10, (0, 2, 3, 1), )
  ttnn_from_device_593 = ttnn.from_device(ttnn_permute_158, )
  ttnn_to_layout_625 = ttnn.to_layout(ttnn_from_device_593, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_390 = ttnn.reshape(ttnn_to_layout_625, (1, 1, 1, 64), )
  ttnn_from_device_594 = ttnn.from_device(ttnn_reshape_390, )
  ttnn_to_layout_626 = ttnn.to_layout(ttnn_from_device_594, ttnn.TILE_LAYOUT, )
  ttnn_to_device_390 = ttnn.to_device(ttnn_to_layout_626, device = device)
  ttnn_from_torch_317 = ttnn.from_torch(arg203_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_73 = conv_wrapper(ttnn_to_device_390, ttnn_from_torch_317, None, 1, 64, 128, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_73 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_73, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_595 = ttnn.from_device(ttnn_sharded_to_interleaved_73, )
  ttnn_to_layout_627 = ttnn.to_layout(ttnn_from_device_595, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_391 = ttnn.reshape(ttnn_to_layout_627, (1, 1, 1, 128), )
  ttnn_from_device_596 = ttnn.from_device(ttnn_reshape_391, )
  ttnn_to_layout_628 = ttnn.to_layout(ttnn_from_device_596, ttnn.TILE_LAYOUT, )
  ttnn_to_device_391 = ttnn.to_device(ttnn_to_layout_628, device = device)
  ttnn_permute_159 = ttnn.permute(ttnn_to_device_391, (0, 3, 1, 2), )
  ttnn_from_torch_318 = ttnn.from_torch(arg444_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_124 = ttnn.add(ttnn_from_torch_318, 0.001, )
  ttnn_rsqrt_57 = ttnn.rsqrt(ttnn_add_124, )
  ttnn_from_device_597 = ttnn.from_device(ttnn_rsqrt_57, )
  ttnn_to_layout_629 = ttnn.to_layout(ttnn_from_device_597, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_392 = ttnn.reshape(ttnn_to_layout_629, (1, 128, 1, 1), )
  ttnn_from_torch_319 = ttnn.from_torch(arg443_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_393 = ttnn.reshape(ttnn_from_torch_319, (1, 128, 1, 1), )
  ttnn_from_device_598 = ttnn.from_device(ttnn_reshape_393, )
  ttnn_to_layout_630 = ttnn.to_layout(ttnn_from_device_598, ttnn.TILE_LAYOUT, )
  ttnn_to_device_392 = ttnn.to_device(ttnn_to_layout_630, device = device)
  ttnn_subtract_57 = ttnn.subtract(ttnn_permute_159, ttnn_to_device_392, )
  ttnn_from_device_599 = ttnn.from_device(ttnn_reshape_392, )
  ttnn_to_layout_631 = ttnn.to_layout(ttnn_from_device_599, ttnn.TILE_LAYOUT, )
  ttnn_to_device_393 = ttnn.to_device(ttnn_to_layout_631, device = device)
  ttnn_multiply_122 = ttnn.multiply(ttnn_subtract_57, ttnn_to_device_393, )
  ttnn_from_torch_320 = ttnn.from_torch(arg204_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_394 = ttnn.reshape(ttnn_from_torch_320, (1, 128, 1, 1), )
  ttnn_from_device_600 = ttnn.from_device(ttnn_reshape_394, )
  ttnn_to_layout_632 = ttnn.to_layout(ttnn_from_device_600, ttnn.TILE_LAYOUT, )
  ttnn_to_device_394 = ttnn.to_device(ttnn_to_layout_632, device = device)
  ttnn_multiply_123 = ttnn.multiply(ttnn_multiply_122, ttnn_to_device_394, )
  ttnn_from_torch_321 = ttnn.from_torch(arg205_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_395 = ttnn.reshape(ttnn_from_torch_321, (1, 128, 1, 1), )
  ttnn_from_device_601 = ttnn.from_device(ttnn_reshape_395, )
  ttnn_to_layout_633 = ttnn.to_layout(ttnn_from_device_601, ttnn.TILE_LAYOUT, )
  ttnn_to_device_395 = ttnn.to_device(ttnn_to_layout_633, device = device)
  ttnn_add_125 = ttnn.add(ttnn_multiply_123, ttnn_to_device_395, )
  ttnn_prefix_pack_to_tuple_57 = pack_to_tuple_wrapper(ttnn_add_125, )
  ttnn_prefix_getitem_57 = ttnn_prefix_pack_to_tuple_57[0]
  ttnn_hardtanh_11 = ttnn.hardtanh(ttnn_prefix_getitem_57, min_val = 0.0, max_val = 6.0)
  ttnn_to_layout_634 = ttnn.to_layout(ttnn_from_device_383, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_396 = ttnn.reshape(ttnn_to_layout_634, (1, 1, 400, 672), )
  ttnn_from_device_603 = ttnn.from_device(ttnn_reshape_396, )
  ttnn_to_layout_635 = ttnn.to_layout(ttnn_from_device_603, ttnn.TILE_LAYOUT, )
  ttnn_to_device_396 = ttnn.to_device(ttnn_to_layout_635, device = device)
  ttnn_from_torch_322 = ttnn.from_torch(arg206_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_74 = conv_wrapper(ttnn_to_device_396, ttnn_from_torch_322, None, 1, 672, 672, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 672, device, False, None, )
  ttnn_sharded_to_interleaved_74 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_74, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_604 = ttnn.from_device(ttnn_sharded_to_interleaved_74, )
  ttnn_to_layout_636 = ttnn.to_layout(ttnn_from_device_604, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_397 = ttnn.reshape(ttnn_to_layout_636, (1, 20, 20, 672), )
  ttnn_from_device_605 = ttnn.from_device(ttnn_reshape_397, )
  ttnn_to_layout_637 = ttnn.to_layout(ttnn_from_device_605, ttnn.TILE_LAYOUT, )
  ttnn_to_device_397 = ttnn.to_device(ttnn_to_layout_637, device = device)
  ttnn_permute_161 = ttnn.permute(ttnn_to_device_397, (0, 3, 1, 2), )
  ttnn_from_torch_323 = ttnn.from_torch(arg447_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_126 = ttnn.add(ttnn_from_torch_323, 0.001, )
  ttnn_rsqrt_58 = ttnn.rsqrt(ttnn_add_126, )
  ttnn_from_device_606 = ttnn.from_device(ttnn_rsqrt_58, )
  ttnn_to_layout_638 = ttnn.to_layout(ttnn_from_device_606, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_398 = ttnn.reshape(ttnn_to_layout_638, (1, 672, 1, 1), )
  ttnn_from_torch_324 = ttnn.from_torch(arg446_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_399 = ttnn.reshape(ttnn_from_torch_324, (1, 672, 1, 1), )
  ttnn_from_device_607 = ttnn.from_device(ttnn_reshape_399, )
  ttnn_to_layout_639 = ttnn.to_layout(ttnn_from_device_607, ttnn.TILE_LAYOUT, )
  ttnn_to_device_398 = ttnn.to_device(ttnn_to_layout_639, device = device)
  ttnn_subtract_58 = ttnn.subtract(ttnn_permute_161, ttnn_to_device_398, )
  ttnn_from_device_608 = ttnn.from_device(ttnn_reshape_398, )
  ttnn_to_layout_640 = ttnn.to_layout(ttnn_from_device_608, ttnn.TILE_LAYOUT, )
  ttnn_to_device_399 = ttnn.to_device(ttnn_to_layout_640, device = device)
  ttnn_multiply_124 = ttnn.multiply(ttnn_subtract_58, ttnn_to_device_399, )
  ttnn_from_torch_325 = ttnn.from_torch(arg207_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_400 = ttnn.reshape(ttnn_from_torch_325, (1, 672, 1, 1), )
  ttnn_from_device_609 = ttnn.from_device(ttnn_reshape_400, )
  ttnn_to_layout_641 = ttnn.to_layout(ttnn_from_device_609, ttnn.TILE_LAYOUT, )
  ttnn_to_device_400 = ttnn.to_device(ttnn_to_layout_641, device = device)
  ttnn_multiply_125 = ttnn.multiply(ttnn_multiply_124, ttnn_to_device_400, )
  ttnn_from_torch_326 = ttnn.from_torch(arg208_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_401 = ttnn.reshape(ttnn_from_torch_326, (1, 672, 1, 1), )
  ttnn_from_device_610 = ttnn.from_device(ttnn_reshape_401, )
  ttnn_to_layout_642 = ttnn.to_layout(ttnn_from_device_610, ttnn.TILE_LAYOUT, )
  ttnn_to_device_401 = ttnn.to_device(ttnn_to_layout_642, device = device)
  ttnn_add_127 = ttnn.add(ttnn_multiply_125, ttnn_to_device_401, )
  ttnn_prefix_pack_to_tuple_58 = pack_to_tuple_wrapper(ttnn_add_127, )
  ttnn_prefix_getitem_58 = ttnn_prefix_pack_to_tuple_58[0]
  ttnn_hardtanh_12 = ttnn.hardtanh(ttnn_prefix_getitem_58, min_val = 0.0, max_val = 6.0)
  ttnn_permute_162 = ttnn.permute(ttnn_hardtanh_12, (0, 2, 3, 1), )
  ttnn_from_device_611 = ttnn.from_device(ttnn_permute_162, )
  ttnn_to_layout_643 = ttnn.to_layout(ttnn_from_device_611, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_402 = ttnn.reshape(ttnn_to_layout_643, (1, 1, 400, 672), )
  ttnn_from_torch_327 = ttnn.from_torch(arg210_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_403 = ttnn.reshape(ttnn_from_torch_327, (1, 1, 1, 24), )
  ttnn_from_device_612 = ttnn.from_device(ttnn_reshape_403, )
  ttnn_to_layout_644 = ttnn.to_layout(ttnn_from_device_612, ttnn.TILE_LAYOUT, )
  ttnn_to_device_402 = ttnn.to_device(ttnn_to_layout_644, device = device)
  ttnn_prefix_move_to_host_16 = move_to_host_wrapper(ttnn_to_device_402, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_613 = ttnn.from_device(ttnn_reshape_402, )
  ttnn_to_layout_645 = ttnn.to_layout(ttnn_from_device_613, ttnn.TILE_LAYOUT, )
  ttnn_to_device_403 = ttnn.to_device(ttnn_to_layout_645, device = device)
  ttnn_from_torch_328 = ttnn.from_torch(arg209_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_75 = conv_wrapper(ttnn_to_device_403, ttnn_from_torch_328, ttnn_prefix_move_to_host_16, 1, 672, 24, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_75 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_75, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_614 = ttnn.from_device(ttnn_sharded_to_interleaved_75, )
  ttnn_to_layout_646 = ttnn.to_layout(ttnn_from_device_614, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_404 = ttnn.reshape(ttnn_to_layout_646, (1, 20, 20, 24), )
  ttnn_from_device_615 = ttnn.from_device(ttnn_reshape_404, )
  ttnn_to_layout_647 = ttnn.to_layout(ttnn_from_device_615, ttnn.TILE_LAYOUT, )
  ttnn_to_device_404 = ttnn.to_device(ttnn_to_layout_647, device = device)
  ttnn_permute_163 = ttnn.permute(ttnn_to_device_404, (0, 3, 1, 2), )
  test_accuracy(convolution_75, ttnn_permute_163)
  ttnn_from_device_616 = ttnn.from_device(ttnn_permute_163, )
  ttnn_to_layout_648 = ttnn.to_layout(ttnn_from_device_616, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_405 = ttnn.reshape(ttnn_to_layout_648, (1, -1, 4, 20, 20), )
  test_accuracy(view, ttnn_reshape_405)
  ttnn_from_device_617 = ttnn.from_device(ttnn_reshape_405, )
  ttnn_to_layout_649 = ttnn.to_layout(ttnn_from_device_617, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_405 = ttnn.to_device(ttnn_to_layout_649, device = device)
  ttnn_permute = ttnn.permute(ttnn_to_device_405, (0, 3, 4, 1, 2), )
  test_accuracy(permute, ttnn_permute)
  ttnn_from_device_618 = ttnn.from_device(ttnn_permute, )
  ttnn_to_layout_650 = ttnn.to_layout(ttnn_from_device_618, ttnn.TILE_LAYOUT, )
  ttnn_to_device_406 = ttnn.to_device(ttnn_to_layout_650, device = device)
  ttnn_prefix_clone = clone_wrapper(ttnn_to_device_406, )
  test_accuracy(clone_1, ttnn_prefix_clone)
  ttnn_from_device_619 = ttnn.from_device(ttnn_prefix_clone, )
  ttnn_to_layout_651 = ttnn.to_layout(ttnn_from_device_619, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_406 = ttnn.reshape(ttnn_to_layout_651, (1, 2400, 4), )
  test_accuracy(_unsafe_view, ttnn_reshape_406)
  ttnn_to_layout_652 = ttnn.to_layout(ttnn_from_device_494, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_407 = ttnn.reshape(ttnn_to_layout_652, (1, 1, 100, 480), )
  ttnn_from_device_621 = ttnn.from_device(ttnn_reshape_407, )
  ttnn_to_layout_653 = ttnn.to_layout(ttnn_from_device_621, ttnn.TILE_LAYOUT, )
  ttnn_to_device_407 = ttnn.to_device(ttnn_to_layout_653, device = device)
  ttnn_from_torch_329 = ttnn.from_torch(arg211_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_76 = conv_wrapper(ttnn_to_device_407, ttnn_from_torch_329, None, 1, 480, 480, [10, 10], [3, 3], [1, 1], [1, 1], [1, 1], 480, device, False, None, )
  ttnn_sharded_to_interleaved_76 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_76, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_622 = ttnn.from_device(ttnn_sharded_to_interleaved_76, )
  ttnn_to_layout_654 = ttnn.to_layout(ttnn_from_device_622, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_408 = ttnn.reshape(ttnn_to_layout_654, (1, 10, 10, 480), )
  ttnn_from_device_623 = ttnn.from_device(ttnn_reshape_408, )
  ttnn_to_layout_655 = ttnn.to_layout(ttnn_from_device_623, ttnn.TILE_LAYOUT, )
  ttnn_to_device_408 = ttnn.to_device(ttnn_to_layout_655, device = device)
  ttnn_permute_165 = ttnn.permute(ttnn_to_device_408, (0, 3, 1, 2), )
  ttnn_from_torch_330 = ttnn.from_torch(arg450_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_128 = ttnn.add(ttnn_from_torch_330, 0.001, )
  ttnn_rsqrt_59 = ttnn.rsqrt(ttnn_add_128, )
  ttnn_from_device_624 = ttnn.from_device(ttnn_rsqrt_59, )
  ttnn_to_layout_656 = ttnn.to_layout(ttnn_from_device_624, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_409 = ttnn.reshape(ttnn_to_layout_656, (1, 480, 1, 1), )
  ttnn_from_torch_331 = ttnn.from_torch(arg449_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_410 = ttnn.reshape(ttnn_from_torch_331, (1, 480, 1, 1), )
  ttnn_from_device_625 = ttnn.from_device(ttnn_reshape_410, )
  ttnn_to_layout_657 = ttnn.to_layout(ttnn_from_device_625, ttnn.TILE_LAYOUT, )
  ttnn_to_device_409 = ttnn.to_device(ttnn_to_layout_657, device = device)
  ttnn_subtract_59 = ttnn.subtract(ttnn_permute_165, ttnn_to_device_409, )
  ttnn_from_device_626 = ttnn.from_device(ttnn_reshape_409, )
  ttnn_to_layout_658 = ttnn.to_layout(ttnn_from_device_626, ttnn.TILE_LAYOUT, )
  ttnn_to_device_410 = ttnn.to_device(ttnn_to_layout_658, device = device)
  ttnn_multiply_126 = ttnn.multiply(ttnn_subtract_59, ttnn_to_device_410, )
  ttnn_from_torch_332 = ttnn.from_torch(arg212_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_411 = ttnn.reshape(ttnn_from_torch_332, (1, 480, 1, 1), )
  ttnn_from_device_627 = ttnn.from_device(ttnn_reshape_411, )
  ttnn_to_layout_659 = ttnn.to_layout(ttnn_from_device_627, ttnn.TILE_LAYOUT, )
  ttnn_to_device_411 = ttnn.to_device(ttnn_to_layout_659, device = device)
  ttnn_multiply_127 = ttnn.multiply(ttnn_multiply_126, ttnn_to_device_411, )
  ttnn_from_torch_333 = ttnn.from_torch(arg213_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_412 = ttnn.reshape(ttnn_from_torch_333, (1, 480, 1, 1), )
  ttnn_from_device_628 = ttnn.from_device(ttnn_reshape_412, )
  ttnn_to_layout_660 = ttnn.to_layout(ttnn_from_device_628, ttnn.TILE_LAYOUT, )
  ttnn_to_device_412 = ttnn.to_device(ttnn_to_layout_660, device = device)
  ttnn_add_129 = ttnn.add(ttnn_multiply_127, ttnn_to_device_412, )
  ttnn_prefix_pack_to_tuple_59 = pack_to_tuple_wrapper(ttnn_add_129, )
  ttnn_prefix_getitem_59 = ttnn_prefix_pack_to_tuple_59[0]
  ttnn_hardtanh_13 = ttnn.hardtanh(ttnn_prefix_getitem_59, min_val = 0.0, max_val = 6.0)
  ttnn_permute_166 = ttnn.permute(ttnn_hardtanh_13, (0, 2, 3, 1), )
  ttnn_from_device_629 = ttnn.from_device(ttnn_permute_166, )
  ttnn_to_layout_661 = ttnn.to_layout(ttnn_from_device_629, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_413 = ttnn.reshape(ttnn_to_layout_661, (1, 1, 100, 480), )
  ttnn_from_torch_334 = ttnn.from_torch(arg215_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_414 = ttnn.reshape(ttnn_from_torch_334, (1, 1, 1, 24), )
  ttnn_from_device_630 = ttnn.from_device(ttnn_reshape_414, )
  ttnn_to_layout_662 = ttnn.to_layout(ttnn_from_device_630, ttnn.TILE_LAYOUT, )
  ttnn_to_device_413 = ttnn.to_device(ttnn_to_layout_662, device = device)
  ttnn_prefix_move_to_host_17 = move_to_host_wrapper(ttnn_to_device_413, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_631 = ttnn.from_device(ttnn_reshape_413, )
  ttnn_to_layout_663 = ttnn.to_layout(ttnn_from_device_631, ttnn.TILE_LAYOUT, )
  ttnn_to_device_414 = ttnn.to_device(ttnn_to_layout_663, device = device)
  ttnn_from_torch_335 = ttnn.from_torch(arg214_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_77 = conv_wrapper(ttnn_to_device_414, ttnn_from_torch_335, ttnn_prefix_move_to_host_17, 1, 480, 24, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_77 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_77, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_632 = ttnn.from_device(ttnn_sharded_to_interleaved_77, )
  ttnn_to_layout_664 = ttnn.to_layout(ttnn_from_device_632, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_415 = ttnn.reshape(ttnn_to_layout_664, (1, 10, 10, 24), )
  ttnn_from_device_633 = ttnn.from_device(ttnn_reshape_415, )
  ttnn_to_layout_665 = ttnn.to_layout(ttnn_from_device_633, ttnn.TILE_LAYOUT, )
  ttnn_to_device_415 = ttnn.to_device(ttnn_to_layout_665, device = device)
  ttnn_permute_167 = ttnn.permute(ttnn_to_device_415, (0, 3, 1, 2), )
  test_accuracy(convolution_77, ttnn_permute_167)
  ttnn_from_device_634 = ttnn.from_device(ttnn_permute_167, )
  ttnn_to_layout_666 = ttnn.to_layout(ttnn_from_device_634, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_416 = ttnn.reshape(ttnn_to_layout_666, (1, -1, 4, 10, 10), )
  test_accuracy(view_1, ttnn_reshape_416)
  ttnn_from_device_635 = ttnn.from_device(ttnn_reshape_416, )
  ttnn_to_layout_667 = ttnn.to_layout(ttnn_from_device_635, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_416 = ttnn.to_device(ttnn_to_layout_667, device = device)
  ttnn_permute_1 = ttnn.permute(ttnn_to_device_416, (0, 3, 4, 1, 2), )
  test_accuracy(permute_1, ttnn_permute_1)
  ttnn_from_device_636 = ttnn.from_device(ttnn_permute_1, )
  ttnn_to_layout_668 = ttnn.to_layout(ttnn_from_device_636, ttnn.TILE_LAYOUT, )
  ttnn_to_device_417 = ttnn.to_device(ttnn_to_layout_668, device = device)
  ttnn_prefix_clone_1 = clone_wrapper(ttnn_to_device_417, )
  test_accuracy(clone_2, ttnn_prefix_clone_1)
  ttnn_from_device_637 = ttnn.from_device(ttnn_prefix_clone_1, )
  ttnn_to_layout_669 = ttnn.to_layout(ttnn_from_device_637, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_417 = ttnn.reshape(ttnn_to_layout_669, (1, 600, 4), )
  test_accuracy(_unsafe_view_1, ttnn_reshape_417)
  ttnn_to_layout_670 = ttnn.to_layout(ttnn_from_device_521, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_418 = ttnn.reshape(ttnn_to_layout_670, (1, 1, 25, 512), )
  ttnn_from_device_639 = ttnn.from_device(ttnn_reshape_418, )
  ttnn_to_layout_671 = ttnn.to_layout(ttnn_from_device_639, ttnn.TILE_LAYOUT, )
  ttnn_to_device_418 = ttnn.to_device(ttnn_to_layout_671, device = device)
  ttnn_from_torch_336 = ttnn.from_torch(arg216_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_78 = conv_wrapper(ttnn_to_device_418, ttnn_from_torch_336, None, 1, 512, 512, [5, 5], [3, 3], [1, 1], [1, 1], [1, 1], 512, device, False, None, )
  ttnn_sharded_to_interleaved_78 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_78, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_640 = ttnn.from_device(ttnn_sharded_to_interleaved_78, )
  ttnn_to_layout_672 = ttnn.to_layout(ttnn_from_device_640, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_419 = ttnn.reshape(ttnn_to_layout_672, (1, 5, 5, 512), )
  ttnn_from_device_641 = ttnn.from_device(ttnn_reshape_419, )
  ttnn_to_layout_673 = ttnn.to_layout(ttnn_from_device_641, ttnn.TILE_LAYOUT, )
  ttnn_to_device_419 = ttnn.to_device(ttnn_to_layout_673, device = device)
  ttnn_permute_169 = ttnn.permute(ttnn_to_device_419, (0, 3, 1, 2), )
  ttnn_from_torch_337 = ttnn.from_torch(arg453_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_130 = ttnn.add(ttnn_from_torch_337, 0.001, )
  ttnn_rsqrt_60 = ttnn.rsqrt(ttnn_add_130, )
  ttnn_from_device_642 = ttnn.from_device(ttnn_rsqrt_60, )
  ttnn_to_layout_674 = ttnn.to_layout(ttnn_from_device_642, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_420 = ttnn.reshape(ttnn_to_layout_674, (1, 512, 1, 1), )
  ttnn_from_torch_338 = ttnn.from_torch(arg452_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_421 = ttnn.reshape(ttnn_from_torch_338, (1, 512, 1, 1), )
  ttnn_from_device_643 = ttnn.from_device(ttnn_reshape_421, )
  ttnn_to_layout_675 = ttnn.to_layout(ttnn_from_device_643, ttnn.TILE_LAYOUT, )
  ttnn_to_device_420 = ttnn.to_device(ttnn_to_layout_675, device = device)
  ttnn_subtract_60 = ttnn.subtract(ttnn_permute_169, ttnn_to_device_420, )
  ttnn_from_device_644 = ttnn.from_device(ttnn_reshape_420, )
  ttnn_to_layout_676 = ttnn.to_layout(ttnn_from_device_644, ttnn.TILE_LAYOUT, )
  ttnn_to_device_421 = ttnn.to_device(ttnn_to_layout_676, device = device)
  ttnn_multiply_128 = ttnn.multiply(ttnn_subtract_60, ttnn_to_device_421, )
  ttnn_from_torch_339 = ttnn.from_torch(arg217_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_422 = ttnn.reshape(ttnn_from_torch_339, (1, 512, 1, 1), )
  ttnn_from_device_645 = ttnn.from_device(ttnn_reshape_422, )
  ttnn_to_layout_677 = ttnn.to_layout(ttnn_from_device_645, ttnn.TILE_LAYOUT, )
  ttnn_to_device_422 = ttnn.to_device(ttnn_to_layout_677, device = device)
  ttnn_multiply_129 = ttnn.multiply(ttnn_multiply_128, ttnn_to_device_422, )
  ttnn_from_torch_340 = ttnn.from_torch(arg218_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_423 = ttnn.reshape(ttnn_from_torch_340, (1, 512, 1, 1), )
  ttnn_from_device_646 = ttnn.from_device(ttnn_reshape_423, )
  ttnn_to_layout_678 = ttnn.to_layout(ttnn_from_device_646, ttnn.TILE_LAYOUT, )
  ttnn_to_device_423 = ttnn.to_device(ttnn_to_layout_678, device = device)
  ttnn_add_131 = ttnn.add(ttnn_multiply_129, ttnn_to_device_423, )
  ttnn_prefix_pack_to_tuple_60 = pack_to_tuple_wrapper(ttnn_add_131, )
  ttnn_prefix_getitem_60 = ttnn_prefix_pack_to_tuple_60[0]
  ttnn_hardtanh_14 = ttnn.hardtanh(ttnn_prefix_getitem_60, min_val = 0.0, max_val = 6.0)
  ttnn_permute_170 = ttnn.permute(ttnn_hardtanh_14, (0, 2, 3, 1), )
  ttnn_from_device_647 = ttnn.from_device(ttnn_permute_170, )
  ttnn_to_layout_679 = ttnn.to_layout(ttnn_from_device_647, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_424 = ttnn.reshape(ttnn_to_layout_679, (1, 1, 25, 512), )
  ttnn_from_torch_341 = ttnn.from_torch(arg220_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_425 = ttnn.reshape(ttnn_from_torch_341, (1, 1, 1, 24), )
  ttnn_from_device_648 = ttnn.from_device(ttnn_reshape_425, )
  ttnn_to_layout_680 = ttnn.to_layout(ttnn_from_device_648, ttnn.TILE_LAYOUT, )
  ttnn_to_device_424 = ttnn.to_device(ttnn_to_layout_680, device = device)
  ttnn_prefix_move_to_host_18 = move_to_host_wrapper(ttnn_to_device_424, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_649 = ttnn.from_device(ttnn_reshape_424, )
  ttnn_to_layout_681 = ttnn.to_layout(ttnn_from_device_649, ttnn.TILE_LAYOUT, )
  ttnn_to_device_425 = ttnn.to_device(ttnn_to_layout_681, device = device)
  ttnn_from_torch_342 = ttnn.from_torch(arg219_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_79 = conv_wrapper(ttnn_to_device_425, ttnn_from_torch_342, ttnn_prefix_move_to_host_18, 1, 512, 24, [5, 5], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_79 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_79, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_650 = ttnn.from_device(ttnn_sharded_to_interleaved_79, )
  ttnn_to_layout_682 = ttnn.to_layout(ttnn_from_device_650, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_426 = ttnn.reshape(ttnn_to_layout_682, (1, 5, 5, 24), )
  ttnn_from_device_651 = ttnn.from_device(ttnn_reshape_426, )
  ttnn_to_layout_683 = ttnn.to_layout(ttnn_from_device_651, ttnn.TILE_LAYOUT, )
  ttnn_to_device_426 = ttnn.to_device(ttnn_to_layout_683, device = device)
  ttnn_permute_171 = ttnn.permute(ttnn_to_device_426, (0, 3, 1, 2), )
  test_accuracy(convolution_79, ttnn_permute_171)
  ttnn_from_device_652 = ttnn.from_device(ttnn_permute_171, )
  ttnn_to_layout_684 = ttnn.to_layout(ttnn_from_device_652, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_427 = ttnn.reshape(ttnn_to_layout_684, (1, -1, 4, 5, 5), )
  test_accuracy(view_2, ttnn_reshape_427)
  ttnn_from_device_653 = ttnn.from_device(ttnn_reshape_427, )
  ttnn_to_layout_685 = ttnn.to_layout(ttnn_from_device_653, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_427 = ttnn.to_device(ttnn_to_layout_685, device = device)
  ttnn_permute_2 = ttnn.permute(ttnn_to_device_427, (0, 3, 4, 1, 2), )
  test_accuracy(permute_2, ttnn_permute_2)
  ttnn_from_device_654 = ttnn.from_device(ttnn_permute_2, )
  ttnn_to_layout_686 = ttnn.to_layout(ttnn_from_device_654, ttnn.TILE_LAYOUT, )
  ttnn_to_device_428 = ttnn.to_device(ttnn_to_layout_686, device = device)
  ttnn_prefix_clone_2 = clone_wrapper(ttnn_to_device_428, )
  test_accuracy(clone_3, ttnn_prefix_clone_2)
  ttnn_from_device_655 = ttnn.from_device(ttnn_prefix_clone_2, )
  ttnn_to_layout_687 = ttnn.to_layout(ttnn_from_device_655, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_428 = ttnn.reshape(ttnn_to_layout_687, (1, 150, 4), )
  test_accuracy(_unsafe_view_2, ttnn_reshape_428)
  ttnn_to_layout_688 = ttnn.to_layout(ttnn_from_device_548, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_429 = ttnn.reshape(ttnn_to_layout_688, (1, 1, 9, 256), )
  ttnn_from_device_657 = ttnn.from_device(ttnn_reshape_429, )
  ttnn_to_layout_689 = ttnn.to_layout(ttnn_from_device_657, ttnn.TILE_LAYOUT, )
  ttnn_to_device_429 = ttnn.to_device(ttnn_to_layout_689, device = device)
  ttnn_from_torch_343 = ttnn.from_torch(arg221_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_80 = conv_wrapper(ttnn_to_device_429, ttnn_from_torch_343, None, 1, 256, 256, [3, 3], [3, 3], [1, 1], [1, 1], [1, 1], 256, device, False, None, )
  ttnn_sharded_to_interleaved_80 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_80, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_658 = ttnn.from_device(ttnn_sharded_to_interleaved_80, )
  ttnn_to_layout_690 = ttnn.to_layout(ttnn_from_device_658, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_430 = ttnn.reshape(ttnn_to_layout_690, (1, 3, 3, 256), )
  ttnn_from_device_659 = ttnn.from_device(ttnn_reshape_430, )
  ttnn_to_layout_691 = ttnn.to_layout(ttnn_from_device_659, ttnn.TILE_LAYOUT, )
  ttnn_to_device_430 = ttnn.to_device(ttnn_to_layout_691, device = device)
  ttnn_permute_173 = ttnn.permute(ttnn_to_device_430, (0, 3, 1, 2), )
  ttnn_from_torch_344 = ttnn.from_torch(arg456_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_132 = ttnn.add(ttnn_from_torch_344, 0.001, )
  ttnn_rsqrt_61 = ttnn.rsqrt(ttnn_add_132, )
  ttnn_from_device_660 = ttnn.from_device(ttnn_rsqrt_61, )
  ttnn_to_layout_692 = ttnn.to_layout(ttnn_from_device_660, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_431 = ttnn.reshape(ttnn_to_layout_692, (1, 256, 1, 1), )
  ttnn_from_torch_345 = ttnn.from_torch(arg455_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_432 = ttnn.reshape(ttnn_from_torch_345, (1, 256, 1, 1), )
  ttnn_from_device_661 = ttnn.from_device(ttnn_reshape_432, )
  ttnn_to_layout_693 = ttnn.to_layout(ttnn_from_device_661, ttnn.TILE_LAYOUT, )
  ttnn_to_device_431 = ttnn.to_device(ttnn_to_layout_693, device = device)
  ttnn_subtract_61 = ttnn.subtract(ttnn_permute_173, ttnn_to_device_431, )
  ttnn_from_device_662 = ttnn.from_device(ttnn_reshape_431, )
  ttnn_to_layout_694 = ttnn.to_layout(ttnn_from_device_662, ttnn.TILE_LAYOUT, )
  ttnn_to_device_432 = ttnn.to_device(ttnn_to_layout_694, device = device)
  ttnn_multiply_130 = ttnn.multiply(ttnn_subtract_61, ttnn_to_device_432, )
  ttnn_from_torch_346 = ttnn.from_torch(arg222_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_433 = ttnn.reshape(ttnn_from_torch_346, (1, 256, 1, 1), )
  ttnn_from_device_663 = ttnn.from_device(ttnn_reshape_433, )
  ttnn_to_layout_695 = ttnn.to_layout(ttnn_from_device_663, ttnn.TILE_LAYOUT, )
  ttnn_to_device_433 = ttnn.to_device(ttnn_to_layout_695, device = device)
  ttnn_multiply_131 = ttnn.multiply(ttnn_multiply_130, ttnn_to_device_433, )
  ttnn_from_torch_347 = ttnn.from_torch(arg223_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_434 = ttnn.reshape(ttnn_from_torch_347, (1, 256, 1, 1), )
  ttnn_from_device_664 = ttnn.from_device(ttnn_reshape_434, )
  ttnn_to_layout_696 = ttnn.to_layout(ttnn_from_device_664, ttnn.TILE_LAYOUT, )
  ttnn_to_device_434 = ttnn.to_device(ttnn_to_layout_696, device = device)
  ttnn_add_133 = ttnn.add(ttnn_multiply_131, ttnn_to_device_434, )
  ttnn_prefix_pack_to_tuple_61 = pack_to_tuple_wrapper(ttnn_add_133, )
  ttnn_prefix_getitem_61 = ttnn_prefix_pack_to_tuple_61[0]
  ttnn_hardtanh_15 = ttnn.hardtanh(ttnn_prefix_getitem_61, min_val = 0.0, max_val = 6.0)
  ttnn_permute_174 = ttnn.permute(ttnn_hardtanh_15, (0, 2, 3, 1), )
  ttnn_from_device_665 = ttnn.from_device(ttnn_permute_174, )
  ttnn_to_layout_697 = ttnn.to_layout(ttnn_from_device_665, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_435 = ttnn.reshape(ttnn_to_layout_697, (1, 1, 9, 256), )
  ttnn_from_torch_348 = ttnn.from_torch(arg225_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_436 = ttnn.reshape(ttnn_from_torch_348, (1, 1, 1, 24), )
  ttnn_from_device_666 = ttnn.from_device(ttnn_reshape_436, )
  ttnn_to_layout_698 = ttnn.to_layout(ttnn_from_device_666, ttnn.TILE_LAYOUT, )
  ttnn_to_device_435 = ttnn.to_device(ttnn_to_layout_698, device = device)
  ttnn_prefix_move_to_host_19 = move_to_host_wrapper(ttnn_to_device_435, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_667 = ttnn.from_device(ttnn_reshape_435, )
  ttnn_to_layout_699 = ttnn.to_layout(ttnn_from_device_667, ttnn.TILE_LAYOUT, )
  ttnn_to_device_436 = ttnn.to_device(ttnn_to_layout_699, device = device)
  ttnn_from_torch_349 = ttnn.from_torch(arg224_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_81 = conv_wrapper(ttnn_to_device_436, ttnn_from_torch_349, ttnn_prefix_move_to_host_19, 1, 256, 24, [3, 3], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_81 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_81, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_668 = ttnn.from_device(ttnn_sharded_to_interleaved_81, )
  ttnn_to_layout_700 = ttnn.to_layout(ttnn_from_device_668, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_437 = ttnn.reshape(ttnn_to_layout_700, (1, 3, 3, 24), )
  ttnn_from_device_669 = ttnn.from_device(ttnn_reshape_437, )
  ttnn_to_layout_701 = ttnn.to_layout(ttnn_from_device_669, ttnn.TILE_LAYOUT, )
  ttnn_to_device_437 = ttnn.to_device(ttnn_to_layout_701, device = device)
  ttnn_permute_175 = ttnn.permute(ttnn_to_device_437, (0, 3, 1, 2), )
  test_accuracy(convolution_81, ttnn_permute_175)
  ttnn_from_device_670 = ttnn.from_device(ttnn_permute_175, )
  ttnn_to_layout_702 = ttnn.to_layout(ttnn_from_device_670, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_438 = ttnn.reshape(ttnn_to_layout_702, (1, -1, 4, 3, 3), )
  test_accuracy(view_3, ttnn_reshape_438)
  ttnn_from_device_671 = ttnn.from_device(ttnn_reshape_438, )
  ttnn_to_layout_703 = ttnn.to_layout(ttnn_from_device_671, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_438 = ttnn.to_device(ttnn_to_layout_703, device = device)
  ttnn_permute_3 = ttnn.permute(ttnn_to_device_438, (0, 3, 4, 1, 2), )
  test_accuracy(permute_3, ttnn_permute_3)
  ttnn_from_device_672 = ttnn.from_device(ttnn_permute_3, )
  ttnn_to_layout_704 = ttnn.to_layout(ttnn_from_device_672, ttnn.TILE_LAYOUT, )
  ttnn_to_device_439 = ttnn.to_device(ttnn_to_layout_704, device = device)
  ttnn_prefix_clone_3 = clone_wrapper(ttnn_to_device_439, )
  test_accuracy(clone_4, ttnn_prefix_clone_3)
  ttnn_from_device_673 = ttnn.from_device(ttnn_prefix_clone_3, )
  ttnn_to_layout_705 = ttnn.to_layout(ttnn_from_device_673, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_439 = ttnn.reshape(ttnn_to_layout_705, (1, 54, 4), )
  test_accuracy(_unsafe_view_3, ttnn_reshape_439)
  ttnn_to_layout_706 = ttnn.to_layout(ttnn_from_device_575, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_440 = ttnn.reshape(ttnn_to_layout_706, (1, 1, 4, 256), )
  ttnn_from_device_675 = ttnn.from_device(ttnn_reshape_440, )
  ttnn_to_layout_707 = ttnn.to_layout(ttnn_from_device_675, ttnn.TILE_LAYOUT, )
  ttnn_to_device_440 = ttnn.to_device(ttnn_to_layout_707, device = device)
  ttnn_from_torch_350 = ttnn.from_torch(arg226_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_82 = conv_wrapper(ttnn_to_device_440, ttnn_from_torch_350, None, 1, 256, 256, [2, 2], [3, 3], [1, 1], [1, 1], [1, 1], 256, device, False, None, )
  ttnn_sharded_to_interleaved_82 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_82, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_676 = ttnn.from_device(ttnn_sharded_to_interleaved_82, )
  ttnn_to_layout_708 = ttnn.to_layout(ttnn_from_device_676, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_441 = ttnn.reshape(ttnn_to_layout_708, (1, 2, 2, 256), )
  ttnn_from_device_677 = ttnn.from_device(ttnn_reshape_441, )
  ttnn_to_layout_709 = ttnn.to_layout(ttnn_from_device_677, ttnn.TILE_LAYOUT, )
  ttnn_to_device_441 = ttnn.to_device(ttnn_to_layout_709, device = device)
  ttnn_permute_177 = ttnn.permute(ttnn_to_device_441, (0, 3, 1, 2), )
  ttnn_from_torch_351 = ttnn.from_torch(arg459_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_134 = ttnn.add(ttnn_from_torch_351, 0.001, )
  ttnn_rsqrt_62 = ttnn.rsqrt(ttnn_add_134, )
  ttnn_from_device_678 = ttnn.from_device(ttnn_rsqrt_62, )
  ttnn_to_layout_710 = ttnn.to_layout(ttnn_from_device_678, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_442 = ttnn.reshape(ttnn_to_layout_710, (1, 256, 1, 1), )
  ttnn_from_torch_352 = ttnn.from_torch(arg458_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_443 = ttnn.reshape(ttnn_from_torch_352, (1, 256, 1, 1), )
  ttnn_from_device_679 = ttnn.from_device(ttnn_reshape_443, )
  ttnn_to_layout_711 = ttnn.to_layout(ttnn_from_device_679, ttnn.TILE_LAYOUT, )
  ttnn_to_device_442 = ttnn.to_device(ttnn_to_layout_711, device = device)
  ttnn_subtract_62 = ttnn.subtract(ttnn_permute_177, ttnn_to_device_442, )
  ttnn_from_device_680 = ttnn.from_device(ttnn_reshape_442, )
  ttnn_to_layout_712 = ttnn.to_layout(ttnn_from_device_680, ttnn.TILE_LAYOUT, )
  ttnn_to_device_443 = ttnn.to_device(ttnn_to_layout_712, device = device)
  ttnn_multiply_132 = ttnn.multiply(ttnn_subtract_62, ttnn_to_device_443, )
  ttnn_from_torch_353 = ttnn.from_torch(arg227_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_444 = ttnn.reshape(ttnn_from_torch_353, (1, 256, 1, 1), )
  ttnn_from_device_681 = ttnn.from_device(ttnn_reshape_444, )
  ttnn_to_layout_713 = ttnn.to_layout(ttnn_from_device_681, ttnn.TILE_LAYOUT, )
  ttnn_to_device_444 = ttnn.to_device(ttnn_to_layout_713, device = device)
  ttnn_multiply_133 = ttnn.multiply(ttnn_multiply_132, ttnn_to_device_444, )
  ttnn_from_torch_354 = ttnn.from_torch(arg228_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_445 = ttnn.reshape(ttnn_from_torch_354, (1, 256, 1, 1), )
  ttnn_from_device_682 = ttnn.from_device(ttnn_reshape_445, )
  ttnn_to_layout_714 = ttnn.to_layout(ttnn_from_device_682, ttnn.TILE_LAYOUT, )
  ttnn_to_device_445 = ttnn.to_device(ttnn_to_layout_714, device = device)
  ttnn_add_135 = ttnn.add(ttnn_multiply_133, ttnn_to_device_445, )
  ttnn_prefix_pack_to_tuple_62 = pack_to_tuple_wrapper(ttnn_add_135, )
  ttnn_prefix_getitem_62 = ttnn_prefix_pack_to_tuple_62[0]
  ttnn_hardtanh_16 = ttnn.hardtanh(ttnn_prefix_getitem_62, min_val = 0.0, max_val = 6.0)
  ttnn_permute_178 = ttnn.permute(ttnn_hardtanh_16, (0, 2, 3, 1), )
  ttnn_from_device_683 = ttnn.from_device(ttnn_permute_178, )
  ttnn_to_layout_715 = ttnn.to_layout(ttnn_from_device_683, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_446 = ttnn.reshape(ttnn_to_layout_715, (1, 1, 4, 256), )
  ttnn_from_torch_355 = ttnn.from_torch(arg230_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_447 = ttnn.reshape(ttnn_from_torch_355, (1, 1, 1, 24), )
  ttnn_from_device_684 = ttnn.from_device(ttnn_reshape_447, )
  ttnn_to_layout_716 = ttnn.to_layout(ttnn_from_device_684, ttnn.TILE_LAYOUT, )
  ttnn_to_device_446 = ttnn.to_device(ttnn_to_layout_716, device = device)
  ttnn_prefix_move_to_host_20 = move_to_host_wrapper(ttnn_to_device_446, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_685 = ttnn.from_device(ttnn_reshape_446, )
  ttnn_to_layout_717 = ttnn.to_layout(ttnn_from_device_685, ttnn.TILE_LAYOUT, )
  ttnn_to_device_447 = ttnn.to_device(ttnn_to_layout_717, device = device)
  ttnn_from_torch_356 = ttnn.from_torch(arg229_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_83 = conv_wrapper(ttnn_to_device_447, ttnn_from_torch_356, ttnn_prefix_move_to_host_20, 1, 256, 24, [2, 2], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_83 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_83, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_686 = ttnn.from_device(ttnn_sharded_to_interleaved_83, )
  ttnn_to_layout_718 = ttnn.to_layout(ttnn_from_device_686, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_448 = ttnn.reshape(ttnn_to_layout_718, (1, 2, 2, 24), )
  ttnn_from_device_687 = ttnn.from_device(ttnn_reshape_448, )
  ttnn_to_layout_719 = ttnn.to_layout(ttnn_from_device_687, ttnn.TILE_LAYOUT, )
  ttnn_to_device_448 = ttnn.to_device(ttnn_to_layout_719, device = device)
  ttnn_permute_179 = ttnn.permute(ttnn_to_device_448, (0, 3, 1, 2), )
  test_accuracy(convolution_83, ttnn_permute_179)
  ttnn_from_device_688 = ttnn.from_device(ttnn_permute_179, )
  ttnn_to_layout_720 = ttnn.to_layout(ttnn_from_device_688, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_449 = ttnn.reshape(ttnn_to_layout_720, (1, -1, 4, 2, 2), )
  test_accuracy(view_4, ttnn_reshape_449)
  ttnn_from_device_689 = ttnn.from_device(ttnn_reshape_449, )
  ttnn_to_layout_721 = ttnn.to_layout(ttnn_from_device_689, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_449 = ttnn.to_device(ttnn_to_layout_721, device = device)
  ttnn_permute_4 = ttnn.permute(ttnn_to_device_449, (0, 3, 4, 1, 2), )
  test_accuracy(permute_4, ttnn_permute_4)
  ttnn_from_device_690 = ttnn.from_device(ttnn_permute_4, )
  ttnn_to_layout_722 = ttnn.to_layout(ttnn_from_device_690, ttnn.TILE_LAYOUT, )
  ttnn_to_device_450 = ttnn.to_device(ttnn_to_layout_722, device = device)
  ttnn_prefix_clone_4 = clone_wrapper(ttnn_to_device_450, )
  test_accuracy(clone_5, ttnn_prefix_clone_4)
  ttnn_from_device_691 = ttnn.from_device(ttnn_prefix_clone_4, )
  ttnn_to_layout_723 = ttnn.to_layout(ttnn_from_device_691, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_450 = ttnn.reshape(ttnn_to_layout_723, (1, 24, 4), )
  test_accuracy(_unsafe_view_4, ttnn_reshape_450)
  ttnn_permute_180 = ttnn.permute(ttnn_hardtanh_11, (0, 2, 3, 1), )
  ttnn_from_device_692 = ttnn.from_device(ttnn_permute_180, )
  ttnn_to_layout_724 = ttnn.to_layout(ttnn_from_device_692, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_451 = ttnn.reshape(ttnn_to_layout_724, (1, 1, 1, 128), )
  ttnn_from_device_693 = ttnn.from_device(ttnn_reshape_451, )
  ttnn_to_layout_725 = ttnn.to_layout(ttnn_from_device_693, ttnn.TILE_LAYOUT, )
  ttnn_to_device_451 = ttnn.to_device(ttnn_to_layout_725, device = device)
  ttnn_from_torch_357 = ttnn.from_torch(arg231_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_84 = conv_wrapper(ttnn_to_device_451, ttnn_from_torch_357, None, 1, 128, 128, [1, 1], [3, 3], [1, 1], [1, 1], [1, 1], 128, device, False, None, )
  ttnn_sharded_to_interleaved_84 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_84, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_694 = ttnn.from_device(ttnn_sharded_to_interleaved_84, )
  ttnn_to_layout_726 = ttnn.to_layout(ttnn_from_device_694, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_452 = ttnn.reshape(ttnn_to_layout_726, (1, 1, 1, 128), )
  ttnn_from_device_695 = ttnn.from_device(ttnn_reshape_452, )
  ttnn_to_layout_727 = ttnn.to_layout(ttnn_from_device_695, ttnn.TILE_LAYOUT, )
  ttnn_to_device_452 = ttnn.to_device(ttnn_to_layout_727, device = device)
  ttnn_permute_181 = ttnn.permute(ttnn_to_device_452, (0, 3, 1, 2), )
  ttnn_from_torch_358 = ttnn.from_torch(arg462_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_136 = ttnn.add(ttnn_from_torch_358, 0.001, )
  ttnn_rsqrt_63 = ttnn.rsqrt(ttnn_add_136, )
  ttnn_from_device_696 = ttnn.from_device(ttnn_rsqrt_63, )
  ttnn_to_layout_728 = ttnn.to_layout(ttnn_from_device_696, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_453 = ttnn.reshape(ttnn_to_layout_728, (1, 128, 1, 1), )
  ttnn_from_torch_359 = ttnn.from_torch(arg461_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_454 = ttnn.reshape(ttnn_from_torch_359, (1, 128, 1, 1), )
  ttnn_from_device_697 = ttnn.from_device(ttnn_reshape_454, )
  ttnn_to_layout_729 = ttnn.to_layout(ttnn_from_device_697, ttnn.TILE_LAYOUT, )
  ttnn_to_device_453 = ttnn.to_device(ttnn_to_layout_729, device = device)
  ttnn_subtract_63 = ttnn.subtract(ttnn_permute_181, ttnn_to_device_453, )
  ttnn_from_device_698 = ttnn.from_device(ttnn_reshape_453, )
  ttnn_to_layout_730 = ttnn.to_layout(ttnn_from_device_698, ttnn.TILE_LAYOUT, )
  ttnn_to_device_454 = ttnn.to_device(ttnn_to_layout_730, device = device)
  ttnn_multiply_134 = ttnn.multiply(ttnn_subtract_63, ttnn_to_device_454, )
  ttnn_from_torch_360 = ttnn.from_torch(arg232_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_455 = ttnn.reshape(ttnn_from_torch_360, (1, 128, 1, 1), )
  ttnn_from_device_699 = ttnn.from_device(ttnn_reshape_455, )
  ttnn_to_layout_731 = ttnn.to_layout(ttnn_from_device_699, ttnn.TILE_LAYOUT, )
  ttnn_to_device_455 = ttnn.to_device(ttnn_to_layout_731, device = device)
  ttnn_multiply_135 = ttnn.multiply(ttnn_multiply_134, ttnn_to_device_455, )
  ttnn_from_torch_361 = ttnn.from_torch(arg233_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_456 = ttnn.reshape(ttnn_from_torch_361, (1, 128, 1, 1), )
  ttnn_from_device_700 = ttnn.from_device(ttnn_reshape_456, )
  ttnn_to_layout_732 = ttnn.to_layout(ttnn_from_device_700, ttnn.TILE_LAYOUT, )
  ttnn_to_device_456 = ttnn.to_device(ttnn_to_layout_732, device = device)
  ttnn_add_137 = ttnn.add(ttnn_multiply_135, ttnn_to_device_456, )
  ttnn_prefix_pack_to_tuple_63 = pack_to_tuple_wrapper(ttnn_add_137, )
  ttnn_prefix_getitem_63 = ttnn_prefix_pack_to_tuple_63[0]
  ttnn_hardtanh_17 = ttnn.hardtanh(ttnn_prefix_getitem_63, min_val = 0.0, max_val = 6.0)
  ttnn_permute_182 = ttnn.permute(ttnn_hardtanh_17, (0, 2, 3, 1), )
  ttnn_from_device_701 = ttnn.from_device(ttnn_permute_182, )
  ttnn_to_layout_733 = ttnn.to_layout(ttnn_from_device_701, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_457 = ttnn.reshape(ttnn_to_layout_733, (1, 1, 1, 128), )
  ttnn_from_torch_362 = ttnn.from_torch(arg235_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_458 = ttnn.reshape(ttnn_from_torch_362, (1, 1, 1, 24), )
  ttnn_from_device_702 = ttnn.from_device(ttnn_reshape_458, )
  ttnn_to_layout_734 = ttnn.to_layout(ttnn_from_device_702, ttnn.TILE_LAYOUT, )
  ttnn_to_device_457 = ttnn.to_device(ttnn_to_layout_734, device = device)
  ttnn_prefix_move_to_host_21 = move_to_host_wrapper(ttnn_to_device_457, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_703 = ttnn.from_device(ttnn_reshape_457, )
  ttnn_to_layout_735 = ttnn.to_layout(ttnn_from_device_703, ttnn.TILE_LAYOUT, )
  ttnn_to_device_458 = ttnn.to_device(ttnn_to_layout_735, device = device)
  ttnn_from_torch_363 = ttnn.from_torch(arg234_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_85 = conv_wrapper(ttnn_to_device_458, ttnn_from_torch_363, ttnn_prefix_move_to_host_21, 1, 128, 24, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_85 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_85, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_704 = ttnn.from_device(ttnn_sharded_to_interleaved_85, )
  ttnn_to_layout_736 = ttnn.to_layout(ttnn_from_device_704, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_459 = ttnn.reshape(ttnn_to_layout_736, (1, 1, 1, 24), )
  ttnn_from_device_705 = ttnn.from_device(ttnn_reshape_459, )
  ttnn_to_layout_737 = ttnn.to_layout(ttnn_from_device_705, ttnn.TILE_LAYOUT, )
  ttnn_to_device_459 = ttnn.to_device(ttnn_to_layout_737, device = device)
  ttnn_permute_183 = ttnn.permute(ttnn_to_device_459, (0, 3, 1, 2), )
  test_accuracy(convolution_85, ttnn_permute_183)
  ttnn_from_device_706 = ttnn.from_device(ttnn_permute_183, )
  ttnn_to_layout_738 = ttnn.to_layout(ttnn_from_device_706, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_460 = ttnn.reshape(ttnn_to_layout_738, (1, -1, 4, 1, 1), )
  test_accuracy(view_5, ttnn_reshape_460)
  ttnn_from_device_707 = ttnn.from_device(ttnn_reshape_460, )
  ttnn_to_layout_739 = ttnn.to_layout(ttnn_from_device_707, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_460 = ttnn.to_device(ttnn_to_layout_739, device = device)
  ttnn_permute_5 = ttnn.permute(ttnn_to_device_460, (0, 3, 4, 1, 2), )
  test_accuracy(permute_5, ttnn_permute_5)
  ttnn_from_device_708 = ttnn.from_device(ttnn_permute_5, )
  ttnn_to_layout_740 = ttnn.to_layout(ttnn_from_device_708, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_461 = ttnn.reshape(ttnn_to_layout_740, (1, -1, 4), )
  test_accuracy(view_6, ttnn_reshape_461)
  ttnn_from_device_709 = ttnn.from_device(ttnn_reshape_406, )
  ttnn_to_layout_741 = ttnn.to_layout(ttnn_from_device_709, ttnn.TILE_LAYOUT, )
  ttnn_to_device_461 = ttnn.to_device(ttnn_to_layout_741, device = device)
  ttnn_to_layout = ttnn.to_layout(ttnn_to_device_461, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_710 = ttnn.from_device(ttnn_reshape_417, )
  ttnn_to_layout_742 = ttnn.to_layout(ttnn_from_device_710, ttnn.TILE_LAYOUT, )
  ttnn_to_device_462 = ttnn.to_device(ttnn_to_layout_742, device = device)
  ttnn_to_layout_1 = ttnn.to_layout(ttnn_to_device_462, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_711 = ttnn.from_device(ttnn_reshape_428, )
  ttnn_to_layout_743 = ttnn.to_layout(ttnn_from_device_711, ttnn.TILE_LAYOUT, )
  ttnn_to_device_463 = ttnn.to_device(ttnn_to_layout_743, device = device)
  ttnn_to_layout_2 = ttnn.to_layout(ttnn_to_device_463, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_712 = ttnn.from_device(ttnn_reshape_439, )
  ttnn_to_layout_744 = ttnn.to_layout(ttnn_from_device_712, ttnn.TILE_LAYOUT, )
  ttnn_to_device_464 = ttnn.to_device(ttnn_to_layout_744, device = device)
  ttnn_to_layout_3 = ttnn.to_layout(ttnn_to_device_464, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_713 = ttnn.from_device(ttnn_reshape_450, )
  ttnn_to_layout_745 = ttnn.to_layout(ttnn_from_device_713, ttnn.TILE_LAYOUT, )
  ttnn_to_device_465 = ttnn.to_device(ttnn_to_layout_745, device = device)
  ttnn_to_layout_4 = ttnn.to_layout(ttnn_to_device_465, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_714 = ttnn.from_device(ttnn_reshape_461, )
  ttnn_to_layout_746 = ttnn.to_layout(ttnn_from_device_714, ttnn.TILE_LAYOUT, )
  ttnn_to_device_466 = ttnn.to_device(ttnn_to_layout_746, device = device)
  ttnn_to_layout_5 = ttnn.to_layout(ttnn_to_device_466, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat = ttnn.concat([ttnn_to_layout, ttnn_to_layout_1, ttnn_to_layout_2, ttnn_to_layout_3, ttnn_to_layout_4, ttnn_to_layout_5], 1, )
  test_accuracy(cat, ttnn_concat)
  ttnn_to_layout_747 = ttnn.to_layout(ttnn_from_device_383, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_462 = ttnn.reshape(ttnn_to_layout_747, (1, 1, 400, 672), )
  ttnn_from_device_716 = ttnn.from_device(ttnn_reshape_462, )
  ttnn_to_layout_748 = ttnn.to_layout(ttnn_from_device_716, ttnn.TILE_LAYOUT, )
  ttnn_to_device_467 = ttnn.to_device(ttnn_to_layout_748, device = device)
  ttnn_from_torch_364 = ttnn.from_torch(arg236_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_86 = conv_wrapper(ttnn_to_device_467, ttnn_from_torch_364, None, 1, 672, 672, [20, 20], [3, 3], [1, 1], [1, 1], [1, 1], 672, device, False, None, )
  ttnn_sharded_to_interleaved_86 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_86, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_717 = ttnn.from_device(ttnn_sharded_to_interleaved_86, )
  ttnn_to_layout_749 = ttnn.to_layout(ttnn_from_device_717, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_463 = ttnn.reshape(ttnn_to_layout_749, (1, 20, 20, 672), )
  ttnn_from_device_718 = ttnn.from_device(ttnn_reshape_463, )
  ttnn_to_layout_750 = ttnn.to_layout(ttnn_from_device_718, ttnn.TILE_LAYOUT, )
  ttnn_to_device_468 = ttnn.to_device(ttnn_to_layout_750, device = device)
  ttnn_permute_185 = ttnn.permute(ttnn_to_device_468, (0, 3, 1, 2), )
  test_accuracy(convolution_86, ttnn_permute_185)
  ttnn_from_torch_365 = ttnn.from_torch(arg465_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_138 = ttnn.add(ttnn_from_torch_365, 0.001, )
  ttnn_rsqrt_64 = ttnn.rsqrt(ttnn_add_138, )
  ttnn_from_device_719 = ttnn.from_device(ttnn_rsqrt_64, )
  ttnn_to_layout_751 = ttnn.to_layout(ttnn_from_device_719, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_464 = ttnn.reshape(ttnn_to_layout_751, (1, 672, 1, 1), )
  ttnn_from_torch_366 = ttnn.from_torch(arg464_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_465 = ttnn.reshape(ttnn_from_torch_366, (1, 672, 1, 1), )
  ttnn_from_device_720 = ttnn.from_device(ttnn_reshape_465, )
  ttnn_to_layout_752 = ttnn.to_layout(ttnn_from_device_720, ttnn.TILE_LAYOUT, )
  ttnn_to_device_469 = ttnn.to_device(ttnn_to_layout_752, device = device)
  ttnn_subtract_64 = ttnn.subtract(ttnn_permute_185, ttnn_to_device_469, )
  ttnn_from_device_721 = ttnn.from_device(ttnn_reshape_464, )
  ttnn_to_layout_753 = ttnn.to_layout(ttnn_from_device_721, ttnn.TILE_LAYOUT, )
  ttnn_to_device_470 = ttnn.to_device(ttnn_to_layout_753, device = device)
  ttnn_multiply_136 = ttnn.multiply(ttnn_subtract_64, ttnn_to_device_470, )
  ttnn_from_torch_367 = ttnn.from_torch(arg237_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_466 = ttnn.reshape(ttnn_from_torch_367, (1, 672, 1, 1), )
  ttnn_from_device_722 = ttnn.from_device(ttnn_reshape_466, )
  ttnn_to_layout_754 = ttnn.to_layout(ttnn_from_device_722, ttnn.TILE_LAYOUT, )
  ttnn_to_device_471 = ttnn.to_device(ttnn_to_layout_754, device = device)
  ttnn_multiply_137 = ttnn.multiply(ttnn_multiply_136, ttnn_to_device_471, )
  ttnn_from_torch_368 = ttnn.from_torch(arg238_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_467 = ttnn.reshape(ttnn_from_torch_368, (1, 672, 1, 1), )
  ttnn_from_device_723 = ttnn.from_device(ttnn_reshape_467, )
  ttnn_to_layout_755 = ttnn.to_layout(ttnn_from_device_723, ttnn.TILE_LAYOUT, )
  ttnn_to_device_472 = ttnn.to_device(ttnn_to_layout_755, device = device)
  ttnn_add_139 = ttnn.add(ttnn_multiply_137, ttnn_to_device_472, )
  ttnn_prefix_pack_to_tuple_64 = pack_to_tuple_wrapper(ttnn_add_139, )
  ttnn_prefix_getitem_64 = ttnn_prefix_pack_to_tuple_64[0]
  test_accuracy(getitem_192, ttnn_prefix_getitem_64)
  ttnn_hardtanh_18 = ttnn.hardtanh(ttnn_prefix_getitem_64, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_18, ttnn_hardtanh_18)
  ttnn_permute_186 = ttnn.permute(ttnn_hardtanh_18, (0, 2, 3, 1), )
  ttnn_from_device_724 = ttnn.from_device(ttnn_permute_186, )
  ttnn_to_layout_756 = ttnn.to_layout(ttnn_from_device_724, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_468 = ttnn.reshape(ttnn_to_layout_756, (1, 1, 400, 672), )
  ttnn_from_torch_369 = ttnn.from_torch(arg240_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_469 = ttnn.reshape(ttnn_from_torch_369, (1, 1, 1, 546), )
  ttnn_from_device_725 = ttnn.from_device(ttnn_reshape_469, )
  ttnn_to_layout_757 = ttnn.to_layout(ttnn_from_device_725, ttnn.TILE_LAYOUT, )
  ttnn_to_device_473 = ttnn.to_device(ttnn_to_layout_757, device = device)
  ttnn_prefix_move_to_host_22 = move_to_host_wrapper(ttnn_to_device_473, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_726 = ttnn.from_device(ttnn_reshape_468, )
  ttnn_to_layout_758 = ttnn.to_layout(ttnn_from_device_726, ttnn.TILE_LAYOUT, )
  ttnn_to_device_474 = ttnn.to_device(ttnn_to_layout_758, device = device)
  ttnn_from_torch_370 = ttnn.from_torch(arg239_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_87 = conv_wrapper(ttnn_to_device_474, ttnn_from_torch_370, ttnn_prefix_move_to_host_22, 1, 672, 546, [20, 20], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_87 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_87, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_727 = ttnn.from_device(ttnn_sharded_to_interleaved_87, )
  ttnn_to_layout_759 = ttnn.to_layout(ttnn_from_device_727, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_470 = ttnn.reshape(ttnn_to_layout_759, (1, 20, 20, 546), )
  ttnn_from_device_728 = ttnn.from_device(ttnn_reshape_470, )
  ttnn_to_layout_760 = ttnn.to_layout(ttnn_from_device_728, ttnn.TILE_LAYOUT, )
  ttnn_to_device_475 = ttnn.to_device(ttnn_to_layout_760, device = device)
  ttnn_permute_187 = ttnn.permute(ttnn_to_device_475, (0, 3, 1, 2), )
  test_accuracy(convolution_87, ttnn_permute_187)
  ttnn_from_device_729 = ttnn.from_device(ttnn_permute_187, )
  ttnn_to_layout_761 = ttnn.to_layout(ttnn_from_device_729, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_471 = ttnn.reshape(ttnn_to_layout_761, (1, -1, 91, 20, 20), )
  test_accuracy(view_7, ttnn_reshape_471)
  ttnn_from_device_730 = ttnn.from_device(ttnn_reshape_471, )
  ttnn_to_layout_762 = ttnn.to_layout(ttnn_from_device_730, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_476 = ttnn.to_device(ttnn_to_layout_762, device = device)
  ttnn_permute_6 = ttnn.permute(ttnn_to_device_476, (0, 3, 4, 1, 2), )
  test_accuracy(permute_6, ttnn_permute_6)
  ttnn_from_device_731 = ttnn.from_device(ttnn_permute_6, )
  ttnn_to_layout_763 = ttnn.to_layout(ttnn_from_device_731, ttnn.TILE_LAYOUT, )
  ttnn_to_device_477 = ttnn.to_device(ttnn_to_layout_763, device = device)
  ttnn_prefix_clone_5 = clone_wrapper(ttnn_to_device_477, )
  test_accuracy(clone_6, ttnn_prefix_clone_5)
  ttnn_from_device_732 = ttnn.from_device(ttnn_prefix_clone_5, )
  ttnn_to_layout_764 = ttnn.to_layout(ttnn_from_device_732, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_472 = ttnn.reshape(ttnn_to_layout_764, (1, 2400, 91), )
  test_accuracy(_unsafe_view_5, ttnn_reshape_472)
  ttnn_to_layout_765 = ttnn.to_layout(ttnn_from_device_494, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_473 = ttnn.reshape(ttnn_to_layout_765, (1, 1, 100, 480), )
  ttnn_from_device_734 = ttnn.from_device(ttnn_reshape_473, )
  ttnn_to_layout_766 = ttnn.to_layout(ttnn_from_device_734, ttnn.TILE_LAYOUT, )
  ttnn_to_device_478 = ttnn.to_device(ttnn_to_layout_766, device = device)
  ttnn_from_torch_371 = ttnn.from_torch(arg241_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_88 = conv_wrapper(ttnn_to_device_478, ttnn_from_torch_371, None, 1, 480, 480, [10, 10], [3, 3], [1, 1], [1, 1], [1, 1], 480, device, False, None, )
  ttnn_sharded_to_interleaved_88 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_88, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_735 = ttnn.from_device(ttnn_sharded_to_interleaved_88, )
  ttnn_to_layout_767 = ttnn.to_layout(ttnn_from_device_735, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_474 = ttnn.reshape(ttnn_to_layout_767, (1, 10, 10, 480), )
  ttnn_from_device_736 = ttnn.from_device(ttnn_reshape_474, )
  ttnn_to_layout_768 = ttnn.to_layout(ttnn_from_device_736, ttnn.TILE_LAYOUT, )
  ttnn_to_device_479 = ttnn.to_device(ttnn_to_layout_768, device = device)
  ttnn_permute_189 = ttnn.permute(ttnn_to_device_479, (0, 3, 1, 2), )
  test_accuracy(convolution_88, ttnn_permute_189)
  ttnn_from_torch_372 = ttnn.from_torch(arg468_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_140 = ttnn.add(ttnn_from_torch_372, 0.001, )
  ttnn_rsqrt_65 = ttnn.rsqrt(ttnn_add_140, )
  ttnn_from_device_737 = ttnn.from_device(ttnn_rsqrt_65, )
  ttnn_to_layout_769 = ttnn.to_layout(ttnn_from_device_737, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_475 = ttnn.reshape(ttnn_to_layout_769, (1, 480, 1, 1), )
  ttnn_from_torch_373 = ttnn.from_torch(arg467_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_476 = ttnn.reshape(ttnn_from_torch_373, (1, 480, 1, 1), )
  ttnn_from_device_738 = ttnn.from_device(ttnn_reshape_476, )
  ttnn_to_layout_770 = ttnn.to_layout(ttnn_from_device_738, ttnn.TILE_LAYOUT, )
  ttnn_to_device_480 = ttnn.to_device(ttnn_to_layout_770, device = device)
  ttnn_subtract_65 = ttnn.subtract(ttnn_permute_189, ttnn_to_device_480, )
  ttnn_from_device_739 = ttnn.from_device(ttnn_reshape_475, )
  ttnn_to_layout_771 = ttnn.to_layout(ttnn_from_device_739, ttnn.TILE_LAYOUT, )
  ttnn_to_device_481 = ttnn.to_device(ttnn_to_layout_771, device = device)
  ttnn_multiply_138 = ttnn.multiply(ttnn_subtract_65, ttnn_to_device_481, )
  ttnn_from_torch_374 = ttnn.from_torch(arg242_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_477 = ttnn.reshape(ttnn_from_torch_374, (1, 480, 1, 1), )
  ttnn_from_device_740 = ttnn.from_device(ttnn_reshape_477, )
  ttnn_to_layout_772 = ttnn.to_layout(ttnn_from_device_740, ttnn.TILE_LAYOUT, )
  ttnn_to_device_482 = ttnn.to_device(ttnn_to_layout_772, device = device)
  ttnn_multiply_139 = ttnn.multiply(ttnn_multiply_138, ttnn_to_device_482, )
  ttnn_from_torch_375 = ttnn.from_torch(arg243_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_478 = ttnn.reshape(ttnn_from_torch_375, (1, 480, 1, 1), )
  ttnn_from_device_741 = ttnn.from_device(ttnn_reshape_478, )
  ttnn_to_layout_773 = ttnn.to_layout(ttnn_from_device_741, ttnn.TILE_LAYOUT, )
  ttnn_to_device_483 = ttnn.to_device(ttnn_to_layout_773, device = device)
  ttnn_add_141 = ttnn.add(ttnn_multiply_139, ttnn_to_device_483, )
  ttnn_prefix_pack_to_tuple_65 = pack_to_tuple_wrapper(ttnn_add_141, )
  ttnn_prefix_getitem_65 = ttnn_prefix_pack_to_tuple_65[0]
  test_accuracy(getitem_195, ttnn_prefix_getitem_65)
  ttnn_hardtanh_19 = ttnn.hardtanh(ttnn_prefix_getitem_65, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_19, ttnn_hardtanh_19)
  ttnn_permute_190 = ttnn.permute(ttnn_hardtanh_19, (0, 2, 3, 1), )
  ttnn_from_device_742 = ttnn.from_device(ttnn_permute_190, )
  ttnn_to_layout_774 = ttnn.to_layout(ttnn_from_device_742, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_479 = ttnn.reshape(ttnn_to_layout_774, (1, 1, 100, 480), )
  ttnn_from_torch_376 = ttnn.from_torch(arg245_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_480 = ttnn.reshape(ttnn_from_torch_376, (1, 1, 1, 546), )
  ttnn_from_device_743 = ttnn.from_device(ttnn_reshape_480, )
  ttnn_to_layout_775 = ttnn.to_layout(ttnn_from_device_743, ttnn.TILE_LAYOUT, )
  ttnn_to_device_484 = ttnn.to_device(ttnn_to_layout_775, device = device)
  ttnn_prefix_move_to_host_23 = move_to_host_wrapper(ttnn_to_device_484, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_744 = ttnn.from_device(ttnn_reshape_479, )
  ttnn_to_layout_776 = ttnn.to_layout(ttnn_from_device_744, ttnn.TILE_LAYOUT, )
  ttnn_to_device_485 = ttnn.to_device(ttnn_to_layout_776, device = device)
  ttnn_from_torch_377 = ttnn.from_torch(arg244_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_89 = conv_wrapper(ttnn_to_device_485, ttnn_from_torch_377, ttnn_prefix_move_to_host_23, 1, 480, 546, [10, 10], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_89 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_89, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_745 = ttnn.from_device(ttnn_sharded_to_interleaved_89, )
  ttnn_to_layout_777 = ttnn.to_layout(ttnn_from_device_745, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_481 = ttnn.reshape(ttnn_to_layout_777, (1, 10, 10, 546), )
  ttnn_from_device_746 = ttnn.from_device(ttnn_reshape_481, )
  ttnn_to_layout_778 = ttnn.to_layout(ttnn_from_device_746, ttnn.TILE_LAYOUT, )
  ttnn_to_device_486 = ttnn.to_device(ttnn_to_layout_778, device = device)
  ttnn_permute_191 = ttnn.permute(ttnn_to_device_486, (0, 3, 1, 2), )
  test_accuracy(convolution_89, ttnn_permute_191)
  ttnn_from_device_747 = ttnn.from_device(ttnn_permute_191, )
  ttnn_to_layout_779 = ttnn.to_layout(ttnn_from_device_747, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_482 = ttnn.reshape(ttnn_to_layout_779, (1, -1, 91, 10, 10), )
  test_accuracy(view_8, ttnn_reshape_482)
  ttnn_from_device_748 = ttnn.from_device(ttnn_reshape_482, )
  ttnn_to_layout_780 = ttnn.to_layout(ttnn_from_device_748, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_487 = ttnn.to_device(ttnn_to_layout_780, device = device)
  ttnn_permute_7 = ttnn.permute(ttnn_to_device_487, (0, 3, 4, 1, 2), )
  test_accuracy(permute_7, ttnn_permute_7)
  ttnn_from_device_749 = ttnn.from_device(ttnn_permute_7, )
  ttnn_to_layout_781 = ttnn.to_layout(ttnn_from_device_749, ttnn.TILE_LAYOUT, )
  ttnn_to_device_488 = ttnn.to_device(ttnn_to_layout_781, device = device)
  ttnn_prefix_clone_6 = clone_wrapper(ttnn_to_device_488, )
  test_accuracy(clone_7, ttnn_prefix_clone_6)
  ttnn_from_device_750 = ttnn.from_device(ttnn_prefix_clone_6, )
  ttnn_to_layout_782 = ttnn.to_layout(ttnn_from_device_750, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_483 = ttnn.reshape(ttnn_to_layout_782, (1, 600, 91), )
  test_accuracy(_unsafe_view_6, ttnn_reshape_483)
  ttnn_to_layout_783 = ttnn.to_layout(ttnn_from_device_521, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_484 = ttnn.reshape(ttnn_to_layout_783, (1, 1, 25, 512), )
  ttnn_from_device_752 = ttnn.from_device(ttnn_reshape_484, )
  ttnn_to_layout_784 = ttnn.to_layout(ttnn_from_device_752, ttnn.TILE_LAYOUT, )
  ttnn_to_device_489 = ttnn.to_device(ttnn_to_layout_784, device = device)
  ttnn_from_torch_378 = ttnn.from_torch(arg246_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_90 = conv_wrapper(ttnn_to_device_489, ttnn_from_torch_378, None, 1, 512, 512, [5, 5], [3, 3], [1, 1], [1, 1], [1, 1], 512, device, False, None, )
  ttnn_sharded_to_interleaved_90 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_90, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_753 = ttnn.from_device(ttnn_sharded_to_interleaved_90, )
  ttnn_to_layout_785 = ttnn.to_layout(ttnn_from_device_753, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_485 = ttnn.reshape(ttnn_to_layout_785, (1, 5, 5, 512), )
  ttnn_from_device_754 = ttnn.from_device(ttnn_reshape_485, )
  ttnn_to_layout_786 = ttnn.to_layout(ttnn_from_device_754, ttnn.TILE_LAYOUT, )
  ttnn_to_device_490 = ttnn.to_device(ttnn_to_layout_786, device = device)
  ttnn_permute_193 = ttnn.permute(ttnn_to_device_490, (0, 3, 1, 2), )
  test_accuracy(convolution_90, ttnn_permute_193)
  ttnn_from_torch_379 = ttnn.from_torch(arg471_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_142 = ttnn.add(ttnn_from_torch_379, 0.001, )
  ttnn_rsqrt_66 = ttnn.rsqrt(ttnn_add_142, )
  ttnn_from_device_755 = ttnn.from_device(ttnn_rsqrt_66, )
  ttnn_to_layout_787 = ttnn.to_layout(ttnn_from_device_755, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_486 = ttnn.reshape(ttnn_to_layout_787, (1, 512, 1, 1), )
  ttnn_from_torch_380 = ttnn.from_torch(arg470_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_487 = ttnn.reshape(ttnn_from_torch_380, (1, 512, 1, 1), )
  ttnn_from_device_756 = ttnn.from_device(ttnn_reshape_487, )
  ttnn_to_layout_788 = ttnn.to_layout(ttnn_from_device_756, ttnn.TILE_LAYOUT, )
  ttnn_to_device_491 = ttnn.to_device(ttnn_to_layout_788, device = device)
  ttnn_subtract_66 = ttnn.subtract(ttnn_permute_193, ttnn_to_device_491, )
  ttnn_from_device_757 = ttnn.from_device(ttnn_reshape_486, )
  ttnn_to_layout_789 = ttnn.to_layout(ttnn_from_device_757, ttnn.TILE_LAYOUT, )
  ttnn_to_device_492 = ttnn.to_device(ttnn_to_layout_789, device = device)
  ttnn_multiply_140 = ttnn.multiply(ttnn_subtract_66, ttnn_to_device_492, )
  ttnn_from_torch_381 = ttnn.from_torch(arg247_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_488 = ttnn.reshape(ttnn_from_torch_381, (1, 512, 1, 1), )
  ttnn_from_device_758 = ttnn.from_device(ttnn_reshape_488, )
  ttnn_to_layout_790 = ttnn.to_layout(ttnn_from_device_758, ttnn.TILE_LAYOUT, )
  ttnn_to_device_493 = ttnn.to_device(ttnn_to_layout_790, device = device)
  ttnn_multiply_141 = ttnn.multiply(ttnn_multiply_140, ttnn_to_device_493, )
  ttnn_from_torch_382 = ttnn.from_torch(arg248_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_489 = ttnn.reshape(ttnn_from_torch_382, (1, 512, 1, 1), )
  ttnn_from_device_759 = ttnn.from_device(ttnn_reshape_489, )
  ttnn_to_layout_791 = ttnn.to_layout(ttnn_from_device_759, ttnn.TILE_LAYOUT, )
  ttnn_to_device_494 = ttnn.to_device(ttnn_to_layout_791, device = device)
  ttnn_add_143 = ttnn.add(ttnn_multiply_141, ttnn_to_device_494, )
  ttnn_prefix_pack_to_tuple_66 = pack_to_tuple_wrapper(ttnn_add_143, )
  ttnn_prefix_getitem_66 = ttnn_prefix_pack_to_tuple_66[0]
  test_accuracy(getitem_198, ttnn_prefix_getitem_66)
  ttnn_hardtanh_20 = ttnn.hardtanh(ttnn_prefix_getitem_66, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_20, ttnn_hardtanh_20)
  ttnn_permute_194 = ttnn.permute(ttnn_hardtanh_20, (0, 2, 3, 1), )
  ttnn_from_device_760 = ttnn.from_device(ttnn_permute_194, )
  ttnn_to_layout_792 = ttnn.to_layout(ttnn_from_device_760, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_490 = ttnn.reshape(ttnn_to_layout_792, (1, 1, 25, 512), )
  ttnn_from_torch_383 = ttnn.from_torch(arg250_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_491 = ttnn.reshape(ttnn_from_torch_383, (1, 1, 1, 546), )
  ttnn_from_device_761 = ttnn.from_device(ttnn_reshape_491, )
  ttnn_to_layout_793 = ttnn.to_layout(ttnn_from_device_761, ttnn.TILE_LAYOUT, )
  ttnn_to_device_495 = ttnn.to_device(ttnn_to_layout_793, device = device)
  ttnn_prefix_move_to_host_24 = move_to_host_wrapper(ttnn_to_device_495, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_762 = ttnn.from_device(ttnn_reshape_490, )
  ttnn_to_layout_794 = ttnn.to_layout(ttnn_from_device_762, ttnn.TILE_LAYOUT, )
  ttnn_to_device_496 = ttnn.to_device(ttnn_to_layout_794, device = device)
  ttnn_from_torch_384 = ttnn.from_torch(arg249_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_91 = conv_wrapper(ttnn_to_device_496, ttnn_from_torch_384, ttnn_prefix_move_to_host_24, 1, 512, 546, [5, 5], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_91 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_91, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_763 = ttnn.from_device(ttnn_sharded_to_interleaved_91, )
  ttnn_to_layout_795 = ttnn.to_layout(ttnn_from_device_763, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_492 = ttnn.reshape(ttnn_to_layout_795, (1, 5, 5, 546), )
  ttnn_from_device_764 = ttnn.from_device(ttnn_reshape_492, )
  ttnn_to_layout_796 = ttnn.to_layout(ttnn_from_device_764, ttnn.TILE_LAYOUT, )
  ttnn_to_device_497 = ttnn.to_device(ttnn_to_layout_796, device = device)
  ttnn_permute_195 = ttnn.permute(ttnn_to_device_497, (0, 3, 1, 2), )
  test_accuracy(convolution_91, ttnn_permute_195)
  ttnn_from_device_765 = ttnn.from_device(ttnn_permute_195, )
  ttnn_to_layout_797 = ttnn.to_layout(ttnn_from_device_765, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_493 = ttnn.reshape(ttnn_to_layout_797, (1, -1, 91, 5, 5), )
  test_accuracy(view_9, ttnn_reshape_493)
  ttnn_from_device_766 = ttnn.from_device(ttnn_reshape_493, )
  ttnn_to_layout_798 = ttnn.to_layout(ttnn_from_device_766, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_498 = ttnn.to_device(ttnn_to_layout_798, device = device)
  ttnn_permute_8 = ttnn.permute(ttnn_to_device_498, (0, 3, 4, 1, 2), )
  test_accuracy(permute_8, ttnn_permute_8)
  ttnn_from_device_767 = ttnn.from_device(ttnn_permute_8, )
  ttnn_to_layout_799 = ttnn.to_layout(ttnn_from_device_767, ttnn.TILE_LAYOUT, )
  ttnn_to_device_499 = ttnn.to_device(ttnn_to_layout_799, device = device)
  ttnn_prefix_clone_7 = clone_wrapper(ttnn_to_device_499, )
  test_accuracy(clone_8, ttnn_prefix_clone_7)
  ttnn_from_device_768 = ttnn.from_device(ttnn_prefix_clone_7, )
  ttnn_to_layout_800 = ttnn.to_layout(ttnn_from_device_768, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_494 = ttnn.reshape(ttnn_to_layout_800, (1, 150, 91), )
  test_accuracy(_unsafe_view_7, ttnn_reshape_494)
  ttnn_to_layout_801 = ttnn.to_layout(ttnn_from_device_548, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_495 = ttnn.reshape(ttnn_to_layout_801, (1, 1, 9, 256), )
  ttnn_from_device_770 = ttnn.from_device(ttnn_reshape_495, )
  ttnn_to_layout_802 = ttnn.to_layout(ttnn_from_device_770, ttnn.TILE_LAYOUT, )
  ttnn_to_device_500 = ttnn.to_device(ttnn_to_layout_802, device = device)
  ttnn_from_torch_385 = ttnn.from_torch(arg251_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_92 = conv_wrapper(ttnn_to_device_500, ttnn_from_torch_385, None, 1, 256, 256, [3, 3], [3, 3], [1, 1], [1, 1], [1, 1], 256, device, False, None, )
  ttnn_sharded_to_interleaved_92 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_92, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_771 = ttnn.from_device(ttnn_sharded_to_interleaved_92, )
  ttnn_to_layout_803 = ttnn.to_layout(ttnn_from_device_771, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_496 = ttnn.reshape(ttnn_to_layout_803, (1, 3, 3, 256), )
  ttnn_from_device_772 = ttnn.from_device(ttnn_reshape_496, )
  ttnn_to_layout_804 = ttnn.to_layout(ttnn_from_device_772, ttnn.TILE_LAYOUT, )
  ttnn_to_device_501 = ttnn.to_device(ttnn_to_layout_804, device = device)
  ttnn_permute_197 = ttnn.permute(ttnn_to_device_501, (0, 3, 1, 2), )
  test_accuracy(convolution_92, ttnn_permute_197)
  ttnn_from_torch_386 = ttnn.from_torch(arg474_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_144 = ttnn.add(ttnn_from_torch_386, 0.001, )
  ttnn_rsqrt_67 = ttnn.rsqrt(ttnn_add_144, )
  ttnn_from_device_773 = ttnn.from_device(ttnn_rsqrt_67, )
  ttnn_to_layout_805 = ttnn.to_layout(ttnn_from_device_773, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_497 = ttnn.reshape(ttnn_to_layout_805, (1, 256, 1, 1), )
  ttnn_from_torch_387 = ttnn.from_torch(arg473_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_498 = ttnn.reshape(ttnn_from_torch_387, (1, 256, 1, 1), )
  ttnn_from_device_774 = ttnn.from_device(ttnn_reshape_498, )
  ttnn_to_layout_806 = ttnn.to_layout(ttnn_from_device_774, ttnn.TILE_LAYOUT, )
  ttnn_to_device_502 = ttnn.to_device(ttnn_to_layout_806, device = device)
  ttnn_subtract_67 = ttnn.subtract(ttnn_permute_197, ttnn_to_device_502, )
  ttnn_from_device_775 = ttnn.from_device(ttnn_reshape_497, )
  ttnn_to_layout_807 = ttnn.to_layout(ttnn_from_device_775, ttnn.TILE_LAYOUT, )
  ttnn_to_device_503 = ttnn.to_device(ttnn_to_layout_807, device = device)
  ttnn_multiply_142 = ttnn.multiply(ttnn_subtract_67, ttnn_to_device_503, )
  ttnn_from_torch_388 = ttnn.from_torch(arg252_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_499 = ttnn.reshape(ttnn_from_torch_388, (1, 256, 1, 1), )
  ttnn_from_device_776 = ttnn.from_device(ttnn_reshape_499, )
  ttnn_to_layout_808 = ttnn.to_layout(ttnn_from_device_776, ttnn.TILE_LAYOUT, )
  ttnn_to_device_504 = ttnn.to_device(ttnn_to_layout_808, device = device)
  ttnn_multiply_143 = ttnn.multiply(ttnn_multiply_142, ttnn_to_device_504, )
  ttnn_from_torch_389 = ttnn.from_torch(arg253_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_500 = ttnn.reshape(ttnn_from_torch_389, (1, 256, 1, 1), )
  ttnn_from_device_777 = ttnn.from_device(ttnn_reshape_500, )
  ttnn_to_layout_809 = ttnn.to_layout(ttnn_from_device_777, ttnn.TILE_LAYOUT, )
  ttnn_to_device_505 = ttnn.to_device(ttnn_to_layout_809, device = device)
  ttnn_add_145 = ttnn.add(ttnn_multiply_143, ttnn_to_device_505, )
  ttnn_prefix_pack_to_tuple_67 = pack_to_tuple_wrapper(ttnn_add_145, )
  ttnn_prefix_getitem_67 = ttnn_prefix_pack_to_tuple_67[0]
  test_accuracy(getitem_201, ttnn_prefix_getitem_67)
  ttnn_hardtanh_21 = ttnn.hardtanh(ttnn_prefix_getitem_67, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_21, ttnn_hardtanh_21)
  ttnn_permute_198 = ttnn.permute(ttnn_hardtanh_21, (0, 2, 3, 1), )
  ttnn_from_device_778 = ttnn.from_device(ttnn_permute_198, )
  ttnn_to_layout_810 = ttnn.to_layout(ttnn_from_device_778, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_501 = ttnn.reshape(ttnn_to_layout_810, (1, 1, 9, 256), )
  ttnn_from_torch_390 = ttnn.from_torch(arg255_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_502 = ttnn.reshape(ttnn_from_torch_390, (1, 1, 1, 546), )
  ttnn_from_device_779 = ttnn.from_device(ttnn_reshape_502, )
  ttnn_to_layout_811 = ttnn.to_layout(ttnn_from_device_779, ttnn.TILE_LAYOUT, )
  ttnn_to_device_506 = ttnn.to_device(ttnn_to_layout_811, device = device)
  ttnn_prefix_move_to_host_25 = move_to_host_wrapper(ttnn_to_device_506, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_780 = ttnn.from_device(ttnn_reshape_501, )
  ttnn_to_layout_812 = ttnn.to_layout(ttnn_from_device_780, ttnn.TILE_LAYOUT, )
  ttnn_to_device_507 = ttnn.to_device(ttnn_to_layout_812, device = device)
  ttnn_from_torch_391 = ttnn.from_torch(arg254_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_93 = conv_wrapper(ttnn_to_device_507, ttnn_from_torch_391, ttnn_prefix_move_to_host_25, 1, 256, 546, [3, 3], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_93 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_93, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_781 = ttnn.from_device(ttnn_sharded_to_interleaved_93, )
  ttnn_to_layout_813 = ttnn.to_layout(ttnn_from_device_781, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_503 = ttnn.reshape(ttnn_to_layout_813, (1, 3, 3, 546), )
  ttnn_from_device_782 = ttnn.from_device(ttnn_reshape_503, )
  ttnn_to_layout_814 = ttnn.to_layout(ttnn_from_device_782, ttnn.TILE_LAYOUT, )
  ttnn_to_device_508 = ttnn.to_device(ttnn_to_layout_814, device = device)
  ttnn_permute_199 = ttnn.permute(ttnn_to_device_508, (0, 3, 1, 2), )
  test_accuracy(convolution_93, ttnn_permute_199)
  ttnn_from_device_783 = ttnn.from_device(ttnn_permute_199, )
  ttnn_to_layout_815 = ttnn.to_layout(ttnn_from_device_783, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_504 = ttnn.reshape(ttnn_to_layout_815, (1, -1, 91, 3, 3), )
  test_accuracy(view_10, ttnn_reshape_504)
  ttnn_from_device_784 = ttnn.from_device(ttnn_reshape_504, )
  ttnn_to_layout_816 = ttnn.to_layout(ttnn_from_device_784, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_509 = ttnn.to_device(ttnn_to_layout_816, device = device)
  ttnn_permute_9 = ttnn.permute(ttnn_to_device_509, (0, 3, 4, 1, 2), )
  test_accuracy(permute_9, ttnn_permute_9)
  ttnn_from_device_785 = ttnn.from_device(ttnn_permute_9, )
  ttnn_to_layout_817 = ttnn.to_layout(ttnn_from_device_785, ttnn.TILE_LAYOUT, )
  ttnn_to_device_510 = ttnn.to_device(ttnn_to_layout_817, device = device)
  ttnn_prefix_clone_8 = clone_wrapper(ttnn_to_device_510, )
  test_accuracy(clone_9, ttnn_prefix_clone_8)
  ttnn_from_device_786 = ttnn.from_device(ttnn_prefix_clone_8, )
  ttnn_to_layout_818 = ttnn.to_layout(ttnn_from_device_786, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_505 = ttnn.reshape(ttnn_to_layout_818, (1, 54, 91), )
  test_accuracy(_unsafe_view_8, ttnn_reshape_505)
  ttnn_to_layout_819 = ttnn.to_layout(ttnn_from_device_575, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_506 = ttnn.reshape(ttnn_to_layout_819, (1, 1, 4, 256), )
  ttnn_from_device_788 = ttnn.from_device(ttnn_reshape_506, )
  ttnn_to_layout_820 = ttnn.to_layout(ttnn_from_device_788, ttnn.TILE_LAYOUT, )
  ttnn_to_device_511 = ttnn.to_device(ttnn_to_layout_820, device = device)
  ttnn_from_torch_392 = ttnn.from_torch(arg256_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_94 = conv_wrapper(ttnn_to_device_511, ttnn_from_torch_392, None, 1, 256, 256, [2, 2], [3, 3], [1, 1], [1, 1], [1, 1], 256, device, False, None, )
  ttnn_sharded_to_interleaved_94 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_94, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_789 = ttnn.from_device(ttnn_sharded_to_interleaved_94, )
  ttnn_to_layout_821 = ttnn.to_layout(ttnn_from_device_789, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_507 = ttnn.reshape(ttnn_to_layout_821, (1, 2, 2, 256), )
  ttnn_from_device_790 = ttnn.from_device(ttnn_reshape_507, )
  ttnn_to_layout_822 = ttnn.to_layout(ttnn_from_device_790, ttnn.TILE_LAYOUT, )
  ttnn_to_device_512 = ttnn.to_device(ttnn_to_layout_822, device = device)
  ttnn_permute_201 = ttnn.permute(ttnn_to_device_512, (0, 3, 1, 2), )
  test_accuracy(convolution_94, ttnn_permute_201)
  ttnn_from_torch_393 = ttnn.from_torch(arg477_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_146 = ttnn.add(ttnn_from_torch_393, 0.001, )
  ttnn_rsqrt_68 = ttnn.rsqrt(ttnn_add_146, )
  ttnn_from_device_791 = ttnn.from_device(ttnn_rsqrt_68, )
  ttnn_to_layout_823 = ttnn.to_layout(ttnn_from_device_791, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_508 = ttnn.reshape(ttnn_to_layout_823, (1, 256, 1, 1), )
  ttnn_from_torch_394 = ttnn.from_torch(arg476_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_509 = ttnn.reshape(ttnn_from_torch_394, (1, 256, 1, 1), )
  ttnn_from_device_792 = ttnn.from_device(ttnn_reshape_509, )
  ttnn_to_layout_824 = ttnn.to_layout(ttnn_from_device_792, ttnn.TILE_LAYOUT, )
  ttnn_to_device_513 = ttnn.to_device(ttnn_to_layout_824, device = device)
  ttnn_subtract_68 = ttnn.subtract(ttnn_permute_201, ttnn_to_device_513, )
  ttnn_from_device_793 = ttnn.from_device(ttnn_reshape_508, )
  ttnn_to_layout_825 = ttnn.to_layout(ttnn_from_device_793, ttnn.TILE_LAYOUT, )
  ttnn_to_device_514 = ttnn.to_device(ttnn_to_layout_825, device = device)
  ttnn_multiply_144 = ttnn.multiply(ttnn_subtract_68, ttnn_to_device_514, )
  ttnn_from_torch_395 = ttnn.from_torch(arg257_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_510 = ttnn.reshape(ttnn_from_torch_395, (1, 256, 1, 1), )
  ttnn_from_device_794 = ttnn.from_device(ttnn_reshape_510, )
  ttnn_to_layout_826 = ttnn.to_layout(ttnn_from_device_794, ttnn.TILE_LAYOUT, )
  ttnn_to_device_515 = ttnn.to_device(ttnn_to_layout_826, device = device)
  ttnn_multiply_145 = ttnn.multiply(ttnn_multiply_144, ttnn_to_device_515, )
  ttnn_from_torch_396 = ttnn.from_torch(arg258_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_511 = ttnn.reshape(ttnn_from_torch_396, (1, 256, 1, 1), )
  ttnn_from_device_795 = ttnn.from_device(ttnn_reshape_511, )
  ttnn_to_layout_827 = ttnn.to_layout(ttnn_from_device_795, ttnn.TILE_LAYOUT, )
  ttnn_to_device_516 = ttnn.to_device(ttnn_to_layout_827, device = device)
  ttnn_add_147 = ttnn.add(ttnn_multiply_145, ttnn_to_device_516, )
  ttnn_prefix_pack_to_tuple_68 = pack_to_tuple_wrapper(ttnn_add_147, )
  ttnn_prefix_getitem_68 = ttnn_prefix_pack_to_tuple_68[0]
  test_accuracy(getitem_204, ttnn_prefix_getitem_68)
  ttnn_hardtanh_22 = ttnn.hardtanh(ttnn_prefix_getitem_68, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_22, ttnn_hardtanh_22)
  ttnn_permute_202 = ttnn.permute(ttnn_hardtanh_22, (0, 2, 3, 1), )
  ttnn_from_device_796 = ttnn.from_device(ttnn_permute_202, )
  ttnn_to_layout_828 = ttnn.to_layout(ttnn_from_device_796, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_512 = ttnn.reshape(ttnn_to_layout_828, (1, 1, 4, 256), )
  ttnn_from_torch_397 = ttnn.from_torch(arg260_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_513 = ttnn.reshape(ttnn_from_torch_397, (1, 1, 1, 546), )
  ttnn_from_device_797 = ttnn.from_device(ttnn_reshape_513, )
  ttnn_to_layout_829 = ttnn.to_layout(ttnn_from_device_797, ttnn.TILE_LAYOUT, )
  ttnn_to_device_517 = ttnn.to_device(ttnn_to_layout_829, device = device)
  ttnn_prefix_move_to_host_26 = move_to_host_wrapper(ttnn_to_device_517, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_798 = ttnn.from_device(ttnn_reshape_512, )
  ttnn_to_layout_830 = ttnn.to_layout(ttnn_from_device_798, ttnn.TILE_LAYOUT, )
  ttnn_to_device_518 = ttnn.to_device(ttnn_to_layout_830, device = device)
  ttnn_from_torch_398 = ttnn.from_torch(arg259_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_95 = conv_wrapper(ttnn_to_device_518, ttnn_from_torch_398, ttnn_prefix_move_to_host_26, 1, 256, 546, [2, 2], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_95 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_95, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_799 = ttnn.from_device(ttnn_sharded_to_interleaved_95, )
  ttnn_to_layout_831 = ttnn.to_layout(ttnn_from_device_799, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_514 = ttnn.reshape(ttnn_to_layout_831, (1, 2, 2, 546), )
  ttnn_from_device_800 = ttnn.from_device(ttnn_reshape_514, )
  ttnn_to_layout_832 = ttnn.to_layout(ttnn_from_device_800, ttnn.TILE_LAYOUT, )
  ttnn_to_device_519 = ttnn.to_device(ttnn_to_layout_832, device = device)
  ttnn_permute_203 = ttnn.permute(ttnn_to_device_519, (0, 3, 1, 2), )
  test_accuracy(convolution_95, ttnn_permute_203)
  ttnn_from_device_801 = ttnn.from_device(ttnn_permute_203, )
  ttnn_to_layout_833 = ttnn.to_layout(ttnn_from_device_801, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_515 = ttnn.reshape(ttnn_to_layout_833, (1, -1, 91, 2, 2), )
  test_accuracy(view_11, ttnn_reshape_515)
  ttnn_from_device_802 = ttnn.from_device(ttnn_reshape_515, )
  ttnn_to_layout_834 = ttnn.to_layout(ttnn_from_device_802, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_520 = ttnn.to_device(ttnn_to_layout_834, device = device)
  ttnn_permute_10 = ttnn.permute(ttnn_to_device_520, (0, 3, 4, 1, 2), )
  test_accuracy(permute_10, ttnn_permute_10)
  ttnn_from_device_803 = ttnn.from_device(ttnn_permute_10, )
  ttnn_to_layout_835 = ttnn.to_layout(ttnn_from_device_803, ttnn.TILE_LAYOUT, )
  ttnn_to_device_521 = ttnn.to_device(ttnn_to_layout_835, device = device)
  ttnn_prefix_clone_9 = clone_wrapper(ttnn_to_device_521, )
  test_accuracy(clone_10, ttnn_prefix_clone_9)
  ttnn_from_device_804 = ttnn.from_device(ttnn_prefix_clone_9, )
  ttnn_to_layout_836 = ttnn.to_layout(ttnn_from_device_804, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_516 = ttnn.reshape(ttnn_to_layout_836, (1, 24, 91), )
  test_accuracy(_unsafe_view_9, ttnn_reshape_516)
  ttnn_to_layout_837 = ttnn.to_layout(ttnn_from_device_692, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_517 = ttnn.reshape(ttnn_to_layout_837, (1, 1, 1, 128), )
  ttnn_from_device_806 = ttnn.from_device(ttnn_reshape_517, )
  ttnn_to_layout_838 = ttnn.to_layout(ttnn_from_device_806, ttnn.TILE_LAYOUT, )
  ttnn_to_device_522 = ttnn.to_device(ttnn_to_layout_838, device = device)
  ttnn_from_torch_399 = ttnn.from_torch(arg261_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_96 = conv_wrapper(ttnn_to_device_522, ttnn_from_torch_399, None, 1, 128, 128, [1, 1], [3, 3], [1, 1], [1, 1], [1, 1], 128, device, False, None, )
  ttnn_sharded_to_interleaved_96 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_96, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_807 = ttnn.from_device(ttnn_sharded_to_interleaved_96, )
  ttnn_to_layout_839 = ttnn.to_layout(ttnn_from_device_807, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_518 = ttnn.reshape(ttnn_to_layout_839, (1, 1, 1, 128), )
  ttnn_from_device_808 = ttnn.from_device(ttnn_reshape_518, )
  ttnn_to_layout_840 = ttnn.to_layout(ttnn_from_device_808, ttnn.TILE_LAYOUT, )
  ttnn_to_device_523 = ttnn.to_device(ttnn_to_layout_840, device = device)
  ttnn_permute_205 = ttnn.permute(ttnn_to_device_523, (0, 3, 1, 2), )
  test_accuracy(convolution_96, ttnn_permute_205)
  ttnn_from_torch_400 = ttnn.from_torch(arg480_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_148 = ttnn.add(ttnn_from_torch_400, 0.001, )
  ttnn_rsqrt_69 = ttnn.rsqrt(ttnn_add_148, )
  ttnn_from_device_809 = ttnn.from_device(ttnn_rsqrt_69, )
  ttnn_to_layout_841 = ttnn.to_layout(ttnn_from_device_809, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_519 = ttnn.reshape(ttnn_to_layout_841, (1, 128, 1, 1), )
  ttnn_from_torch_401 = ttnn.from_torch(arg479_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_520 = ttnn.reshape(ttnn_from_torch_401, (1, 128, 1, 1), )
  ttnn_from_device_810 = ttnn.from_device(ttnn_reshape_520, )
  ttnn_to_layout_842 = ttnn.to_layout(ttnn_from_device_810, ttnn.TILE_LAYOUT, )
  ttnn_to_device_524 = ttnn.to_device(ttnn_to_layout_842, device = device)
  ttnn_subtract_69 = ttnn.subtract(ttnn_permute_205, ttnn_to_device_524, )
  ttnn_from_device_811 = ttnn.from_device(ttnn_reshape_519, )
  ttnn_to_layout_843 = ttnn.to_layout(ttnn_from_device_811, ttnn.TILE_LAYOUT, )
  ttnn_to_device_525 = ttnn.to_device(ttnn_to_layout_843, device = device)
  ttnn_multiply_146 = ttnn.multiply(ttnn_subtract_69, ttnn_to_device_525, )
  ttnn_from_torch_402 = ttnn.from_torch(arg262_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_521 = ttnn.reshape(ttnn_from_torch_402, (1, 128, 1, 1), )
  ttnn_from_device_812 = ttnn.from_device(ttnn_reshape_521, )
  ttnn_to_layout_844 = ttnn.to_layout(ttnn_from_device_812, ttnn.TILE_LAYOUT, )
  ttnn_to_device_526 = ttnn.to_device(ttnn_to_layout_844, device = device)
  ttnn_multiply_147 = ttnn.multiply(ttnn_multiply_146, ttnn_to_device_526, )
  ttnn_from_torch_403 = ttnn.from_torch(arg263_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_522 = ttnn.reshape(ttnn_from_torch_403, (1, 128, 1, 1), )
  ttnn_from_device_813 = ttnn.from_device(ttnn_reshape_522, )
  ttnn_to_layout_845 = ttnn.to_layout(ttnn_from_device_813, ttnn.TILE_LAYOUT, )
  ttnn_to_device_527 = ttnn.to_device(ttnn_to_layout_845, device = device)
  ttnn_add_149 = ttnn.add(ttnn_multiply_147, ttnn_to_device_527, )
  ttnn_prefix_pack_to_tuple_69 = pack_to_tuple_wrapper(ttnn_add_149, )
  ttnn_prefix_getitem_69 = ttnn_prefix_pack_to_tuple_69[0]
  test_accuracy(getitem_207, ttnn_prefix_getitem_69)
  ttnn_hardtanh_23 = ttnn.hardtanh(ttnn_prefix_getitem_69, min_val = 0.0, max_val = 6.0)
  test_accuracy(hardtanh_23, ttnn_hardtanh_23)
  ttnn_permute_206 = ttnn.permute(ttnn_hardtanh_23, (0, 2, 3, 1), )
  ttnn_from_device_814 = ttnn.from_device(ttnn_permute_206, )
  ttnn_to_layout_846 = ttnn.to_layout(ttnn_from_device_814, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_523 = ttnn.reshape(ttnn_to_layout_846, (1, 1, 1, 128), )
  ttnn_from_torch_404 = ttnn.from_torch(arg265_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_reshape_524 = ttnn.reshape(ttnn_from_torch_404, (1, 1, 1, 546), )
  ttnn_from_device_815 = ttnn.from_device(ttnn_reshape_524, )
  ttnn_to_layout_847 = ttnn.to_layout(ttnn_from_device_815, ttnn.TILE_LAYOUT, )
  ttnn_to_device_528 = ttnn.to_device(ttnn_to_layout_847, device = device)
  ttnn_prefix_move_to_host_27 = move_to_host_wrapper(ttnn_to_device_528, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_from_device_816 = ttnn.from_device(ttnn_reshape_523, )
  ttnn_to_layout_848 = ttnn.to_layout(ttnn_from_device_816, ttnn.TILE_LAYOUT, )
  ttnn_to_device_529 = ttnn.to_device(ttnn_to_layout_848, device = device)
  ttnn_from_torch_405 = ttnn.from_torch(arg264_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_conv_97 = conv_wrapper(ttnn_to_device_529, ttnn_from_torch_405, ttnn_prefix_move_to_host_27, 1, 128, 546, [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], 1, device, False, None, )
  ttnn_sharded_to_interleaved_97 = ttnn.sharded_to_interleaved(ttnn_prefix_conv_97, ttnn.L1_MEMORY_CONFIG, )
  ttnn_from_device_817 = ttnn.from_device(ttnn_sharded_to_interleaved_97, )
  ttnn_to_layout_849 = ttnn.to_layout(ttnn_from_device_817, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_525 = ttnn.reshape(ttnn_to_layout_849, (1, 1, 1, 546), )
  ttnn_from_device_818 = ttnn.from_device(ttnn_reshape_525, )
  ttnn_to_layout_850 = ttnn.to_layout(ttnn_from_device_818, ttnn.TILE_LAYOUT, )
  ttnn_to_device_530 = ttnn.to_device(ttnn_to_layout_850, device = device)
  ttnn_permute_207 = ttnn.permute(ttnn_to_device_530, (0, 3, 1, 2), )
  test_accuracy(convolution_97, ttnn_permute_207)
  ttnn_from_device_819 = ttnn.from_device(ttnn_permute_207, )
  ttnn_to_layout_851 = ttnn.to_layout(ttnn_from_device_819, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_526 = ttnn.reshape(ttnn_to_layout_851, (1, -1, 91, 1, 1), )
  test_accuracy(view_12, ttnn_reshape_526)
  ttnn_from_device_820 = ttnn.from_device(ttnn_reshape_526, )
  ttnn_to_layout_852 = ttnn.to_layout(ttnn_from_device_820, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_531 = ttnn.to_device(ttnn_to_layout_852, device = device)
  ttnn_permute_11 = ttnn.permute(ttnn_to_device_531, (0, 3, 4, 1, 2), )
  test_accuracy(permute_11, ttnn_permute_11)
  ttnn_from_device_821 = ttnn.from_device(ttnn_permute_11, )
  ttnn_to_layout_853 = ttnn.to_layout(ttnn_from_device_821, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_527 = ttnn.reshape(ttnn_to_layout_853, (1, -1, 91), )
  test_accuracy(view_13, ttnn_reshape_527)
  ttnn_from_device_822 = ttnn.from_device(ttnn_reshape_472, )
  ttnn_to_layout_854 = ttnn.to_layout(ttnn_from_device_822, ttnn.TILE_LAYOUT, )
  ttnn_to_device_532 = ttnn.to_device(ttnn_to_layout_854, device = device)
  ttnn_to_layout_6 = ttnn.to_layout(ttnn_to_device_532, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_823 = ttnn.from_device(ttnn_reshape_483, )
  ttnn_to_layout_855 = ttnn.to_layout(ttnn_from_device_823, ttnn.TILE_LAYOUT, )
  ttnn_to_device_533 = ttnn.to_device(ttnn_to_layout_855, device = device)
  ttnn_to_layout_7 = ttnn.to_layout(ttnn_to_device_533, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_824 = ttnn.from_device(ttnn_reshape_494, )
  ttnn_to_layout_856 = ttnn.to_layout(ttnn_from_device_824, ttnn.TILE_LAYOUT, )
  ttnn_to_device_534 = ttnn.to_device(ttnn_to_layout_856, device = device)
  ttnn_to_layout_8 = ttnn.to_layout(ttnn_to_device_534, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_825 = ttnn.from_device(ttnn_reshape_505, )
  ttnn_to_layout_857 = ttnn.to_layout(ttnn_from_device_825, ttnn.TILE_LAYOUT, )
  ttnn_to_device_535 = ttnn.to_device(ttnn_to_layout_857, device = device)
  ttnn_to_layout_9 = ttnn.to_layout(ttnn_to_device_535, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_826 = ttnn.from_device(ttnn_reshape_516, )
  ttnn_to_layout_858 = ttnn.to_layout(ttnn_from_device_826, ttnn.TILE_LAYOUT, )
  ttnn_to_device_536 = ttnn.to_device(ttnn_to_layout_858, device = device)
  ttnn_to_layout_10 = ttnn.to_layout(ttnn_to_device_536, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_827 = ttnn.from_device(ttnn_reshape_527, )
  ttnn_to_layout_859 = ttnn.to_layout(ttnn_from_device_827, ttnn.TILE_LAYOUT, )
  ttnn_to_device_537 = ttnn.to_device(ttnn_to_layout_859, device = device)
  ttnn_to_layout_11 = ttnn.to_layout(ttnn_to_device_537, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_1 = ttnn.concat([ttnn_to_layout_6, ttnn_to_layout_7, ttnn_to_layout_8, ttnn_to_layout_9, ttnn_to_layout_10, ttnn_to_layout_11], 1, )
  test_accuracy(cat_1, ttnn_concat_1)
  _folded_add_15 = torch.tensor([ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,  7.5000,
         8.5000,  9.5000, 10.5000, 11.5000, 12.5000, 13.5000, 14.5000, 15.5000,
        16.5000, 17.5000, 18.5000, 19.5000])
  ttnn_from_torch_406 = ttnn.from_torch(_folded_add_15, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_148 = ttnn.multiply(ttnn_from_torch_406, 0.05, )
  _folded_add_16 = torch.tensor([ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,  7.5000,
         8.5000,  9.5000, 10.5000, 11.5000, 12.5000, 13.5000, 14.5000, 15.5000,
        16.5000, 17.5000, 18.5000, 19.5000])
  test_accuracy(add_16, _folded_add_16)
  ttnn_from_torch_407 = ttnn.from_torch(_folded_add_16, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_149 = ttnn.multiply(ttnn_from_torch_407, 0.05, )
  test_accuracy(div_2, ttnn_multiply_149)
  ttnn_from_device_828 = ttnn.from_device(ttnn_multiply_149, )
  ttnn_to_layout_860 = ttnn.to_layout(ttnn_from_device_828, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_528 = ttnn.reshape(ttnn_to_layout_860, (-1, 1), )
  test_accuracy(view_14, ttnn_reshape_528)
  ttnn_to_torch = ttnn.to_torch(ttnn_reshape_528, dtype = torch.float32)
  expand_default = aten.expand.default(ttnn_to_torch, [20, 20], )
  test_accuracy(expand, expand_default)
  ttnn_from_device_829 = ttnn.from_device(ttnn_multiply_148, )
  ttnn_to_layout_861 = ttnn.to_layout(ttnn_from_device_829, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_529 = ttnn.reshape(ttnn_to_layout_861, (1, -1), )
  test_accuracy(view_15, ttnn_reshape_529)
  ttnn_from_device_830 = ttnn.from_device(ttnn_reshape_529, )
  ttnn_to_layout_862 = ttnn.to_layout(ttnn_from_device_830, ttnn.TILE_LAYOUT, )
  ttnn_to_device_538 = ttnn.to_device(ttnn_to_layout_862, device = device)
  ttnn_expand = ttnn.expand(ttnn_to_device_538, [20, 20], )
  test_accuracy(expand_1, ttnn_expand)
  ttnn_prefix_clone_10 = clone_wrapper(ttnn_expand, )
  ttnn_from_device_831 = ttnn.from_device(ttnn_prefix_clone_10, )
  ttnn_to_layout_863 = ttnn.to_layout(ttnn_from_device_831, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_530 = ttnn.reshape(ttnn_to_layout_863, (400,), )
  ttnn_from_torch_408 = ttnn.from_torch(expand_default, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_clone_11 = clone_wrapper(ttnn_from_torch_408, )
  test_accuracy(clone_12, ttnn_prefix_clone_11)
  ttnn_from_device_832 = ttnn.from_device(ttnn_prefix_clone_11, )
  ttnn_to_layout_864 = ttnn.to_layout(ttnn_from_device_832, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_531 = ttnn.reshape(ttnn_to_layout_864, (400,), )
  test_accuracy(_unsafe_view_11, ttnn_reshape_531)
  ttnn_from_device_833 = ttnn.from_device(ttnn_reshape_530, )
  ttnn_to_layout_865 = ttnn.to_layout(ttnn_from_device_833, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_539 = ttnn.to_device(ttnn_to_layout_865, device = device)
  ttnn_from_device_834 = ttnn.from_device(ttnn_reshape_531, )
  ttnn_to_layout_866 = ttnn.to_layout(ttnn_from_device_834, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_540 = ttnn.to_device(ttnn_to_layout_866, device = device)
  ttnn_prefix_stack = stack_wrapper((ttnn_to_device_539, ttnn_to_device_540, ttnn_to_device_539, ttnn_to_device_540, ttnn_to_device_539, ttnn_to_device_540, ttnn_to_device_539, ttnn_to_device_540, ttnn_to_device_539, ttnn_to_device_540, ttnn_to_device_539, ttnn_to_device_540), -1, [400, 12], )
  test_accuracy(stack, ttnn_prefix_stack)
  ttnn_from_device_835 = ttnn.from_device(ttnn_prefix_stack, )
  ttnn_to_layout_867 = ttnn.to_layout(ttnn_from_device_835, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_532 = ttnn.reshape(ttnn_to_layout_867, (-1, 2), )
  test_accuracy(view_16, ttnn_reshape_532)
  ttnn_from_torch_409 = ttnn.from_torch(arg266_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_clip = ttnn.clip(ttnn_from_torch_409, 0, 1, )
  ttnn_from_device_836 = ttnn.from_device(ttnn_clip, )
  ttnn_to_layout_868 = ttnn.to_layout(ttnn_from_device_836, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_541 = ttnn.to_device(ttnn_to_layout_868, device = device)
  ttnn_prefix_repeat = repeat_wrapper(ttnn_to_device_541, [400, 1], )
  test_accuracy(repeat, ttnn_prefix_repeat)
  ttnn_from_device_837 = ttnn.from_device(ttnn_reshape_532, )
  ttnn_to_layout_869 = ttnn.to_layout(ttnn_from_device_837, ttnn.TILE_LAYOUT, )
  ttnn_to_device_542 = ttnn.to_device(ttnn_to_layout_869, device = device)
  ttnn_to_layout_12 = ttnn.to_layout(ttnn_to_device_542, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_838 = ttnn.from_device(ttnn_prefix_repeat, )
  ttnn_to_layout_870 = ttnn.to_layout(ttnn_from_device_838, ttnn.TILE_LAYOUT, )
  ttnn_to_device_543 = ttnn.to_device(ttnn_to_layout_870, device = device)
  ttnn_to_layout_13 = ttnn.to_layout(ttnn_to_device_543, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_2 = ttnn.concat([ttnn_to_layout_12, ttnn_to_layout_13], 1, )
  test_accuracy(cat_2, ttnn_concat_2)
  _folded_add_17 = torch.tensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000, 5.5000, 6.5000, 7.5000, 8.5000,
        9.5000])
  ttnn_from_torch_410 = ttnn.from_torch(_folded_add_17, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_150 = ttnn.multiply(ttnn_from_torch_410, 0.1, )
  _folded_add_18 = torch.tensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000, 5.5000, 6.5000, 7.5000, 8.5000,
        9.5000])
  test_accuracy(add_18, _folded_add_18)
  ttnn_from_torch_411 = ttnn.from_torch(_folded_add_18, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_151 = ttnn.multiply(ttnn_from_torch_411, 0.1, )
  test_accuracy(div_4, ttnn_multiply_151)
  ttnn_from_device_839 = ttnn.from_device(ttnn_multiply_151, )
  ttnn_to_layout_871 = ttnn.to_layout(ttnn_from_device_839, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_533 = ttnn.reshape(ttnn_to_layout_871, (-1, 1), )
  test_accuracy(view_17, ttnn_reshape_533)
  ttnn_to_torch_1 = ttnn.to_torch(ttnn_reshape_533, dtype = torch.float32)
  expand_default_2 = aten.expand.default(ttnn_to_torch_1, [10, 10], )
  test_accuracy(expand_2, expand_default_2)
  ttnn_from_device_840 = ttnn.from_device(ttnn_multiply_150, )
  ttnn_to_layout_872 = ttnn.to_layout(ttnn_from_device_840, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_534 = ttnn.reshape(ttnn_to_layout_872, (1, -1), )
  test_accuracy(view_18, ttnn_reshape_534)
  ttnn_from_device_841 = ttnn.from_device(ttnn_reshape_534, )
  ttnn_to_layout_873 = ttnn.to_layout(ttnn_from_device_841, ttnn.TILE_LAYOUT, )
  ttnn_to_device_544 = ttnn.to_device(ttnn_to_layout_873, device = device)
  ttnn_expand_1 = ttnn.expand(ttnn_to_device_544, [10, 10], )
  test_accuracy(expand_3, ttnn_expand_1)
  ttnn_prefix_clone_12 = clone_wrapper(ttnn_expand_1, )
  ttnn_from_device_842 = ttnn.from_device(ttnn_prefix_clone_12, )
  ttnn_to_layout_874 = ttnn.to_layout(ttnn_from_device_842, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_535 = ttnn.reshape(ttnn_to_layout_874, (100,), )
  ttnn_from_torch_412 = ttnn.from_torch(expand_default_2, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_clone_13 = clone_wrapper(ttnn_from_torch_412, )
  test_accuracy(clone_14, ttnn_prefix_clone_13)
  ttnn_from_device_843 = ttnn.from_device(ttnn_prefix_clone_13, )
  ttnn_to_layout_875 = ttnn.to_layout(ttnn_from_device_843, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_536 = ttnn.reshape(ttnn_to_layout_875, (100,), )
  test_accuracy(_unsafe_view_13, ttnn_reshape_536)
  ttnn_from_device_844 = ttnn.from_device(ttnn_reshape_535, )
  ttnn_to_layout_876 = ttnn.to_layout(ttnn_from_device_844, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_545 = ttnn.to_device(ttnn_to_layout_876, device = device)
  ttnn_from_device_845 = ttnn.from_device(ttnn_reshape_536, )
  ttnn_to_layout_877 = ttnn.to_layout(ttnn_from_device_845, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_546 = ttnn.to_device(ttnn_to_layout_877, device = device)
  ttnn_prefix_stack_1 = stack_wrapper((ttnn_to_device_545, ttnn_to_device_546, ttnn_to_device_545, ttnn_to_device_546, ttnn_to_device_545, ttnn_to_device_546, ttnn_to_device_545, ttnn_to_device_546, ttnn_to_device_545, ttnn_to_device_546, ttnn_to_device_545, ttnn_to_device_546), -1, [100, 12], )
  test_accuracy(stack_1, ttnn_prefix_stack_1)
  ttnn_from_device_846 = ttnn.from_device(ttnn_prefix_stack_1, )
  ttnn_to_layout_878 = ttnn.to_layout(ttnn_from_device_846, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_537 = ttnn.reshape(ttnn_to_layout_878, (-1, 2), )
  test_accuracy(view_19, ttnn_reshape_537)
  ttnn_from_torch_413 = ttnn.from_torch(arg267_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_clip_1 = ttnn.clip(ttnn_from_torch_413, 0, 1, )
  ttnn_from_device_847 = ttnn.from_device(ttnn_clip_1, )
  ttnn_to_layout_879 = ttnn.to_layout(ttnn_from_device_847, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_547 = ttnn.to_device(ttnn_to_layout_879, device = device)
  ttnn_prefix_repeat_1 = repeat_wrapper(ttnn_to_device_547, [100, 1], )
  test_accuracy(repeat_1, ttnn_prefix_repeat_1)
  ttnn_from_device_848 = ttnn.from_device(ttnn_reshape_537, )
  ttnn_to_layout_880 = ttnn.to_layout(ttnn_from_device_848, ttnn.TILE_LAYOUT, )
  ttnn_to_device_548 = ttnn.to_device(ttnn_to_layout_880, device = device)
  ttnn_to_layout_14 = ttnn.to_layout(ttnn_to_device_548, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_849 = ttnn.from_device(ttnn_prefix_repeat_1, )
  ttnn_to_layout_881 = ttnn.to_layout(ttnn_from_device_849, ttnn.TILE_LAYOUT, )
  ttnn_to_device_549 = ttnn.to_device(ttnn_to_layout_881, device = device)
  ttnn_to_layout_15 = ttnn.to_layout(ttnn_to_device_549, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_3 = ttnn.concat([ttnn_to_layout_14, ttnn_to_layout_15], 1, )
  test_accuracy(cat_3, ttnn_concat_3)
  _folded_add_19 = torch.tensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000])
  ttnn_from_torch_414 = ttnn.from_torch(_folded_add_19, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_152 = ttnn.multiply(ttnn_from_torch_414, 0.2, )
  _folded_add_20 = torch.tensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000])
  test_accuracy(add_20, _folded_add_20)
  ttnn_from_torch_415 = ttnn.from_torch(_folded_add_20, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_153 = ttnn.multiply(ttnn_from_torch_415, 0.2, )
  test_accuracy(div_6, ttnn_multiply_153)
  ttnn_from_device_850 = ttnn.from_device(ttnn_multiply_153, )
  ttnn_to_layout_882 = ttnn.to_layout(ttnn_from_device_850, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_538 = ttnn.reshape(ttnn_to_layout_882, (-1, 1), )
  test_accuracy(view_20, ttnn_reshape_538)
  ttnn_to_torch_2 = ttnn.to_torch(ttnn_reshape_538, dtype = torch.float32)
  expand_default_4 = aten.expand.default(ttnn_to_torch_2, [5, 5], )
  test_accuracy(expand_4, expand_default_4)
  ttnn_from_device_851 = ttnn.from_device(ttnn_multiply_152, )
  ttnn_to_layout_883 = ttnn.to_layout(ttnn_from_device_851, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_539 = ttnn.reshape(ttnn_to_layout_883, (1, -1), )
  test_accuracy(view_21, ttnn_reshape_539)
  ttnn_from_device_852 = ttnn.from_device(ttnn_reshape_539, )
  ttnn_to_layout_884 = ttnn.to_layout(ttnn_from_device_852, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_550 = ttnn.to_device(ttnn_to_layout_884, device = device)
  ttnn_prefix_repeat_2 = repeat_wrapper(ttnn_to_device_550, [5, 1], )
  test_accuracy(expand_5, ttnn_prefix_repeat_2)
  ttnn_from_device_853 = ttnn.from_device(ttnn_prefix_repeat_2, )
  ttnn_to_layout_885 = ttnn.to_layout(ttnn_from_device_853, ttnn.TILE_LAYOUT, )
  ttnn_to_device_551 = ttnn.to_device(ttnn_to_layout_885, device = device)
  ttnn_prefix_clone_14 = clone_wrapper(ttnn_to_device_551, )
  ttnn_from_device_854 = ttnn.from_device(ttnn_prefix_clone_14, )
  ttnn_to_layout_886 = ttnn.to_layout(ttnn_from_device_854, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_540 = ttnn.reshape(ttnn_to_layout_886, (25,), )
  ttnn_from_torch_416 = ttnn.from_torch(expand_default_4, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_clone_15 = clone_wrapper(ttnn_from_torch_416, )
  test_accuracy(clone_16, ttnn_prefix_clone_15)
  ttnn_from_device_855 = ttnn.from_device(ttnn_prefix_clone_15, )
  ttnn_to_layout_887 = ttnn.to_layout(ttnn_from_device_855, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_541 = ttnn.reshape(ttnn_to_layout_887, (25,), )
  test_accuracy(_unsafe_view_15, ttnn_reshape_541)
  ttnn_from_device_856 = ttnn.from_device(ttnn_reshape_540, )
  ttnn_to_layout_888 = ttnn.to_layout(ttnn_from_device_856, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_552 = ttnn.to_device(ttnn_to_layout_888, device = device)
  ttnn_from_device_857 = ttnn.from_device(ttnn_reshape_541, )
  ttnn_to_layout_889 = ttnn.to_layout(ttnn_from_device_857, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_553 = ttnn.to_device(ttnn_to_layout_889, device = device)
  ttnn_prefix_stack_2 = stack_wrapper((ttnn_to_device_552, ttnn_to_device_553, ttnn_to_device_552, ttnn_to_device_553, ttnn_to_device_552, ttnn_to_device_553, ttnn_to_device_552, ttnn_to_device_553, ttnn_to_device_552, ttnn_to_device_553, ttnn_to_device_552, ttnn_to_device_553), -1, [25, 12], )
  test_accuracy(stack_2, ttnn_prefix_stack_2)
  ttnn_from_device_858 = ttnn.from_device(ttnn_prefix_stack_2, )
  ttnn_to_layout_890 = ttnn.to_layout(ttnn_from_device_858, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_542 = ttnn.reshape(ttnn_to_layout_890, (-1, 2), )
  test_accuracy(view_22, ttnn_reshape_542)
  ttnn_from_torch_417 = ttnn.from_torch(arg268_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_clip_2 = ttnn.clip(ttnn_from_torch_417, 0, 1, )
  ttnn_from_device_859 = ttnn.from_device(ttnn_clip_2, )
  ttnn_to_layout_891 = ttnn.to_layout(ttnn_from_device_859, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_554 = ttnn.to_device(ttnn_to_layout_891, device = device)
  ttnn_prefix_repeat_3 = repeat_wrapper(ttnn_to_device_554, [25, 1], )
  test_accuracy(repeat_2, ttnn_prefix_repeat_3)
  ttnn_from_device_860 = ttnn.from_device(ttnn_reshape_542, )
  ttnn_to_layout_892 = ttnn.to_layout(ttnn_from_device_860, ttnn.TILE_LAYOUT, )
  ttnn_to_device_555 = ttnn.to_device(ttnn_to_layout_892, device = device)
  ttnn_to_layout_16 = ttnn.to_layout(ttnn_to_device_555, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_861 = ttnn.from_device(ttnn_prefix_repeat_3, )
  ttnn_to_layout_893 = ttnn.to_layout(ttnn_from_device_861, ttnn.TILE_LAYOUT, )
  ttnn_to_device_556 = ttnn.to_device(ttnn_to_layout_893, device = device)
  ttnn_to_layout_17 = ttnn.to_layout(ttnn_to_device_556, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_4 = ttnn.concat([ttnn_to_layout_16, ttnn_to_layout_17], 1, )
  test_accuracy(cat_4, ttnn_concat_4)
  _folded_add_21 = torch.tensor([0.5000, 1.5000, 2.5000])
  ttnn_from_torch_418 = ttnn.from_torch(_folded_add_21, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_154 = ttnn.multiply(ttnn_from_torch_418, 0.3333333333333333, )
  _folded_add_22 = torch.tensor([0.5000, 1.5000, 2.5000])
  test_accuracy(add_22, _folded_add_22)
  ttnn_from_torch_419 = ttnn.from_torch(_folded_add_22, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_155 = ttnn.multiply(ttnn_from_torch_419, 0.3333333333333333, )
  test_accuracy(div_8, ttnn_multiply_155)
  ttnn_from_device_862 = ttnn.from_device(ttnn_multiply_155, )
  ttnn_to_layout_894 = ttnn.to_layout(ttnn_from_device_862, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_543 = ttnn.reshape(ttnn_to_layout_894, (-1, 1), )
  test_accuracy(view_23, ttnn_reshape_543)
  ttnn_to_torch_3 = ttnn.to_torch(ttnn_reshape_543, dtype = torch.float32)
  expand_default_6 = aten.expand.default(ttnn_to_torch_3, [3, 3], )
  test_accuracy(expand_6, expand_default_6)
  ttnn_from_device_863 = ttnn.from_device(ttnn_multiply_154, )
  ttnn_to_layout_895 = ttnn.to_layout(ttnn_from_device_863, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_544 = ttnn.reshape(ttnn_to_layout_895, (1, -1), )
  test_accuracy(view_24, ttnn_reshape_544)
  ttnn_from_device_864 = ttnn.from_device(ttnn_reshape_544, )
  ttnn_to_layout_896 = ttnn.to_layout(ttnn_from_device_864, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_557 = ttnn.to_device(ttnn_to_layout_896, device = device)
  ttnn_prefix_repeat_4 = repeat_wrapper(ttnn_to_device_557, [3, 1], )
  test_accuracy(expand_7, ttnn_prefix_repeat_4)
  ttnn_from_device_865 = ttnn.from_device(ttnn_prefix_repeat_4, )
  ttnn_to_layout_897 = ttnn.to_layout(ttnn_from_device_865, ttnn.TILE_LAYOUT, )
  ttnn_to_device_558 = ttnn.to_device(ttnn_to_layout_897, device = device)
  ttnn_prefix_clone_16 = clone_wrapper(ttnn_to_device_558, )
  ttnn_from_device_866 = ttnn.from_device(ttnn_prefix_clone_16, )
  ttnn_to_layout_898 = ttnn.to_layout(ttnn_from_device_866, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_545 = ttnn.reshape(ttnn_to_layout_898, (9,), )
  ttnn_from_torch_420 = ttnn.from_torch(expand_default_6, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_clone_17 = clone_wrapper(ttnn_from_torch_420, )
  test_accuracy(clone_18, ttnn_prefix_clone_17)
  ttnn_from_device_867 = ttnn.from_device(ttnn_prefix_clone_17, )
  ttnn_to_layout_899 = ttnn.to_layout(ttnn_from_device_867, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_546 = ttnn.reshape(ttnn_to_layout_899, (9,), )
  test_accuracy(_unsafe_view_17, ttnn_reshape_546)
  ttnn_from_device_868 = ttnn.from_device(ttnn_reshape_545, )
  ttnn_to_layout_900 = ttnn.to_layout(ttnn_from_device_868, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_559 = ttnn.to_device(ttnn_to_layout_900, device = device)
  ttnn_from_device_869 = ttnn.from_device(ttnn_reshape_546, )
  ttnn_to_layout_901 = ttnn.to_layout(ttnn_from_device_869, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_560 = ttnn.to_device(ttnn_to_layout_901, device = device)
  ttnn_prefix_stack_3 = stack_wrapper((ttnn_to_device_559, ttnn_to_device_560, ttnn_to_device_559, ttnn_to_device_560, ttnn_to_device_559, ttnn_to_device_560, ttnn_to_device_559, ttnn_to_device_560, ttnn_to_device_559, ttnn_to_device_560, ttnn_to_device_559, ttnn_to_device_560), -1, [9, 12], )
  test_accuracy(stack_3, ttnn_prefix_stack_3)
  ttnn_from_device_870 = ttnn.from_device(ttnn_prefix_stack_3, )
  ttnn_to_layout_902 = ttnn.to_layout(ttnn_from_device_870, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_547 = ttnn.reshape(ttnn_to_layout_902, (-1, 2), )
  test_accuracy(view_25, ttnn_reshape_547)
  ttnn_from_torch_421 = ttnn.from_torch(arg269_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_clip_3 = ttnn.clip(ttnn_from_torch_421, 0, 1, )
  ttnn_from_device_871 = ttnn.from_device(ttnn_clip_3, )
  ttnn_to_layout_903 = ttnn.to_layout(ttnn_from_device_871, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_561 = ttnn.to_device(ttnn_to_layout_903, device = device)
  ttnn_prefix_repeat_5 = repeat_wrapper(ttnn_to_device_561, [9, 1], )
  test_accuracy(repeat_3, ttnn_prefix_repeat_5)
  ttnn_from_device_872 = ttnn.from_device(ttnn_reshape_547, )
  ttnn_to_layout_904 = ttnn.to_layout(ttnn_from_device_872, ttnn.TILE_LAYOUT, )
  ttnn_to_device_562 = ttnn.to_device(ttnn_to_layout_904, device = device)
  ttnn_to_layout_18 = ttnn.to_layout(ttnn_to_device_562, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_873 = ttnn.from_device(ttnn_prefix_repeat_5, )
  ttnn_to_layout_905 = ttnn.to_layout(ttnn_from_device_873, ttnn.TILE_LAYOUT, )
  ttnn_to_device_563 = ttnn.to_device(ttnn_to_layout_905, device = device)
  ttnn_to_layout_19 = ttnn.to_layout(ttnn_to_device_563, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_5 = ttnn.concat([ttnn_to_layout_18, ttnn_to_layout_19], 1, )
  test_accuracy(cat_5, ttnn_concat_5)
  _folded_add_23 = torch.tensor([0.5000, 1.5000])
  ttnn_from_torch_422 = ttnn.from_torch(_folded_add_23, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_156 = ttnn.multiply(ttnn_from_torch_422, 0.5, )
  _folded_add_24 = torch.tensor([0.5000, 1.5000])
  test_accuracy(add_24, _folded_add_24)
  ttnn_from_torch_423 = ttnn.from_torch(_folded_add_24, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_157 = ttnn.multiply(ttnn_from_torch_423, 0.5, )
  test_accuracy(div_10, ttnn_multiply_157)
  ttnn_from_device_874 = ttnn.from_device(ttnn_multiply_157, )
  ttnn_to_layout_906 = ttnn.to_layout(ttnn_from_device_874, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_548 = ttnn.reshape(ttnn_to_layout_906, (-1, 1), )
  test_accuracy(view_26, ttnn_reshape_548)
  ttnn_to_torch_4 = ttnn.to_torch(ttnn_reshape_548, dtype = torch.float32)
  expand_default_8 = aten.expand.default(ttnn_to_torch_4, [2, 2], )
  test_accuracy(expand_8, expand_default_8)
  ttnn_from_device_875 = ttnn.from_device(ttnn_multiply_156, )
  ttnn_to_layout_907 = ttnn.to_layout(ttnn_from_device_875, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_549 = ttnn.reshape(ttnn_to_layout_907, (1, -1), )
  test_accuracy(view_27, ttnn_reshape_549)
  ttnn_from_device_876 = ttnn.from_device(ttnn_reshape_549, )
  ttnn_to_layout_908 = ttnn.to_layout(ttnn_from_device_876, ttnn.TILE_LAYOUT, )
  ttnn_to_device_564 = ttnn.to_device(ttnn_to_layout_908, device = device)
  ttnn_expand_2 = ttnn.expand(ttnn_to_device_564, [2, 2], )
  test_accuracy(expand_9, ttnn_expand_2)
  ttnn_prefix_clone_18 = clone_wrapper(ttnn_expand_2, )
  ttnn_from_device_877 = ttnn.from_device(ttnn_prefix_clone_18, )
  ttnn_to_layout_909 = ttnn.to_layout(ttnn_from_device_877, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_550 = ttnn.reshape(ttnn_to_layout_909, (4,), )
  ttnn_from_torch_424 = ttnn.from_torch(expand_default_8, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_prefix_clone_19 = clone_wrapper(ttnn_from_torch_424, )
  test_accuracy(clone_20, ttnn_prefix_clone_19)
  ttnn_from_device_878 = ttnn.from_device(ttnn_prefix_clone_19, )
  ttnn_to_layout_910 = ttnn.to_layout(ttnn_from_device_878, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_551 = ttnn.reshape(ttnn_to_layout_910, (4,), )
  test_accuracy(_unsafe_view_19, ttnn_reshape_551)
  ttnn_from_device_879 = ttnn.from_device(ttnn_reshape_550, )
  ttnn_to_layout_911 = ttnn.to_layout(ttnn_from_device_879, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_565 = ttnn.to_device(ttnn_to_layout_911, device = device)
  ttnn_from_device_880 = ttnn.from_device(ttnn_reshape_551, )
  ttnn_to_layout_912 = ttnn.to_layout(ttnn_from_device_880, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_566 = ttnn.to_device(ttnn_to_layout_912, device = device)
  ttnn_prefix_stack_4 = stack_wrapper((ttnn_to_device_565, ttnn_to_device_566, ttnn_to_device_565, ttnn_to_device_566, ttnn_to_device_565, ttnn_to_device_566, ttnn_to_device_565, ttnn_to_device_566, ttnn_to_device_565, ttnn_to_device_566, ttnn_to_device_565, ttnn_to_device_566), -1, [4, 12], )
  test_accuracy(stack_4, ttnn_prefix_stack_4)
  ttnn_from_device_881 = ttnn.from_device(ttnn_prefix_stack_4, )
  ttnn_to_layout_913 = ttnn.to_layout(ttnn_from_device_881, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_552 = ttnn.reshape(ttnn_to_layout_913, (-1, 2), )
  test_accuracy(view_28, ttnn_reshape_552)
  ttnn_from_torch_425 = ttnn.from_torch(arg270_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_clip_4 = ttnn.clip(ttnn_from_torch_425, 0, 1, )
  ttnn_from_device_882 = ttnn.from_device(ttnn_clip_4, )
  ttnn_to_layout_914 = ttnn.to_layout(ttnn_from_device_882, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_567 = ttnn.to_device(ttnn_to_layout_914, device = device)
  ttnn_prefix_repeat_6 = repeat_wrapper(ttnn_to_device_567, [4, 1], )
  test_accuracy(repeat_4, ttnn_prefix_repeat_6)
  ttnn_from_device_883 = ttnn.from_device(ttnn_reshape_552, )
  ttnn_to_layout_915 = ttnn.to_layout(ttnn_from_device_883, ttnn.TILE_LAYOUT, )
  ttnn_to_device_568 = ttnn.to_device(ttnn_to_layout_915, device = device)
  ttnn_to_layout_20 = ttnn.to_layout(ttnn_to_device_568, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_884 = ttnn.from_device(ttnn_prefix_repeat_6, )
  ttnn_to_layout_916 = ttnn.to_layout(ttnn_from_device_884, ttnn.TILE_LAYOUT, )
  ttnn_to_device_569 = ttnn.to_device(ttnn_to_layout_916, device = device)
  ttnn_to_layout_21 = ttnn.to_layout(ttnn_to_device_569, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_6 = ttnn.concat([ttnn_to_layout_20, ttnn_to_layout_21], 1, )
  test_accuracy(cat_6, ttnn_concat_6)
  _folded_add_25 = torch.tensor([0.5000])
  ttnn_from_torch_426 = ttnn.from_torch(_folded_add_25, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_158 = ttnn.multiply(ttnn_from_torch_426, 1.0, )
  _folded_add_26 = torch.tensor([0.5000])
  test_accuracy(add_26, _folded_add_26)
  ttnn_from_torch_427 = ttnn.from_torch(_folded_add_26, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_159 = ttnn.multiply(ttnn_from_torch_427, 1.0, )
  test_accuracy(div_12, ttnn_multiply_159)
  ttnn_from_device_885 = ttnn.from_device(ttnn_multiply_159, )
  ttnn_to_layout_917 = ttnn.to_layout(ttnn_from_device_885, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_553 = ttnn.reshape(ttnn_to_layout_917, (-1, 1), )
  ttnn_from_device_886 = ttnn.from_device(ttnn_multiply_158, )
  ttnn_to_layout_918 = ttnn.to_layout(ttnn_from_device_886, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_554 = ttnn.reshape(ttnn_to_layout_918, (1, -1), )
  test_accuracy(view_30, ttnn_reshape_554)
  ttnn_from_device_887 = ttnn.from_device(ttnn_reshape_554, )
  ttnn_to_layout_919 = ttnn.to_layout(ttnn_from_device_887, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_555 = ttnn.reshape(ttnn_to_layout_919, (-1,), )
  ttnn_from_device_888 = ttnn.from_device(ttnn_reshape_553, )
  ttnn_to_layout_920 = ttnn.to_layout(ttnn_from_device_888, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_556 = ttnn.reshape(ttnn_to_layout_920, (-1,), )
  test_accuracy(view_32, ttnn_reshape_556)
  ttnn_from_device_889 = ttnn.from_device(ttnn_reshape_555, )
  ttnn_to_layout_921 = ttnn.to_layout(ttnn_from_device_889, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_570 = ttnn.to_device(ttnn_to_layout_921, device = device)
  ttnn_from_device_890 = ttnn.from_device(ttnn_reshape_556, )
  ttnn_to_layout_922 = ttnn.to_layout(ttnn_from_device_890, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_571 = ttnn.to_device(ttnn_to_layout_922, device = device)
  ttnn_prefix_stack_5 = stack_wrapper((ttnn_to_device_570, ttnn_to_device_571, ttnn_to_device_570, ttnn_to_device_571, ttnn_to_device_570, ttnn_to_device_571, ttnn_to_device_570, ttnn_to_device_571, ttnn_to_device_570, ttnn_to_device_571, ttnn_to_device_570, ttnn_to_device_571), -1, [1, 12], )
  test_accuracy(stack_5, ttnn_prefix_stack_5)
  ttnn_from_device_891 = ttnn.from_device(ttnn_prefix_stack_5, )
  ttnn_to_layout_923 = ttnn.to_layout(ttnn_from_device_891, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_557 = ttnn.reshape(ttnn_to_layout_923, (-1, 2), )
  test_accuracy(view_33, ttnn_reshape_557)
  ttnn_from_torch_428 = ttnn.from_torch(arg271_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_clip_5 = ttnn.clip(ttnn_from_torch_428, 0, 1, )
  test_accuracy(clamp_9, ttnn_clip_5)
  ttnn_from_device_892 = ttnn.from_device(ttnn_reshape_557, )
  ttnn_to_layout_924 = ttnn.to_layout(ttnn_from_device_892, ttnn.TILE_LAYOUT, )
  ttnn_to_device_572 = ttnn.to_device(ttnn_to_layout_924, device = device)
  ttnn_to_layout_22 = ttnn.to_layout(ttnn_to_device_572, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout_23 = ttnn.to_layout(ttnn_clip_5, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_7 = ttnn.concat([ttnn_to_layout_22, ttnn_to_layout_23], 1, )
  test_accuracy(cat_7, ttnn_concat_7)
  ttnn_from_device_893 = ttnn.from_device(ttnn_concat_2, )
  ttnn_to_layout_925 = ttnn.to_layout(ttnn_from_device_893, ttnn.TILE_LAYOUT, )
  ttnn_to_device_573 = ttnn.to_device(ttnn_to_layout_925, device = device)
  ttnn_to_layout_24 = ttnn.to_layout(ttnn_to_device_573, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_894 = ttnn.from_device(ttnn_concat_3, )
  ttnn_to_layout_926 = ttnn.to_layout(ttnn_from_device_894, ttnn.TILE_LAYOUT, )
  ttnn_to_device_574 = ttnn.to_device(ttnn_to_layout_926, device = device)
  ttnn_to_layout_25 = ttnn.to_layout(ttnn_to_device_574, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_895 = ttnn.from_device(ttnn_concat_4, )
  ttnn_to_layout_927 = ttnn.to_layout(ttnn_from_device_895, ttnn.TILE_LAYOUT, )
  ttnn_to_device_575 = ttnn.to_device(ttnn_to_layout_927, device = device)
  ttnn_to_layout_26 = ttnn.to_layout(ttnn_to_device_575, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_896 = ttnn.from_device(ttnn_concat_5, )
  ttnn_to_layout_928 = ttnn.to_layout(ttnn_from_device_896, ttnn.TILE_LAYOUT, )
  ttnn_to_device_576 = ttnn.to_device(ttnn_to_layout_928, device = device)
  ttnn_to_layout_27 = ttnn.to_layout(ttnn_to_device_576, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_897 = ttnn.from_device(ttnn_concat_6, )
  ttnn_to_layout_929 = ttnn.to_layout(ttnn_from_device_897, ttnn.TILE_LAYOUT, )
  ttnn_to_device_577 = ttnn.to_device(ttnn_to_layout_929, device = device)
  ttnn_to_layout_28 = ttnn.to_layout(ttnn_to_device_577, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_from_device_898 = ttnn.from_device(ttnn_concat_7, )
  ttnn_to_layout_930 = ttnn.to_layout(ttnn_from_device_898, ttnn.TILE_LAYOUT, )
  ttnn_to_device_578 = ttnn.to_device(ttnn_to_layout_930, device = device)
  ttnn_to_layout_29 = ttnn.to_layout(ttnn_to_device_578, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_8 = ttnn.concat([ttnn_to_layout_24, ttnn_to_layout_25, ttnn_to_layout_26, ttnn_to_layout_27, ttnn_to_layout_28, ttnn_to_layout_29], 0, )
  _folded_lift_fresh_copy_2 = torch.tensor([320, 320])
  test_accuracy(lift_fresh_copy_2, _folded_lift_fresh_copy_2)
  ttnn_from_device_899 = ttnn.from_device(ttnn_concat_8, )
  ttnn_to_layout_931 = ttnn.to_layout(ttnn_from_device_899, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_slice = ttnn.slice(ttnn_to_layout_931, [0, 0], [3234, 2], )
  ttnn_slice_1 = ttnn.slice(ttnn_to_layout_931, [0, 2], [3234, 4], )
  test_accuracy(slice_10, ttnn_slice_1)
  ttnn_from_device_900 = ttnn.from_device(ttnn_slice_1, )
  ttnn_to_layout_932 = ttnn.to_layout(ttnn_from_device_900, ttnn.TILE_LAYOUT, )
  ttnn_to_device_579 = ttnn.to_device(ttnn_to_layout_932, device = device)
  ttnn_multiply_160 = ttnn.multiply(ttnn_to_device_579, 0.5, )
  ttnn_from_device_901 = ttnn.from_device(ttnn_slice, )
  ttnn_to_layout_933 = ttnn.to_layout(ttnn_from_device_901, ttnn.TILE_LAYOUT, )
  ttnn_to_device_580 = ttnn.to_device(ttnn_to_layout_933, device = device)
  ttnn_subtract_70 = ttnn.subtract(ttnn_to_device_580, ttnn_multiply_160, )
  test_accuracy(sub_5, ttnn_subtract_70)
  ttnn_from_torch_429 = ttnn.from_torch(_folded_lift_fresh_copy_2, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_multiply_161 = ttnn.multiply(ttnn_subtract_70, ttnn_from_torch_429, )
  ttnn_to_layout_934 = ttnn.to_layout(ttnn_from_device_900, ttnn.TILE_LAYOUT, )
  ttnn_to_device_581 = ttnn.to_device(ttnn_to_layout_934, device = device)
  ttnn_multiply_162 = ttnn.multiply(ttnn_to_device_581, 0.5, )
  ttnn_to_layout_935 = ttnn.to_layout(ttnn_from_device_901, ttnn.TILE_LAYOUT, )
  ttnn_to_device_582 = ttnn.to_device(ttnn_to_layout_935, device = device)
  ttnn_add_150 = ttnn.add(ttnn_to_device_582, ttnn_multiply_162, )
  test_accuracy(add_27, ttnn_add_150)
  ttnn_multiply_163 = ttnn.multiply(ttnn_add_150, ttnn_from_torch_429, )
  test_accuracy(mul_19, ttnn_multiply_163)
  ttnn_to_layout_30 = ttnn.to_layout(ttnn_multiply_161, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout_31 = ttnn.to_layout(ttnn_multiply_163, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_concat_9 = ttnn.concat([ttnn_to_layout_30, ttnn_to_layout_31], 1, )
  test_accuracy(cat_9, ttnn_concat_9)
  ttnn_to_torch_5 = ttnn.to_torch(ttnn_concat, dtype = torch.float32)
  ttnn_to_torch_6 = ttnn.to_torch(ttnn_concat_1, dtype = torch.float32)
  ttnn_to_torch_7 = ttnn.to_torch(ttnn_concat_9, dtype = torch.float32)
  ttnn_to_torch_8 = ttnn.to_torch(ttnn_full, dtype = torch.float32)
  ttnn.close_device(device)
   # def forward():
  device = ttnn.open_device(device_id=0, l1_small_size=16384)
  # return ()
  ttnn.close_device(device)
   # def forward():
  device = ttnn.open_device(device_id=0, l1_small_size=16384)
  # return ()
  ttnn.close_device(device)
   # def forward():
  device = ttnn.open_device(device_id=0, l1_small_size=16384)
  # return ()
  ttnn.close_device(device)

if __name__ == "__main__":
    filepath = Path(__file__).with_name("MobileNetSSD_inputs.pickle")
    file = lzma.open(filepath, "rb")
    inputs = pickle.load(file)
    forward(*inputs)

