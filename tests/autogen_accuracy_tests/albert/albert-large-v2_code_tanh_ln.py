import lzma
import numpy as np
import pickle
import torch
import ttnn
from pathlib import Path
aten = torch.ops.aten
@torch.fx.wrap
def clone(t):
    return ttnn.clone(t, memory_config=t.memory_config(), dtype=t.dtype)


ref = globals()["clone"]
globals()["clone_wrapper"] = ref
del globals()["clone"]

def comp_pcc(golden, calculated, pcc=0.99):
    golden = torch.Tensor(golden)
    calculated = torch.Tensor(calculated)

    if golden.dtype != calculated.dtype:
        calculated = calculated.type(golden.dtype)

    if torch.all(torch.isnan(golden)) and torch.all(torch.isnan(calculated)):
        # logger.warning("Both tensors are 'nan'")
        return True, 1.0

    if torch.all(torch.isnan(golden)) or torch.all(torch.isnan(calculated)):
        # logger.error("One tensor is all nan, the other is not.")
        return False, 0.0

    # Test if either is completely zero
    if torch.any(golden.bool()) != torch.any(calculated.bool()):
        # logger.error("One tensor is all zero")
        return False, 0.0

    # For now, mask all infs and nans so that we check the rest... TODO
    golden = golden.clone()
    golden[
        torch.logical_or(
            torch.isnan(golden),
            torch.logical_or(torch.isinf(golden), torch.isneginf(golden)),
        )
    ] = 0
    calculated = calculated.clone()
    calculated[
        torch.logical_or(
            torch.isnan(calculated),
            torch.logical_or(torch.isinf(calculated), torch.isneginf(calculated)),
        )
    ] = 0

    if torch.equal(golden, calculated):
        return True, 1.0

    if golden.dtype == torch.bfloat16:
        golden = golden.type(torch.float32)
        calculated = calculated.type(torch.float32)
    cal_pcc = np.min(
        np.ma.corrcoef(
            np.ma.masked_invalid(torch.squeeze(golden).detach().numpy()).flatten(),
            np.ma.masked_invalid(torch.squeeze(calculated).detach().numpy()).flatten(),
        )
    )

    if isinstance(cal_pcc, np.ma.core.MaskedConstant):
        return True, 1.0

    return cal_pcc >= pcc, cal_pcc

def construct_pcc_assert_message(message, expected_pytorch_result, actual_pytorch_result):
    messages = []
    messages.append(message)
    # messages.append("Expected")
    # messages.append(str(expected_pytorch_result))
    # messages.append("Actual")
    # messages.append(str(actual_pytorch_result))
    messages = [str(m) for m in messages]
    return "\n".join(messages)

def assert_with_pcc(expected_pytorch_result, actual_pytorch_result, pcc=0.999):
    assert list(expected_pytorch_result.shape) == list(
        actual_pytorch_result.shape
    ), f"list(expected_pytorch_result.shape)={list(expected_pytorch_result.shape)} vs list(actual_pytorch_result.shape)={list(actual_pytorch_result.shape)}"
    pcc_passed, pcc_message = comp_pcc(expected_pytorch_result, actual_pytorch_result, pcc)
    assert pcc_passed, construct_pcc_assert_message(pcc_message, expected_pytorch_result, actual_pytorch_result)
    return pcc_passed, pcc_message


def test_accuracy(expected, actual):
    if isinstance(actual, ttnn.Tensor):
        actual = ttnn.to_torch(actual)
    return assert_with_pcc(expected, actual, pcc = 0.90)

def forward(arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1):
  device = ttnn.open_device(device_id=0, l1_small_size=16384)
  unsqueeze = aten.unsqueeze.default(arg31_1, 1, )
  unsqueeze_1 = aten.unsqueeze.default(unsqueeze, 2, )
  _to_copy = aten._to_copy.default(unsqueeze_1, dtype = torch.bfloat16)
  rsub = aten.rsub.Scalar(_to_copy, 1.0, )
  mul = aten.mul.Tensor(rsub, -3.3895313892515355e+38, )
  slice_1 = aten.slice.Tensor(arg29_1, 0, 0, 9223372036854775807, )
  slice_2 = aten.slice.Tensor(slice_1, 1, 0, 9, )
  embedding = aten.embedding.default(arg0_1, arg30_1, 0, )
  embedding_1 = aten.embedding.default(arg1_1, arg32_1, )
  add = aten.add.Tensor(embedding, embedding_1, )
  embedding_2 = aten.embedding.default(arg2_1, slice_2, )
  add_1 = aten.add.Tensor(add, embedding_2, )
  native_layer_norm = aten.native_layer_norm.default(add_1, [128], arg3_1, arg4_1, 1e-12, )
  getitem = native_layer_norm[0]
  clone_3 = aten.clone.default(getitem, )
  view = aten.view.default(clone_3, [9, 128], )
  t = aten.t.default(arg5_1, )
  addmm = aten.addmm.default(arg6_1, view, t, )
  view_1 = aten.view.default(addmm, [1, 9, 1024], )
  view_2 = aten.view.default(view_1, [9, 1024], )
  t_1 = aten.t.default(arg7_1, )
  addmm_1 = aten.addmm.default(arg8_1, view_2, t_1, )
  view_3 = aten.view.default(addmm_1, [1, 9, 1024], )
  view_4 = aten.view.default(view_1, [9, 1024], )
  t_2 = aten.t.default(arg9_1, )
  addmm_2 = aten.addmm.default(arg10_1, view_4, t_2, )
  view_5 = aten.view.default(addmm_2, [1, 9, 1024], )
  view_6 = aten.view.default(view_1, [9, 1024], )
  t_3 = aten.t.default(arg11_1, )
  addmm_3 = aten.addmm.default(arg12_1, view_6, t_3, )
  view_7 = aten.view.default(addmm_3, [1, 9, 1024], )
  view_8 = aten.view.default(view_3, [1, 9, 16, 64], )
  permute = aten.permute.default(view_8, [0, 2, 1, 3], )
  view_9 = aten.view.default(view_5, [1, 9, 16, 64], )
  permute_1 = aten.permute.default(view_9, [0, 2, 1, 3], )
  view_10 = aten.view.default(view_7, [1, 9, 16, 64], )
  permute_2 = aten.permute.default(view_10, [0, 2, 1, 3], )
  transpose = aten.transpose.int(permute_1, -1, -2, )
  expand = aten.expand.default(permute, [1, 16, 9, 64], )
  view_11 = aten.view.default(expand, [16, 9, 64], )
  expand_1 = aten.expand.default(transpose, [1, 16, 64, 9], )
  view_12 = aten.view.default(expand_1, [16, 64, 9], )
  bmm = aten.bmm.default(view_11, view_12, )
  view_13 = aten.view.default(bmm, [1, 16, 9, 9], )
  div = aten.div.Tensor(view_13, 8.0, )
  add_2 = aten.add.Tensor(div, mul, )
  _softmax = aten._softmax.default(add_2, -1, False, )
  clone_4 = aten.clone.default(_softmax, )
  expand_2 = aten.expand.default(clone_4, [1, 16, 9, 9], )
  view_14 = aten.view.default(expand_2, [16, 9, 9], )
  expand_3 = aten.expand.default(permute_2, [1, 16, 9, 64], )
  view_15 = aten.view.default(expand_3, [16, 9, 64], )
  bmm_1 = aten.bmm.default(view_14, view_15, )
  view_16 = aten.view.default(bmm_1, [1, 16, 9, 64], )
  transpose_1 = aten.transpose.int(view_16, 2, 1, )
  clone_5 = aten.clone.default(transpose_1, memory_format = torch.contiguous_format)
  _unsafe_view = aten._unsafe_view.default(clone_5, [1, 9, 1024], )
  view_17 = aten.view.default(_unsafe_view, [9, 1024], )
  t_4 = aten.t.default(arg13_1, )
  addmm_4 = aten.addmm.default(arg14_1, view_17, t_4, )
  view_18 = aten.view.default(addmm_4, [1, 9, 1024], )
  clone_6 = aten.clone.default(view_18, )
  add_3 = aten.add.Tensor(view_1, clone_6, )
  native_layer_norm_1 = aten.native_layer_norm.default(add_3, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_3 = native_layer_norm_1[0]
  view_19 = aten.view.default(getitem_3, [9, 1024], )
  t_5 = aten.t.default(arg17_1, )
  addmm_5 = aten.addmm.default(arg18_1, view_19, t_5, )
  view_20 = aten.view.default(addmm_5, [1, 9, 4096], )
  mul_1 = aten.mul.Tensor(view_20, 0.5, )
  pow_1 = aten.pow.Tensor_Scalar(view_20, 3.0, )
  mul_2 = aten.mul.Tensor(pow_1, 0.044715, )
  add_4 = aten.add.Tensor(view_20, mul_2, )
  mul_3 = aten.mul.Tensor(add_4, 0.7978845608028654, )
  tanh = aten.tanh.default(mul_3, )
  add_5 = aten.add.Tensor(tanh, 1.0, )
  mul_4 = aten.mul.Tensor(mul_1, add_5, )
  view_21 = aten.view.default(mul_4, [9, 4096], )
  t_6 = aten.t.default(arg19_1, )
  addmm_6 = aten.addmm.default(arg20_1, view_21, t_6, )
  view_22 = aten.view.default(addmm_6, [1, 9, 1024], )
  add_6 = aten.add.Tensor(view_22, getitem_3, )
  native_layer_norm_2 = aten.native_layer_norm.default(add_6, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_6 = native_layer_norm_2[0]
  view_23 = aten.view.default(getitem_6, [9, 1024], )
  t_7 = aten.t.default(arg7_1, )
  addmm_7 = aten.addmm.default(arg8_1, view_23, t_7, )
  view_24 = aten.view.default(addmm_7, [1, 9, 1024], )
  view_25 = aten.view.default(getitem_6, [9, 1024], )
  t_8 = aten.t.default(arg9_1, )
  addmm_8 = aten.addmm.default(arg10_1, view_25, t_8, )
  view_26 = aten.view.default(addmm_8, [1, 9, 1024], )
  view_27 = aten.view.default(getitem_6, [9, 1024], )
  t_9 = aten.t.default(arg11_1, )
  addmm_9 = aten.addmm.default(arg12_1, view_27, t_9, )
  view_28 = aten.view.default(addmm_9, [1, 9, 1024], )
  view_29 = aten.view.default(view_24, [1, 9, 16, 64], )
  permute_3 = aten.permute.default(view_29, [0, 2, 1, 3], )
  view_30 = aten.view.default(view_26, [1, 9, 16, 64], )
  permute_4 = aten.permute.default(view_30, [0, 2, 1, 3], )
  view_31 = aten.view.default(view_28, [1, 9, 16, 64], )
  permute_5 = aten.permute.default(view_31, [0, 2, 1, 3], )
  transpose_2 = aten.transpose.int(permute_4, -1, -2, )
  expand_4 = aten.expand.default(permute_3, [1, 16, 9, 64], )
  view_32 = aten.view.default(expand_4, [16, 9, 64], )
  expand_5 = aten.expand.default(transpose_2, [1, 16, 64, 9], )
  view_33 = aten.view.default(expand_5, [16, 64, 9], )
  bmm_2 = aten.bmm.default(view_32, view_33, )
  view_34 = aten.view.default(bmm_2, [1, 16, 9, 9], )
  div_1 = aten.div.Tensor(view_34, 8.0, )
  add_7 = aten.add.Tensor(div_1, mul, )
  _softmax_1 = aten._softmax.default(add_7, -1, False, )
  clone_7 = aten.clone.default(_softmax_1, )
  expand_6 = aten.expand.default(clone_7, [1, 16, 9, 9], )
  view_35 = aten.view.default(expand_6, [16, 9, 9], )
  expand_7 = aten.expand.default(permute_5, [1, 16, 9, 64], )
  view_36 = aten.view.default(expand_7, [16, 9, 64], )
  bmm_3 = aten.bmm.default(view_35, view_36, )
  view_37 = aten.view.default(bmm_3, [1, 16, 9, 64], )
  transpose_3 = aten.transpose.int(view_37, 2, 1, )
  clone_8 = aten.clone.default(transpose_3, memory_format = torch.contiguous_format)
  _unsafe_view_1 = aten._unsafe_view.default(clone_8, [1, 9, 1024], )
  view_38 = aten.view.default(_unsafe_view_1, [9, 1024], )
  t_10 = aten.t.default(arg13_1, )
  addmm_10 = aten.addmm.default(arg14_1, view_38, t_10, )
  view_39 = aten.view.default(addmm_10, [1, 9, 1024], )
  clone_9 = aten.clone.default(view_39, )
  add_8 = aten.add.Tensor(getitem_6, clone_9, )
  native_layer_norm_3 = aten.native_layer_norm.default(add_8, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_9 = native_layer_norm_3[0]
  view_40 = aten.view.default(getitem_9, [9, 1024], )
  t_11 = aten.t.default(arg17_1, )
  addmm_11 = aten.addmm.default(arg18_1, view_40, t_11, )
  view_41 = aten.view.default(addmm_11, [1, 9, 4096], )
  mul_5 = aten.mul.Tensor(view_41, 0.5, )
  pow_2 = aten.pow.Tensor_Scalar(view_41, 3.0, )
  mul_6 = aten.mul.Tensor(pow_2, 0.044715, )
  add_9 = aten.add.Tensor(view_41, mul_6, )
  mul_7 = aten.mul.Tensor(add_9, 0.7978845608028654, )
  tanh_1 = aten.tanh.default(mul_7, )
  add_10 = aten.add.Tensor(tanh_1, 1.0, )
  mul_8 = aten.mul.Tensor(mul_5, add_10, )
  view_42 = aten.view.default(mul_8, [9, 4096], )
  t_12 = aten.t.default(arg19_1, )
  addmm_12 = aten.addmm.default(arg20_1, view_42, t_12, )
  view_43 = aten.view.default(addmm_12, [1, 9, 1024], )
  add_11 = aten.add.Tensor(view_43, getitem_9, )
  native_layer_norm_4 = aten.native_layer_norm.default(add_11, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_12 = native_layer_norm_4[0]
  view_44 = aten.view.default(getitem_12, [9, 1024], )
  t_13 = aten.t.default(arg7_1, )
  addmm_13 = aten.addmm.default(arg8_1, view_44, t_13, )
  view_45 = aten.view.default(addmm_13, [1, 9, 1024], )
  view_46 = aten.view.default(getitem_12, [9, 1024], )
  t_14 = aten.t.default(arg9_1, )
  addmm_14 = aten.addmm.default(arg10_1, view_46, t_14, )
  view_47 = aten.view.default(addmm_14, [1, 9, 1024], )
  view_48 = aten.view.default(getitem_12, [9, 1024], )
  t_15 = aten.t.default(arg11_1, )
  addmm_15 = aten.addmm.default(arg12_1, view_48, t_15, )
  view_49 = aten.view.default(addmm_15, [1, 9, 1024], )
  view_50 = aten.view.default(view_45, [1, 9, 16, 64], )
  permute_6 = aten.permute.default(view_50, [0, 2, 1, 3], )
  view_51 = aten.view.default(view_47, [1, 9, 16, 64], )
  permute_7 = aten.permute.default(view_51, [0, 2, 1, 3], )
  view_52 = aten.view.default(view_49, [1, 9, 16, 64], )
  permute_8 = aten.permute.default(view_52, [0, 2, 1, 3], )
  transpose_4 = aten.transpose.int(permute_7, -1, -2, )
  expand_8 = aten.expand.default(permute_6, [1, 16, 9, 64], )
  view_53 = aten.view.default(expand_8, [16, 9, 64], )
  expand_9 = aten.expand.default(transpose_4, [1, 16, 64, 9], )
  view_54 = aten.view.default(expand_9, [16, 64, 9], )
  bmm_4 = aten.bmm.default(view_53, view_54, )
  view_55 = aten.view.default(bmm_4, [1, 16, 9, 9], )
  div_2 = aten.div.Tensor(view_55, 8.0, )
  add_12 = aten.add.Tensor(div_2, mul, )
  _softmax_2 = aten._softmax.default(add_12, -1, False, )
  clone_10 = aten.clone.default(_softmax_2, )
  expand_10 = aten.expand.default(clone_10, [1, 16, 9, 9], )
  view_56 = aten.view.default(expand_10, [16, 9, 9], )
  expand_11 = aten.expand.default(permute_8, [1, 16, 9, 64], )
  view_57 = aten.view.default(expand_11, [16, 9, 64], )
  bmm_5 = aten.bmm.default(view_56, view_57, )
  view_58 = aten.view.default(bmm_5, [1, 16, 9, 64], )
  transpose_5 = aten.transpose.int(view_58, 2, 1, )
  clone_11 = aten.clone.default(transpose_5, memory_format = torch.contiguous_format)
  _unsafe_view_2 = aten._unsafe_view.default(clone_11, [1, 9, 1024], )
  view_59 = aten.view.default(_unsafe_view_2, [9, 1024], )
  t_16 = aten.t.default(arg13_1, )
  addmm_16 = aten.addmm.default(arg14_1, view_59, t_16, )
  view_60 = aten.view.default(addmm_16, [1, 9, 1024], )
  clone_12 = aten.clone.default(view_60, )
  add_13 = aten.add.Tensor(getitem_12, clone_12, )
  native_layer_norm_5 = aten.native_layer_norm.default(add_13, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_15 = native_layer_norm_5[0]
  view_61 = aten.view.default(getitem_15, [9, 1024], )
  t_17 = aten.t.default(arg17_1, )
  addmm_17 = aten.addmm.default(arg18_1, view_61, t_17, )
  view_62 = aten.view.default(addmm_17, [1, 9, 4096], )
  mul_9 = aten.mul.Tensor(view_62, 0.5, )
  pow_3 = aten.pow.Tensor_Scalar(view_62, 3.0, )
  mul_10 = aten.mul.Tensor(pow_3, 0.044715, )
  add_14 = aten.add.Tensor(view_62, mul_10, )
  mul_11 = aten.mul.Tensor(add_14, 0.7978845608028654, )
  tanh_2 = aten.tanh.default(mul_11, )
  add_15 = aten.add.Tensor(tanh_2, 1.0, )
  mul_12 = aten.mul.Tensor(mul_9, add_15, )
  view_63 = aten.view.default(mul_12, [9, 4096], )
  t_18 = aten.t.default(arg19_1, )
  addmm_18 = aten.addmm.default(arg20_1, view_63, t_18, )
  view_64 = aten.view.default(addmm_18, [1, 9, 1024], )
  add_16 = aten.add.Tensor(view_64, getitem_15, )
  native_layer_norm_6 = aten.native_layer_norm.default(add_16, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_18 = native_layer_norm_6[0]
  view_65 = aten.view.default(getitem_18, [9, 1024], )
  t_19 = aten.t.default(arg7_1, )
  addmm_19 = aten.addmm.default(arg8_1, view_65, t_19, )
  view_66 = aten.view.default(addmm_19, [1, 9, 1024], )
  view_67 = aten.view.default(getitem_18, [9, 1024], )
  t_20 = aten.t.default(arg9_1, )
  addmm_20 = aten.addmm.default(arg10_1, view_67, t_20, )
  view_68 = aten.view.default(addmm_20, [1, 9, 1024], )
  view_69 = aten.view.default(getitem_18, [9, 1024], )
  t_21 = aten.t.default(arg11_1, )
  addmm_21 = aten.addmm.default(arg12_1, view_69, t_21, )
  view_70 = aten.view.default(addmm_21, [1, 9, 1024], )
  view_71 = aten.view.default(view_66, [1, 9, 16, 64], )
  permute_9 = aten.permute.default(view_71, [0, 2, 1, 3], )
  view_72 = aten.view.default(view_68, [1, 9, 16, 64], )
  permute_10 = aten.permute.default(view_72, [0, 2, 1, 3], )
  view_73 = aten.view.default(view_70, [1, 9, 16, 64], )
  permute_11 = aten.permute.default(view_73, [0, 2, 1, 3], )
  transpose_6 = aten.transpose.int(permute_10, -1, -2, )
  expand_12 = aten.expand.default(permute_9, [1, 16, 9, 64], )
  view_74 = aten.view.default(expand_12, [16, 9, 64], )
  expand_13 = aten.expand.default(transpose_6, [1, 16, 64, 9], )
  view_75 = aten.view.default(expand_13, [16, 64, 9], )
  bmm_6 = aten.bmm.default(view_74, view_75, )
  view_76 = aten.view.default(bmm_6, [1, 16, 9, 9], )
  div_3 = aten.div.Tensor(view_76, 8.0, )
  add_17 = aten.add.Tensor(div_3, mul, )
  _softmax_3 = aten._softmax.default(add_17, -1, False, )
  clone_13 = aten.clone.default(_softmax_3, )
  expand_14 = aten.expand.default(clone_13, [1, 16, 9, 9], )
  view_77 = aten.view.default(expand_14, [16, 9, 9], )
  expand_15 = aten.expand.default(permute_11, [1, 16, 9, 64], )
  view_78 = aten.view.default(expand_15, [16, 9, 64], )
  bmm_7 = aten.bmm.default(view_77, view_78, )
  view_79 = aten.view.default(bmm_7, [1, 16, 9, 64], )
  transpose_7 = aten.transpose.int(view_79, 2, 1, )
  clone_14 = aten.clone.default(transpose_7, memory_format = torch.contiguous_format)
  _unsafe_view_3 = aten._unsafe_view.default(clone_14, [1, 9, 1024], )
  view_80 = aten.view.default(_unsafe_view_3, [9, 1024], )
  t_22 = aten.t.default(arg13_1, )
  addmm_22 = aten.addmm.default(arg14_1, view_80, t_22, )
  view_81 = aten.view.default(addmm_22, [1, 9, 1024], )
  clone_15 = aten.clone.default(view_81, )
  add_18 = aten.add.Tensor(getitem_18, clone_15, )
  native_layer_norm_7 = aten.native_layer_norm.default(add_18, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_21 = native_layer_norm_7[0]
  view_82 = aten.view.default(getitem_21, [9, 1024], )
  t_23 = aten.t.default(arg17_1, )
  addmm_23 = aten.addmm.default(arg18_1, view_82, t_23, )
  view_83 = aten.view.default(addmm_23, [1, 9, 4096], )
  mul_13 = aten.mul.Tensor(view_83, 0.5, )
  pow_4 = aten.pow.Tensor_Scalar(view_83, 3.0, )
  mul_14 = aten.mul.Tensor(pow_4, 0.044715, )
  add_19 = aten.add.Tensor(view_83, mul_14, )
  mul_15 = aten.mul.Tensor(add_19, 0.7978845608028654, )
  tanh_3 = aten.tanh.default(mul_15, )
  add_20 = aten.add.Tensor(tanh_3, 1.0, )
  mul_16 = aten.mul.Tensor(mul_13, add_20, )
  view_84 = aten.view.default(mul_16, [9, 4096], )
  t_24 = aten.t.default(arg19_1, )
  addmm_24 = aten.addmm.default(arg20_1, view_84, t_24, )
  view_85 = aten.view.default(addmm_24, [1, 9, 1024], )
  add_21 = aten.add.Tensor(view_85, getitem_21, )
  native_layer_norm_8 = aten.native_layer_norm.default(add_21, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_24 = native_layer_norm_8[0]
  view_86 = aten.view.default(getitem_24, [9, 1024], )
  t_25 = aten.t.default(arg7_1, )
  addmm_25 = aten.addmm.default(arg8_1, view_86, t_25, )
  view_87 = aten.view.default(addmm_25, [1, 9, 1024], )
  view_88 = aten.view.default(getitem_24, [9, 1024], )
  t_26 = aten.t.default(arg9_1, )
  addmm_26 = aten.addmm.default(arg10_1, view_88, t_26, )
  view_89 = aten.view.default(addmm_26, [1, 9, 1024], )
  view_90 = aten.view.default(getitem_24, [9, 1024], )
  t_27 = aten.t.default(arg11_1, )
  addmm_27 = aten.addmm.default(arg12_1, view_90, t_27, )
  view_91 = aten.view.default(addmm_27, [1, 9, 1024], )
  view_92 = aten.view.default(view_87, [1, 9, 16, 64], )
  permute_12 = aten.permute.default(view_92, [0, 2, 1, 3], )
  view_93 = aten.view.default(view_89, [1, 9, 16, 64], )
  permute_13 = aten.permute.default(view_93, [0, 2, 1, 3], )
  view_94 = aten.view.default(view_91, [1, 9, 16, 64], )
  permute_14 = aten.permute.default(view_94, [0, 2, 1, 3], )
  transpose_8 = aten.transpose.int(permute_13, -1, -2, )
  expand_16 = aten.expand.default(permute_12, [1, 16, 9, 64], )
  view_95 = aten.view.default(expand_16, [16, 9, 64], )
  expand_17 = aten.expand.default(transpose_8, [1, 16, 64, 9], )
  view_96 = aten.view.default(expand_17, [16, 64, 9], )
  bmm_8 = aten.bmm.default(view_95, view_96, )
  view_97 = aten.view.default(bmm_8, [1, 16, 9, 9], )
  div_4 = aten.div.Tensor(view_97, 8.0, )
  add_22 = aten.add.Tensor(div_4, mul, )
  _softmax_4 = aten._softmax.default(add_22, -1, False, )
  clone_16 = aten.clone.default(_softmax_4, )
  expand_18 = aten.expand.default(clone_16, [1, 16, 9, 9], )
  view_98 = aten.view.default(expand_18, [16, 9, 9], )
  expand_19 = aten.expand.default(permute_14, [1, 16, 9, 64], )
  view_99 = aten.view.default(expand_19, [16, 9, 64], )
  bmm_9 = aten.bmm.default(view_98, view_99, )
  view_100 = aten.view.default(bmm_9, [1, 16, 9, 64], )
  transpose_9 = aten.transpose.int(view_100, 2, 1, )
  clone_17 = aten.clone.default(transpose_9, memory_format = torch.contiguous_format)
  _unsafe_view_4 = aten._unsafe_view.default(clone_17, [1, 9, 1024], )
  view_101 = aten.view.default(_unsafe_view_4, [9, 1024], )
  t_28 = aten.t.default(arg13_1, )
  addmm_28 = aten.addmm.default(arg14_1, view_101, t_28, )
  view_102 = aten.view.default(addmm_28, [1, 9, 1024], )
  clone_18 = aten.clone.default(view_102, )
  add_23 = aten.add.Tensor(getitem_24, clone_18, )
  native_layer_norm_9 = aten.native_layer_norm.default(add_23, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_27 = native_layer_norm_9[0]
  view_103 = aten.view.default(getitem_27, [9, 1024], )
  t_29 = aten.t.default(arg17_1, )
  addmm_29 = aten.addmm.default(arg18_1, view_103, t_29, )
  view_104 = aten.view.default(addmm_29, [1, 9, 4096], )
  mul_17 = aten.mul.Tensor(view_104, 0.5, )
  pow_5 = aten.pow.Tensor_Scalar(view_104, 3.0, )
  mul_18 = aten.mul.Tensor(pow_5, 0.044715, )
  add_24 = aten.add.Tensor(view_104, mul_18, )
  mul_19 = aten.mul.Tensor(add_24, 0.7978845608028654, )
  tanh_4 = aten.tanh.default(mul_19, )
  add_25 = aten.add.Tensor(tanh_4, 1.0, )
  mul_20 = aten.mul.Tensor(mul_17, add_25, )
  view_105 = aten.view.default(mul_20, [9, 4096], )
  t_30 = aten.t.default(arg19_1, )
  addmm_30 = aten.addmm.default(arg20_1, view_105, t_30, )
  view_106 = aten.view.default(addmm_30, [1, 9, 1024], )
  add_26 = aten.add.Tensor(view_106, getitem_27, )
  native_layer_norm_10 = aten.native_layer_norm.default(add_26, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_30 = native_layer_norm_10[0]
  view_107 = aten.view.default(getitem_30, [9, 1024], )
  t_31 = aten.t.default(arg7_1, )
  addmm_31 = aten.addmm.default(arg8_1, view_107, t_31, )
  view_108 = aten.view.default(addmm_31, [1, 9, 1024], )
  view_109 = aten.view.default(getitem_30, [9, 1024], )
  t_32 = aten.t.default(arg9_1, )
  addmm_32 = aten.addmm.default(arg10_1, view_109, t_32, )
  view_110 = aten.view.default(addmm_32, [1, 9, 1024], )
  view_111 = aten.view.default(getitem_30, [9, 1024], )
  t_33 = aten.t.default(arg11_1, )
  addmm_33 = aten.addmm.default(arg12_1, view_111, t_33, )
  view_112 = aten.view.default(addmm_33, [1, 9, 1024], )
  view_113 = aten.view.default(view_108, [1, 9, 16, 64], )
  permute_15 = aten.permute.default(view_113, [0, 2, 1, 3], )
  view_114 = aten.view.default(view_110, [1, 9, 16, 64], )
  permute_16 = aten.permute.default(view_114, [0, 2, 1, 3], )
  view_115 = aten.view.default(view_112, [1, 9, 16, 64], )
  permute_17 = aten.permute.default(view_115, [0, 2, 1, 3], )
  transpose_10 = aten.transpose.int(permute_16, -1, -2, )
  expand_20 = aten.expand.default(permute_15, [1, 16, 9, 64], )
  view_116 = aten.view.default(expand_20, [16, 9, 64], )
  expand_21 = aten.expand.default(transpose_10, [1, 16, 64, 9], )
  view_117 = aten.view.default(expand_21, [16, 64, 9], )
  bmm_10 = aten.bmm.default(view_116, view_117, )
  view_118 = aten.view.default(bmm_10, [1, 16, 9, 9], )
  div_5 = aten.div.Tensor(view_118, 8.0, )
  add_27 = aten.add.Tensor(div_5, mul, )
  _softmax_5 = aten._softmax.default(add_27, -1, False, )
  clone_19 = aten.clone.default(_softmax_5, )
  expand_22 = aten.expand.default(clone_19, [1, 16, 9, 9], )
  view_119 = aten.view.default(expand_22, [16, 9, 9], )
  expand_23 = aten.expand.default(permute_17, [1, 16, 9, 64], )
  view_120 = aten.view.default(expand_23, [16, 9, 64], )
  bmm_11 = aten.bmm.default(view_119, view_120, )
  view_121 = aten.view.default(bmm_11, [1, 16, 9, 64], )
  transpose_11 = aten.transpose.int(view_121, 2, 1, )
  clone_20 = aten.clone.default(transpose_11, memory_format = torch.contiguous_format)
  _unsafe_view_5 = aten._unsafe_view.default(clone_20, [1, 9, 1024], )
  view_122 = aten.view.default(_unsafe_view_5, [9, 1024], )
  t_34 = aten.t.default(arg13_1, )
  addmm_34 = aten.addmm.default(arg14_1, view_122, t_34, )
  view_123 = aten.view.default(addmm_34, [1, 9, 1024], )
  clone_21 = aten.clone.default(view_123, )
  add_28 = aten.add.Tensor(getitem_30, clone_21, )
  native_layer_norm_11 = aten.native_layer_norm.default(add_28, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_33 = native_layer_norm_11[0]
  view_124 = aten.view.default(getitem_33, [9, 1024], )
  t_35 = aten.t.default(arg17_1, )
  addmm_35 = aten.addmm.default(arg18_1, view_124, t_35, )
  view_125 = aten.view.default(addmm_35, [1, 9, 4096], )
  mul_21 = aten.mul.Tensor(view_125, 0.5, )
  pow_6 = aten.pow.Tensor_Scalar(view_125, 3.0, )
  mul_22 = aten.mul.Tensor(pow_6, 0.044715, )
  add_29 = aten.add.Tensor(view_125, mul_22, )
  mul_23 = aten.mul.Tensor(add_29, 0.7978845608028654, )
  tanh_5 = aten.tanh.default(mul_23, )
  add_30 = aten.add.Tensor(tanh_5, 1.0, )
  mul_24 = aten.mul.Tensor(mul_21, add_30, )
  view_126 = aten.view.default(mul_24, [9, 4096], )
  t_36 = aten.t.default(arg19_1, )
  addmm_36 = aten.addmm.default(arg20_1, view_126, t_36, )
  view_127 = aten.view.default(addmm_36, [1, 9, 1024], )
  add_31 = aten.add.Tensor(view_127, getitem_33, )
  native_layer_norm_12 = aten.native_layer_norm.default(add_31, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_36 = native_layer_norm_12[0]
  view_128 = aten.view.default(getitem_36, [9, 1024], )
  t_37 = aten.t.default(arg7_1, )
  addmm_37 = aten.addmm.default(arg8_1, view_128, t_37, )
  view_129 = aten.view.default(addmm_37, [1, 9, 1024], )
  view_130 = aten.view.default(getitem_36, [9, 1024], )
  t_38 = aten.t.default(arg9_1, )
  addmm_38 = aten.addmm.default(arg10_1, view_130, t_38, )
  view_131 = aten.view.default(addmm_38, [1, 9, 1024], )
  view_132 = aten.view.default(getitem_36, [9, 1024], )
  t_39 = aten.t.default(arg11_1, )
  addmm_39 = aten.addmm.default(arg12_1, view_132, t_39, )
  view_133 = aten.view.default(addmm_39, [1, 9, 1024], )
  view_134 = aten.view.default(view_129, [1, 9, 16, 64], )
  permute_18 = aten.permute.default(view_134, [0, 2, 1, 3], )
  view_135 = aten.view.default(view_131, [1, 9, 16, 64], )
  permute_19 = aten.permute.default(view_135, [0, 2, 1, 3], )
  view_136 = aten.view.default(view_133, [1, 9, 16, 64], )
  permute_20 = aten.permute.default(view_136, [0, 2, 1, 3], )
  transpose_12 = aten.transpose.int(permute_19, -1, -2, )
  expand_24 = aten.expand.default(permute_18, [1, 16, 9, 64], )
  view_137 = aten.view.default(expand_24, [16, 9, 64], )
  expand_25 = aten.expand.default(transpose_12, [1, 16, 64, 9], )
  view_138 = aten.view.default(expand_25, [16, 64, 9], )
  bmm_12 = aten.bmm.default(view_137, view_138, )
  view_139 = aten.view.default(bmm_12, [1, 16, 9, 9], )
  div_6 = aten.div.Tensor(view_139, 8.0, )
  add_32 = aten.add.Tensor(div_6, mul, )
  _softmax_6 = aten._softmax.default(add_32, -1, False, )
  clone_22 = aten.clone.default(_softmax_6, )
  expand_26 = aten.expand.default(clone_22, [1, 16, 9, 9], )
  view_140 = aten.view.default(expand_26, [16, 9, 9], )
  expand_27 = aten.expand.default(permute_20, [1, 16, 9, 64], )
  view_141 = aten.view.default(expand_27, [16, 9, 64], )
  bmm_13 = aten.bmm.default(view_140, view_141, )
  view_142 = aten.view.default(bmm_13, [1, 16, 9, 64], )
  transpose_13 = aten.transpose.int(view_142, 2, 1, )
  clone_23 = aten.clone.default(transpose_13, memory_format = torch.contiguous_format)
  _unsafe_view_6 = aten._unsafe_view.default(clone_23, [1, 9, 1024], )
  view_143 = aten.view.default(_unsafe_view_6, [9, 1024], )
  t_40 = aten.t.default(arg13_1, )
  addmm_40 = aten.addmm.default(arg14_1, view_143, t_40, )
  view_144 = aten.view.default(addmm_40, [1, 9, 1024], )
  clone_24 = aten.clone.default(view_144, )
  add_33 = aten.add.Tensor(getitem_36, clone_24, )
  native_layer_norm_13 = aten.native_layer_norm.default(add_33, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_39 = native_layer_norm_13[0]
  view_145 = aten.view.default(getitem_39, [9, 1024], )
  t_41 = aten.t.default(arg17_1, )
  addmm_41 = aten.addmm.default(arg18_1, view_145, t_41, )
  view_146 = aten.view.default(addmm_41, [1, 9, 4096], )
  mul_25 = aten.mul.Tensor(view_146, 0.5, )
  pow_7 = aten.pow.Tensor_Scalar(view_146, 3.0, )
  mul_26 = aten.mul.Tensor(pow_7, 0.044715, )
  add_34 = aten.add.Tensor(view_146, mul_26, )
  mul_27 = aten.mul.Tensor(add_34, 0.7978845608028654, )
  tanh_6 = aten.tanh.default(mul_27, )
  add_35 = aten.add.Tensor(tanh_6, 1.0, )
  mul_28 = aten.mul.Tensor(mul_25, add_35, )
  view_147 = aten.view.default(mul_28, [9, 4096], )
  t_42 = aten.t.default(arg19_1, )
  addmm_42 = aten.addmm.default(arg20_1, view_147, t_42, )
  view_148 = aten.view.default(addmm_42, [1, 9, 1024], )
  add_36 = aten.add.Tensor(view_148, getitem_39, )
  native_layer_norm_14 = aten.native_layer_norm.default(add_36, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_42 = native_layer_norm_14[0]
  view_149 = aten.view.default(getitem_42, [9, 1024], )
  t_43 = aten.t.default(arg7_1, )
  addmm_43 = aten.addmm.default(arg8_1, view_149, t_43, )
  view_150 = aten.view.default(addmm_43, [1, 9, 1024], )
  view_151 = aten.view.default(getitem_42, [9, 1024], )
  t_44 = aten.t.default(arg9_1, )
  addmm_44 = aten.addmm.default(arg10_1, view_151, t_44, )
  view_152 = aten.view.default(addmm_44, [1, 9, 1024], )
  view_153 = aten.view.default(getitem_42, [9, 1024], )
  t_45 = aten.t.default(arg11_1, )
  addmm_45 = aten.addmm.default(arg12_1, view_153, t_45, )
  view_154 = aten.view.default(addmm_45, [1, 9, 1024], )
  view_155 = aten.view.default(view_150, [1, 9, 16, 64], )
  permute_21 = aten.permute.default(view_155, [0, 2, 1, 3], )
  view_156 = aten.view.default(view_152, [1, 9, 16, 64], )
  permute_22 = aten.permute.default(view_156, [0, 2, 1, 3], )
  view_157 = aten.view.default(view_154, [1, 9, 16, 64], )
  permute_23 = aten.permute.default(view_157, [0, 2, 1, 3], )
  transpose_14 = aten.transpose.int(permute_22, -1, -2, )
  expand_28 = aten.expand.default(permute_21, [1, 16, 9, 64], )
  view_158 = aten.view.default(expand_28, [16, 9, 64], )
  expand_29 = aten.expand.default(transpose_14, [1, 16, 64, 9], )
  view_159 = aten.view.default(expand_29, [16, 64, 9], )
  bmm_14 = aten.bmm.default(view_158, view_159, )
  view_160 = aten.view.default(bmm_14, [1, 16, 9, 9], )
  div_7 = aten.div.Tensor(view_160, 8.0, )
  add_37 = aten.add.Tensor(div_7, mul, )
  _softmax_7 = aten._softmax.default(add_37, -1, False, )
  clone_25 = aten.clone.default(_softmax_7, )
  expand_30 = aten.expand.default(clone_25, [1, 16, 9, 9], )
  view_161 = aten.view.default(expand_30, [16, 9, 9], )
  expand_31 = aten.expand.default(permute_23, [1, 16, 9, 64], )
  view_162 = aten.view.default(expand_31, [16, 9, 64], )
  bmm_15 = aten.bmm.default(view_161, view_162, )
  view_163 = aten.view.default(bmm_15, [1, 16, 9, 64], )
  transpose_15 = aten.transpose.int(view_163, 2, 1, )
  clone_26 = aten.clone.default(transpose_15, memory_format = torch.contiguous_format)
  _unsafe_view_7 = aten._unsafe_view.default(clone_26, [1, 9, 1024], )
  view_164 = aten.view.default(_unsafe_view_7, [9, 1024], )
  t_46 = aten.t.default(arg13_1, )
  addmm_46 = aten.addmm.default(arg14_1, view_164, t_46, )
  view_165 = aten.view.default(addmm_46, [1, 9, 1024], )
  clone_27 = aten.clone.default(view_165, )
  add_38 = aten.add.Tensor(getitem_42, clone_27, )
  native_layer_norm_15 = aten.native_layer_norm.default(add_38, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_45 = native_layer_norm_15[0]
  view_166 = aten.view.default(getitem_45, [9, 1024], )
  t_47 = aten.t.default(arg17_1, )
  addmm_47 = aten.addmm.default(arg18_1, view_166, t_47, )
  view_167 = aten.view.default(addmm_47, [1, 9, 4096], )
  mul_29 = aten.mul.Tensor(view_167, 0.5, )
  pow_8 = aten.pow.Tensor_Scalar(view_167, 3.0, )
  mul_30 = aten.mul.Tensor(pow_8, 0.044715, )
  add_39 = aten.add.Tensor(view_167, mul_30, )
  mul_31 = aten.mul.Tensor(add_39, 0.7978845608028654, )
  tanh_7 = aten.tanh.default(mul_31, )
  add_40 = aten.add.Tensor(tanh_7, 1.0, )
  mul_32 = aten.mul.Tensor(mul_29, add_40, )
  view_168 = aten.view.default(mul_32, [9, 4096], )
  t_48 = aten.t.default(arg19_1, )
  addmm_48 = aten.addmm.default(arg20_1, view_168, t_48, )
  view_169 = aten.view.default(addmm_48, [1, 9, 1024], )
  add_41 = aten.add.Tensor(view_169, getitem_45, )
  native_layer_norm_16 = aten.native_layer_norm.default(add_41, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_48 = native_layer_norm_16[0]
  view_170 = aten.view.default(getitem_48, [9, 1024], )
  t_49 = aten.t.default(arg7_1, )
  addmm_49 = aten.addmm.default(arg8_1, view_170, t_49, )
  view_171 = aten.view.default(addmm_49, [1, 9, 1024], )
  view_172 = aten.view.default(getitem_48, [9, 1024], )
  t_50 = aten.t.default(arg9_1, )
  addmm_50 = aten.addmm.default(arg10_1, view_172, t_50, )
  view_173 = aten.view.default(addmm_50, [1, 9, 1024], )
  view_174 = aten.view.default(getitem_48, [9, 1024], )
  t_51 = aten.t.default(arg11_1, )
  addmm_51 = aten.addmm.default(arg12_1, view_174, t_51, )
  view_175 = aten.view.default(addmm_51, [1, 9, 1024], )
  view_176 = aten.view.default(view_171, [1, 9, 16, 64], )
  permute_24 = aten.permute.default(view_176, [0, 2, 1, 3], )
  view_177 = aten.view.default(view_173, [1, 9, 16, 64], )
  permute_25 = aten.permute.default(view_177, [0, 2, 1, 3], )
  view_178 = aten.view.default(view_175, [1, 9, 16, 64], )
  permute_26 = aten.permute.default(view_178, [0, 2, 1, 3], )
  transpose_16 = aten.transpose.int(permute_25, -1, -2, )
  expand_32 = aten.expand.default(permute_24, [1, 16, 9, 64], )
  view_179 = aten.view.default(expand_32, [16, 9, 64], )
  expand_33 = aten.expand.default(transpose_16, [1, 16, 64, 9], )
  view_180 = aten.view.default(expand_33, [16, 64, 9], )
  bmm_16 = aten.bmm.default(view_179, view_180, )
  view_181 = aten.view.default(bmm_16, [1, 16, 9, 9], )
  div_8 = aten.div.Tensor(view_181, 8.0, )
  add_42 = aten.add.Tensor(div_8, mul, )
  _softmax_8 = aten._softmax.default(add_42, -1, False, )
  clone_28 = aten.clone.default(_softmax_8, )
  expand_34 = aten.expand.default(clone_28, [1, 16, 9, 9], )
  view_182 = aten.view.default(expand_34, [16, 9, 9], )
  expand_35 = aten.expand.default(permute_26, [1, 16, 9, 64], )
  view_183 = aten.view.default(expand_35, [16, 9, 64], )
  bmm_17 = aten.bmm.default(view_182, view_183, )
  view_184 = aten.view.default(bmm_17, [1, 16, 9, 64], )
  transpose_17 = aten.transpose.int(view_184, 2, 1, )
  clone_29 = aten.clone.default(transpose_17, memory_format = torch.contiguous_format)
  _unsafe_view_8 = aten._unsafe_view.default(clone_29, [1, 9, 1024], )
  view_185 = aten.view.default(_unsafe_view_8, [9, 1024], )
  t_52 = aten.t.default(arg13_1, )
  addmm_52 = aten.addmm.default(arg14_1, view_185, t_52, )
  view_186 = aten.view.default(addmm_52, [1, 9, 1024], )
  clone_30 = aten.clone.default(view_186, )
  add_43 = aten.add.Tensor(getitem_48, clone_30, )
  native_layer_norm_17 = aten.native_layer_norm.default(add_43, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_51 = native_layer_norm_17[0]
  view_187 = aten.view.default(getitem_51, [9, 1024], )
  t_53 = aten.t.default(arg17_1, )
  addmm_53 = aten.addmm.default(arg18_1, view_187, t_53, )
  view_188 = aten.view.default(addmm_53, [1, 9, 4096], )
  mul_33 = aten.mul.Tensor(view_188, 0.5, )
  pow_9 = aten.pow.Tensor_Scalar(view_188, 3.0, )
  mul_34 = aten.mul.Tensor(pow_9, 0.044715, )
  add_44 = aten.add.Tensor(view_188, mul_34, )
  mul_35 = aten.mul.Tensor(add_44, 0.7978845608028654, )
  tanh_8 = aten.tanh.default(mul_35, )
  add_45 = aten.add.Tensor(tanh_8, 1.0, )
  mul_36 = aten.mul.Tensor(mul_33, add_45, )
  view_189 = aten.view.default(mul_36, [9, 4096], )
  t_54 = aten.t.default(arg19_1, )
  addmm_54 = aten.addmm.default(arg20_1, view_189, t_54, )
  view_190 = aten.view.default(addmm_54, [1, 9, 1024], )
  add_46 = aten.add.Tensor(view_190, getitem_51, )
  native_layer_norm_18 = aten.native_layer_norm.default(add_46, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_54 = native_layer_norm_18[0]
  view_191 = aten.view.default(getitem_54, [9, 1024], )
  t_55 = aten.t.default(arg7_1, )
  addmm_55 = aten.addmm.default(arg8_1, view_191, t_55, )
  view_192 = aten.view.default(addmm_55, [1, 9, 1024], )
  view_193 = aten.view.default(getitem_54, [9, 1024], )
  t_56 = aten.t.default(arg9_1, )
  addmm_56 = aten.addmm.default(arg10_1, view_193, t_56, )
  view_194 = aten.view.default(addmm_56, [1, 9, 1024], )
  view_195 = aten.view.default(getitem_54, [9, 1024], )
  t_57 = aten.t.default(arg11_1, )
  addmm_57 = aten.addmm.default(arg12_1, view_195, t_57, )
  view_196 = aten.view.default(addmm_57, [1, 9, 1024], )
  view_197 = aten.view.default(view_192, [1, 9, 16, 64], )
  permute_27 = aten.permute.default(view_197, [0, 2, 1, 3], )
  view_198 = aten.view.default(view_194, [1, 9, 16, 64], )
  permute_28 = aten.permute.default(view_198, [0, 2, 1, 3], )
  view_199 = aten.view.default(view_196, [1, 9, 16, 64], )
  permute_29 = aten.permute.default(view_199, [0, 2, 1, 3], )
  transpose_18 = aten.transpose.int(permute_28, -1, -2, )
  expand_36 = aten.expand.default(permute_27, [1, 16, 9, 64], )
  view_200 = aten.view.default(expand_36, [16, 9, 64], )
  expand_37 = aten.expand.default(transpose_18, [1, 16, 64, 9], )
  view_201 = aten.view.default(expand_37, [16, 64, 9], )
  bmm_18 = aten.bmm.default(view_200, view_201, )
  view_202 = aten.view.default(bmm_18, [1, 16, 9, 9], )
  div_9 = aten.div.Tensor(view_202, 8.0, )
  add_47 = aten.add.Tensor(div_9, mul, )
  _softmax_9 = aten._softmax.default(add_47, -1, False, )
  clone_31 = aten.clone.default(_softmax_9, )
  expand_38 = aten.expand.default(clone_31, [1, 16, 9, 9], )
  view_203 = aten.view.default(expand_38, [16, 9, 9], )
  expand_39 = aten.expand.default(permute_29, [1, 16, 9, 64], )
  view_204 = aten.view.default(expand_39, [16, 9, 64], )
  bmm_19 = aten.bmm.default(view_203, view_204, )
  view_205 = aten.view.default(bmm_19, [1, 16, 9, 64], )
  transpose_19 = aten.transpose.int(view_205, 2, 1, )
  clone_32 = aten.clone.default(transpose_19, memory_format = torch.contiguous_format)
  _unsafe_view_9 = aten._unsafe_view.default(clone_32, [1, 9, 1024], )
  view_206 = aten.view.default(_unsafe_view_9, [9, 1024], )
  t_58 = aten.t.default(arg13_1, )
  addmm_58 = aten.addmm.default(arg14_1, view_206, t_58, )
  view_207 = aten.view.default(addmm_58, [1, 9, 1024], )
  clone_33 = aten.clone.default(view_207, )
  add_48 = aten.add.Tensor(getitem_54, clone_33, )
  native_layer_norm_19 = aten.native_layer_norm.default(add_48, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_57 = native_layer_norm_19[0]
  view_208 = aten.view.default(getitem_57, [9, 1024], )
  t_59 = aten.t.default(arg17_1, )
  addmm_59 = aten.addmm.default(arg18_1, view_208, t_59, )
  view_209 = aten.view.default(addmm_59, [1, 9, 4096], )
  mul_37 = aten.mul.Tensor(view_209, 0.5, )
  pow_10 = aten.pow.Tensor_Scalar(view_209, 3.0, )
  mul_38 = aten.mul.Tensor(pow_10, 0.044715, )
  add_49 = aten.add.Tensor(view_209, mul_38, )
  mul_39 = aten.mul.Tensor(add_49, 0.7978845608028654, )
  tanh_9 = aten.tanh.default(mul_39, )
  add_50 = aten.add.Tensor(tanh_9, 1.0, )
  mul_40 = aten.mul.Tensor(mul_37, add_50, )
  view_210 = aten.view.default(mul_40, [9, 4096], )
  t_60 = aten.t.default(arg19_1, )
  addmm_60 = aten.addmm.default(arg20_1, view_210, t_60, )
  view_211 = aten.view.default(addmm_60, [1, 9, 1024], )
  add_51 = aten.add.Tensor(view_211, getitem_57, )
  native_layer_norm_20 = aten.native_layer_norm.default(add_51, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_60 = native_layer_norm_20[0]
  view_212 = aten.view.default(getitem_60, [9, 1024], )
  t_61 = aten.t.default(arg7_1, )
  addmm_61 = aten.addmm.default(arg8_1, view_212, t_61, )
  view_213 = aten.view.default(addmm_61, [1, 9, 1024], )
  view_214 = aten.view.default(getitem_60, [9, 1024], )
  t_62 = aten.t.default(arg9_1, )
  addmm_62 = aten.addmm.default(arg10_1, view_214, t_62, )
  view_215 = aten.view.default(addmm_62, [1, 9, 1024], )
  view_216 = aten.view.default(getitem_60, [9, 1024], )
  t_63 = aten.t.default(arg11_1, )
  addmm_63 = aten.addmm.default(arg12_1, view_216, t_63, )
  view_217 = aten.view.default(addmm_63, [1, 9, 1024], )
  view_218 = aten.view.default(view_213, [1, 9, 16, 64], )
  permute_30 = aten.permute.default(view_218, [0, 2, 1, 3], )
  view_219 = aten.view.default(view_215, [1, 9, 16, 64], )
  permute_31 = aten.permute.default(view_219, [0, 2, 1, 3], )
  view_220 = aten.view.default(view_217, [1, 9, 16, 64], )
  permute_32 = aten.permute.default(view_220, [0, 2, 1, 3], )
  transpose_20 = aten.transpose.int(permute_31, -1, -2, )
  expand_40 = aten.expand.default(permute_30, [1, 16, 9, 64], )
  view_221 = aten.view.default(expand_40, [16, 9, 64], )
  expand_41 = aten.expand.default(transpose_20, [1, 16, 64, 9], )
  view_222 = aten.view.default(expand_41, [16, 64, 9], )
  bmm_20 = aten.bmm.default(view_221, view_222, )
  view_223 = aten.view.default(bmm_20, [1, 16, 9, 9], )
  div_10 = aten.div.Tensor(view_223, 8.0, )
  add_52 = aten.add.Tensor(div_10, mul, )
  _softmax_10 = aten._softmax.default(add_52, -1, False, )
  clone_34 = aten.clone.default(_softmax_10, )
  expand_42 = aten.expand.default(clone_34, [1, 16, 9, 9], )
  view_224 = aten.view.default(expand_42, [16, 9, 9], )
  expand_43 = aten.expand.default(permute_32, [1, 16, 9, 64], )
  view_225 = aten.view.default(expand_43, [16, 9, 64], )
  bmm_21 = aten.bmm.default(view_224, view_225, )
  view_226 = aten.view.default(bmm_21, [1, 16, 9, 64], )
  transpose_21 = aten.transpose.int(view_226, 2, 1, )
  clone_35 = aten.clone.default(transpose_21, memory_format = torch.contiguous_format)
  _unsafe_view_10 = aten._unsafe_view.default(clone_35, [1, 9, 1024], )
  view_227 = aten.view.default(_unsafe_view_10, [9, 1024], )
  t_64 = aten.t.default(arg13_1, )
  addmm_64 = aten.addmm.default(arg14_1, view_227, t_64, )
  view_228 = aten.view.default(addmm_64, [1, 9, 1024], )
  clone_36 = aten.clone.default(view_228, )
  add_53 = aten.add.Tensor(getitem_60, clone_36, )
  native_layer_norm_21 = aten.native_layer_norm.default(add_53, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_63 = native_layer_norm_21[0]
  view_229 = aten.view.default(getitem_63, [9, 1024], )
  t_65 = aten.t.default(arg17_1, )
  addmm_65 = aten.addmm.default(arg18_1, view_229, t_65, )
  view_230 = aten.view.default(addmm_65, [1, 9, 4096], )
  mul_41 = aten.mul.Tensor(view_230, 0.5, )
  pow_11 = aten.pow.Tensor_Scalar(view_230, 3.0, )
  mul_42 = aten.mul.Tensor(pow_11, 0.044715, )
  add_54 = aten.add.Tensor(view_230, mul_42, )
  mul_43 = aten.mul.Tensor(add_54, 0.7978845608028654, )
  tanh_10 = aten.tanh.default(mul_43, )
  add_55 = aten.add.Tensor(tanh_10, 1.0, )
  mul_44 = aten.mul.Tensor(mul_41, add_55, )
  view_231 = aten.view.default(mul_44, [9, 4096], )
  t_66 = aten.t.default(arg19_1, )
  addmm_66 = aten.addmm.default(arg20_1, view_231, t_66, )
  view_232 = aten.view.default(addmm_66, [1, 9, 1024], )
  add_56 = aten.add.Tensor(view_232, getitem_63, )
  native_layer_norm_22 = aten.native_layer_norm.default(add_56, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_66 = native_layer_norm_22[0]
  view_233 = aten.view.default(getitem_66, [9, 1024], )
  t_67 = aten.t.default(arg7_1, )
  addmm_67 = aten.addmm.default(arg8_1, view_233, t_67, )
  view_234 = aten.view.default(addmm_67, [1, 9, 1024], )
  view_235 = aten.view.default(getitem_66, [9, 1024], )
  t_68 = aten.t.default(arg9_1, )
  addmm_68 = aten.addmm.default(arg10_1, view_235, t_68, )
  view_236 = aten.view.default(addmm_68, [1, 9, 1024], )
  view_237 = aten.view.default(getitem_66, [9, 1024], )
  t_69 = aten.t.default(arg11_1, )
  addmm_69 = aten.addmm.default(arg12_1, view_237, t_69, )
  view_238 = aten.view.default(addmm_69, [1, 9, 1024], )
  view_239 = aten.view.default(view_234, [1, 9, 16, 64], )
  permute_33 = aten.permute.default(view_239, [0, 2, 1, 3], )
  view_240 = aten.view.default(view_236, [1, 9, 16, 64], )
  permute_34 = aten.permute.default(view_240, [0, 2, 1, 3], )
  view_241 = aten.view.default(view_238, [1, 9, 16, 64], )
  permute_35 = aten.permute.default(view_241, [0, 2, 1, 3], )
  transpose_22 = aten.transpose.int(permute_34, -1, -2, )
  expand_44 = aten.expand.default(permute_33, [1, 16, 9, 64], )
  view_242 = aten.view.default(expand_44, [16, 9, 64], )
  expand_45 = aten.expand.default(transpose_22, [1, 16, 64, 9], )
  view_243 = aten.view.default(expand_45, [16, 64, 9], )
  bmm_22 = aten.bmm.default(view_242, view_243, )
  view_244 = aten.view.default(bmm_22, [1, 16, 9, 9], )
  div_11 = aten.div.Tensor(view_244, 8.0, )
  add_57 = aten.add.Tensor(div_11, mul, )
  _softmax_11 = aten._softmax.default(add_57, -1, False, )
  clone_37 = aten.clone.default(_softmax_11, )
  expand_46 = aten.expand.default(clone_37, [1, 16, 9, 9], )
  view_245 = aten.view.default(expand_46, [16, 9, 9], )
  expand_47 = aten.expand.default(permute_35, [1, 16, 9, 64], )
  view_246 = aten.view.default(expand_47, [16, 9, 64], )
  bmm_23 = aten.bmm.default(view_245, view_246, )
  view_247 = aten.view.default(bmm_23, [1, 16, 9, 64], )
  transpose_23 = aten.transpose.int(view_247, 2, 1, )
  clone_38 = aten.clone.default(transpose_23, memory_format = torch.contiguous_format)
  _unsafe_view_11 = aten._unsafe_view.default(clone_38, [1, 9, 1024], )
  view_248 = aten.view.default(_unsafe_view_11, [9, 1024], )
  t_70 = aten.t.default(arg13_1, )
  addmm_70 = aten.addmm.default(arg14_1, view_248, t_70, )
  view_249 = aten.view.default(addmm_70, [1, 9, 1024], )
  clone_39 = aten.clone.default(view_249, )
  add_58 = aten.add.Tensor(getitem_66, clone_39, )
  native_layer_norm_23 = aten.native_layer_norm.default(add_58, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_69 = native_layer_norm_23[0]
  view_250 = aten.view.default(getitem_69, [9, 1024], )
  t_71 = aten.t.default(arg17_1, )
  addmm_71 = aten.addmm.default(arg18_1, view_250, t_71, )
  view_251 = aten.view.default(addmm_71, [1, 9, 4096], )
  mul_45 = aten.mul.Tensor(view_251, 0.5, )
  pow_12 = aten.pow.Tensor_Scalar(view_251, 3.0, )
  mul_46 = aten.mul.Tensor(pow_12, 0.044715, )
  add_59 = aten.add.Tensor(view_251, mul_46, )
  mul_47 = aten.mul.Tensor(add_59, 0.7978845608028654, )
  tanh_11 = aten.tanh.default(mul_47, )
  add_60 = aten.add.Tensor(tanh_11, 1.0, )
  mul_48 = aten.mul.Tensor(mul_45, add_60, )
  view_252 = aten.view.default(mul_48, [9, 4096], )
  t_72 = aten.t.default(arg19_1, )
  addmm_72 = aten.addmm.default(arg20_1, view_252, t_72, )
  view_253 = aten.view.default(addmm_72, [1, 9, 1024], )
  add_61 = aten.add.Tensor(view_253, getitem_69, )
  native_layer_norm_24 = aten.native_layer_norm.default(add_61, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_72 = native_layer_norm_24[0]
  view_254 = aten.view.default(getitem_72, [9, 1024], )
  t_73 = aten.t.default(arg7_1, )
  addmm_73 = aten.addmm.default(arg8_1, view_254, t_73, )
  view_255 = aten.view.default(addmm_73, [1, 9, 1024], )
  view_256 = aten.view.default(getitem_72, [9, 1024], )
  t_74 = aten.t.default(arg9_1, )
  addmm_74 = aten.addmm.default(arg10_1, view_256, t_74, )
  view_257 = aten.view.default(addmm_74, [1, 9, 1024], )
  view_258 = aten.view.default(getitem_72, [9, 1024], )
  t_75 = aten.t.default(arg11_1, )
  addmm_75 = aten.addmm.default(arg12_1, view_258, t_75, )
  view_259 = aten.view.default(addmm_75, [1, 9, 1024], )
  view_260 = aten.view.default(view_255, [1, 9, 16, 64], )
  permute_36 = aten.permute.default(view_260, [0, 2, 1, 3], )
  view_261 = aten.view.default(view_257, [1, 9, 16, 64], )
  permute_37 = aten.permute.default(view_261, [0, 2, 1, 3], )
  view_262 = aten.view.default(view_259, [1, 9, 16, 64], )
  permute_38 = aten.permute.default(view_262, [0, 2, 1, 3], )
  transpose_24 = aten.transpose.int(permute_37, -1, -2, )
  expand_48 = aten.expand.default(permute_36, [1, 16, 9, 64], )
  view_263 = aten.view.default(expand_48, [16, 9, 64], )
  expand_49 = aten.expand.default(transpose_24, [1, 16, 64, 9], )
  view_264 = aten.view.default(expand_49, [16, 64, 9], )
  bmm_24 = aten.bmm.default(view_263, view_264, )
  view_265 = aten.view.default(bmm_24, [1, 16, 9, 9], )
  div_12 = aten.div.Tensor(view_265, 8.0, )
  add_62 = aten.add.Tensor(div_12, mul, )
  _softmax_12 = aten._softmax.default(add_62, -1, False, )
  clone_40 = aten.clone.default(_softmax_12, )
  expand_50 = aten.expand.default(clone_40, [1, 16, 9, 9], )
  view_266 = aten.view.default(expand_50, [16, 9, 9], )
  expand_51 = aten.expand.default(permute_38, [1, 16, 9, 64], )
  view_267 = aten.view.default(expand_51, [16, 9, 64], )
  bmm_25 = aten.bmm.default(view_266, view_267, )
  view_268 = aten.view.default(bmm_25, [1, 16, 9, 64], )
  transpose_25 = aten.transpose.int(view_268, 2, 1, )
  clone_41 = aten.clone.default(transpose_25, memory_format = torch.contiguous_format)
  _unsafe_view_12 = aten._unsafe_view.default(clone_41, [1, 9, 1024], )
  view_269 = aten.view.default(_unsafe_view_12, [9, 1024], )
  t_76 = aten.t.default(arg13_1, )
  addmm_76 = aten.addmm.default(arg14_1, view_269, t_76, )
  view_270 = aten.view.default(addmm_76, [1, 9, 1024], )
  clone_42 = aten.clone.default(view_270, )
  add_63 = aten.add.Tensor(getitem_72, clone_42, )
  native_layer_norm_25 = aten.native_layer_norm.default(add_63, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_75 = native_layer_norm_25[0]
  view_271 = aten.view.default(getitem_75, [9, 1024], )
  t_77 = aten.t.default(arg17_1, )
  addmm_77 = aten.addmm.default(arg18_1, view_271, t_77, )
  view_272 = aten.view.default(addmm_77, [1, 9, 4096], )
  mul_49 = aten.mul.Tensor(view_272, 0.5, )
  pow_13 = aten.pow.Tensor_Scalar(view_272, 3.0, )
  mul_50 = aten.mul.Tensor(pow_13, 0.044715, )
  add_64 = aten.add.Tensor(view_272, mul_50, )
  mul_51 = aten.mul.Tensor(add_64, 0.7978845608028654, )
  tanh_12 = aten.tanh.default(mul_51, )
  add_65 = aten.add.Tensor(tanh_12, 1.0, )
  mul_52 = aten.mul.Tensor(mul_49, add_65, )
  view_273 = aten.view.default(mul_52, [9, 4096], )
  t_78 = aten.t.default(arg19_1, )
  addmm_78 = aten.addmm.default(arg20_1, view_273, t_78, )
  view_274 = aten.view.default(addmm_78, [1, 9, 1024], )
  add_66 = aten.add.Tensor(view_274, getitem_75, )
  native_layer_norm_26 = aten.native_layer_norm.default(add_66, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_78 = native_layer_norm_26[0]
  view_275 = aten.view.default(getitem_78, [9, 1024], )
  t_79 = aten.t.default(arg7_1, )
  addmm_79 = aten.addmm.default(arg8_1, view_275, t_79, )
  view_276 = aten.view.default(addmm_79, [1, 9, 1024], )
  view_277 = aten.view.default(getitem_78, [9, 1024], )
  t_80 = aten.t.default(arg9_1, )
  addmm_80 = aten.addmm.default(arg10_1, view_277, t_80, )
  view_278 = aten.view.default(addmm_80, [1, 9, 1024], )
  view_279 = aten.view.default(getitem_78, [9, 1024], )
  t_81 = aten.t.default(arg11_1, )
  addmm_81 = aten.addmm.default(arg12_1, view_279, t_81, )
  view_280 = aten.view.default(addmm_81, [1, 9, 1024], )
  view_281 = aten.view.default(view_276, [1, 9, 16, 64], )
  permute_39 = aten.permute.default(view_281, [0, 2, 1, 3], )
  view_282 = aten.view.default(view_278, [1, 9, 16, 64], )
  permute_40 = aten.permute.default(view_282, [0, 2, 1, 3], )
  view_283 = aten.view.default(view_280, [1, 9, 16, 64], )
  permute_41 = aten.permute.default(view_283, [0, 2, 1, 3], )
  transpose_26 = aten.transpose.int(permute_40, -1, -2, )
  expand_52 = aten.expand.default(permute_39, [1, 16, 9, 64], )
  view_284 = aten.view.default(expand_52, [16, 9, 64], )
  expand_53 = aten.expand.default(transpose_26, [1, 16, 64, 9], )
  view_285 = aten.view.default(expand_53, [16, 64, 9], )
  bmm_26 = aten.bmm.default(view_284, view_285, )
  view_286 = aten.view.default(bmm_26, [1, 16, 9, 9], )
  div_13 = aten.div.Tensor(view_286, 8.0, )
  add_67 = aten.add.Tensor(div_13, mul, )
  _softmax_13 = aten._softmax.default(add_67, -1, False, )
  clone_43 = aten.clone.default(_softmax_13, )
  expand_54 = aten.expand.default(clone_43, [1, 16, 9, 9], )
  view_287 = aten.view.default(expand_54, [16, 9, 9], )
  expand_55 = aten.expand.default(permute_41, [1, 16, 9, 64], )
  view_288 = aten.view.default(expand_55, [16, 9, 64], )
  bmm_27 = aten.bmm.default(view_287, view_288, )
  view_289 = aten.view.default(bmm_27, [1, 16, 9, 64], )
  transpose_27 = aten.transpose.int(view_289, 2, 1, )
  clone_44 = aten.clone.default(transpose_27, memory_format = torch.contiguous_format)
  _unsafe_view_13 = aten._unsafe_view.default(clone_44, [1, 9, 1024], )
  view_290 = aten.view.default(_unsafe_view_13, [9, 1024], )
  t_82 = aten.t.default(arg13_1, )
  addmm_82 = aten.addmm.default(arg14_1, view_290, t_82, )
  view_291 = aten.view.default(addmm_82, [1, 9, 1024], )
  clone_45 = aten.clone.default(view_291, )
  add_68 = aten.add.Tensor(getitem_78, clone_45, )
  native_layer_norm_27 = aten.native_layer_norm.default(add_68, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_81 = native_layer_norm_27[0]
  view_292 = aten.view.default(getitem_81, [9, 1024], )
  t_83 = aten.t.default(arg17_1, )
  addmm_83 = aten.addmm.default(arg18_1, view_292, t_83, )
  view_293 = aten.view.default(addmm_83, [1, 9, 4096], )
  mul_53 = aten.mul.Tensor(view_293, 0.5, )
  pow_14 = aten.pow.Tensor_Scalar(view_293, 3.0, )
  mul_54 = aten.mul.Tensor(pow_14, 0.044715, )
  add_69 = aten.add.Tensor(view_293, mul_54, )
  mul_55 = aten.mul.Tensor(add_69, 0.7978845608028654, )
  tanh_13 = aten.tanh.default(mul_55, )
  add_70 = aten.add.Tensor(tanh_13, 1.0, )
  mul_56 = aten.mul.Tensor(mul_53, add_70, )
  view_294 = aten.view.default(mul_56, [9, 4096], )
  t_84 = aten.t.default(arg19_1, )
  addmm_84 = aten.addmm.default(arg20_1, view_294, t_84, )
  view_295 = aten.view.default(addmm_84, [1, 9, 1024], )
  add_71 = aten.add.Tensor(view_295, getitem_81, )
  native_layer_norm_28 = aten.native_layer_norm.default(add_71, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_84 = native_layer_norm_28[0]
  view_296 = aten.view.default(getitem_84, [9, 1024], )
  t_85 = aten.t.default(arg7_1, )
  addmm_85 = aten.addmm.default(arg8_1, view_296, t_85, )
  view_297 = aten.view.default(addmm_85, [1, 9, 1024], )
  view_298 = aten.view.default(getitem_84, [9, 1024], )
  t_86 = aten.t.default(arg9_1, )
  addmm_86 = aten.addmm.default(arg10_1, view_298, t_86, )
  view_299 = aten.view.default(addmm_86, [1, 9, 1024], )
  view_300 = aten.view.default(getitem_84, [9, 1024], )
  t_87 = aten.t.default(arg11_1, )
  addmm_87 = aten.addmm.default(arg12_1, view_300, t_87, )
  view_301 = aten.view.default(addmm_87, [1, 9, 1024], )
  view_302 = aten.view.default(view_297, [1, 9, 16, 64], )
  permute_42 = aten.permute.default(view_302, [0, 2, 1, 3], )
  view_303 = aten.view.default(view_299, [1, 9, 16, 64], )
  permute_43 = aten.permute.default(view_303, [0, 2, 1, 3], )
  view_304 = aten.view.default(view_301, [1, 9, 16, 64], )
  permute_44 = aten.permute.default(view_304, [0, 2, 1, 3], )
  transpose_28 = aten.transpose.int(permute_43, -1, -2, )
  expand_56 = aten.expand.default(permute_42, [1, 16, 9, 64], )
  view_305 = aten.view.default(expand_56, [16, 9, 64], )
  expand_57 = aten.expand.default(transpose_28, [1, 16, 64, 9], )
  view_306 = aten.view.default(expand_57, [16, 64, 9], )
  bmm_28 = aten.bmm.default(view_305, view_306, )
  view_307 = aten.view.default(bmm_28, [1, 16, 9, 9], )
  div_14 = aten.div.Tensor(view_307, 8.0, )
  add_72 = aten.add.Tensor(div_14, mul, )
  _softmax_14 = aten._softmax.default(add_72, -1, False, )
  clone_46 = aten.clone.default(_softmax_14, )
  expand_58 = aten.expand.default(clone_46, [1, 16, 9, 9], )
  view_308 = aten.view.default(expand_58, [16, 9, 9], )
  expand_59 = aten.expand.default(permute_44, [1, 16, 9, 64], )
  view_309 = aten.view.default(expand_59, [16, 9, 64], )
  bmm_29 = aten.bmm.default(view_308, view_309, )
  view_310 = aten.view.default(bmm_29, [1, 16, 9, 64], )
  transpose_29 = aten.transpose.int(view_310, 2, 1, )
  clone_47 = aten.clone.default(transpose_29, memory_format = torch.contiguous_format)
  _unsafe_view_14 = aten._unsafe_view.default(clone_47, [1, 9, 1024], )
  view_311 = aten.view.default(_unsafe_view_14, [9, 1024], )
  t_88 = aten.t.default(arg13_1, )
  addmm_88 = aten.addmm.default(arg14_1, view_311, t_88, )
  view_312 = aten.view.default(addmm_88, [1, 9, 1024], )
  clone_48 = aten.clone.default(view_312, )
  add_73 = aten.add.Tensor(getitem_84, clone_48, )
  native_layer_norm_29 = aten.native_layer_norm.default(add_73, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_87 = native_layer_norm_29[0]
  view_313 = aten.view.default(getitem_87, [9, 1024], )
  t_89 = aten.t.default(arg17_1, )
  addmm_89 = aten.addmm.default(arg18_1, view_313, t_89, )
  view_314 = aten.view.default(addmm_89, [1, 9, 4096], )
  mul_57 = aten.mul.Tensor(view_314, 0.5, )
  pow_15 = aten.pow.Tensor_Scalar(view_314, 3.0, )
  mul_58 = aten.mul.Tensor(pow_15, 0.044715, )
  add_74 = aten.add.Tensor(view_314, mul_58, )
  mul_59 = aten.mul.Tensor(add_74, 0.7978845608028654, )
  tanh_14 = aten.tanh.default(mul_59, )
  add_75 = aten.add.Tensor(tanh_14, 1.0, )
  mul_60 = aten.mul.Tensor(mul_57, add_75, )
  view_315 = aten.view.default(mul_60, [9, 4096], )
  t_90 = aten.t.default(arg19_1, )
  addmm_90 = aten.addmm.default(arg20_1, view_315, t_90, )
  view_316 = aten.view.default(addmm_90, [1, 9, 1024], )
  add_76 = aten.add.Tensor(view_316, getitem_87, )
  native_layer_norm_30 = aten.native_layer_norm.default(add_76, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_90 = native_layer_norm_30[0]
  view_317 = aten.view.default(getitem_90, [9, 1024], )
  t_91 = aten.t.default(arg7_1, )
  addmm_91 = aten.addmm.default(arg8_1, view_317, t_91, )
  view_318 = aten.view.default(addmm_91, [1, 9, 1024], )
  view_319 = aten.view.default(getitem_90, [9, 1024], )
  t_92 = aten.t.default(arg9_1, )
  addmm_92 = aten.addmm.default(arg10_1, view_319, t_92, )
  view_320 = aten.view.default(addmm_92, [1, 9, 1024], )
  view_321 = aten.view.default(getitem_90, [9, 1024], )
  t_93 = aten.t.default(arg11_1, )
  addmm_93 = aten.addmm.default(arg12_1, view_321, t_93, )
  view_322 = aten.view.default(addmm_93, [1, 9, 1024], )
  view_323 = aten.view.default(view_318, [1, 9, 16, 64], )
  permute_45 = aten.permute.default(view_323, [0, 2, 1, 3], )
  view_324 = aten.view.default(view_320, [1, 9, 16, 64], )
  permute_46 = aten.permute.default(view_324, [0, 2, 1, 3], )
  view_325 = aten.view.default(view_322, [1, 9, 16, 64], )
  permute_47 = aten.permute.default(view_325, [0, 2, 1, 3], )
  transpose_30 = aten.transpose.int(permute_46, -1, -2, )
  expand_60 = aten.expand.default(permute_45, [1, 16, 9, 64], )
  view_326 = aten.view.default(expand_60, [16, 9, 64], )
  expand_61 = aten.expand.default(transpose_30, [1, 16, 64, 9], )
  view_327 = aten.view.default(expand_61, [16, 64, 9], )
  bmm_30 = aten.bmm.default(view_326, view_327, )
  view_328 = aten.view.default(bmm_30, [1, 16, 9, 9], )
  div_15 = aten.div.Tensor(view_328, 8.0, )
  add_77 = aten.add.Tensor(div_15, mul, )
  _softmax_15 = aten._softmax.default(add_77, -1, False, )
  clone_49 = aten.clone.default(_softmax_15, )
  expand_62 = aten.expand.default(clone_49, [1, 16, 9, 9], )
  view_329 = aten.view.default(expand_62, [16, 9, 9], )
  expand_63 = aten.expand.default(permute_47, [1, 16, 9, 64], )
  view_330 = aten.view.default(expand_63, [16, 9, 64], )
  bmm_31 = aten.bmm.default(view_329, view_330, )
  view_331 = aten.view.default(bmm_31, [1, 16, 9, 64], )
  transpose_31 = aten.transpose.int(view_331, 2, 1, )
  clone_50 = aten.clone.default(transpose_31, memory_format = torch.contiguous_format)
  _unsafe_view_15 = aten._unsafe_view.default(clone_50, [1, 9, 1024], )
  view_332 = aten.view.default(_unsafe_view_15, [9, 1024], )
  t_94 = aten.t.default(arg13_1, )
  addmm_94 = aten.addmm.default(arg14_1, view_332, t_94, )
  view_333 = aten.view.default(addmm_94, [1, 9, 1024], )
  clone_51 = aten.clone.default(view_333, )
  add_78 = aten.add.Tensor(getitem_90, clone_51, )
  native_layer_norm_31 = aten.native_layer_norm.default(add_78, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_93 = native_layer_norm_31[0]
  view_334 = aten.view.default(getitem_93, [9, 1024], )
  t_95 = aten.t.default(arg17_1, )
  addmm_95 = aten.addmm.default(arg18_1, view_334, t_95, )
  view_335 = aten.view.default(addmm_95, [1, 9, 4096], )
  mul_61 = aten.mul.Tensor(view_335, 0.5, )
  pow_16 = aten.pow.Tensor_Scalar(view_335, 3.0, )
  mul_62 = aten.mul.Tensor(pow_16, 0.044715, )
  add_79 = aten.add.Tensor(view_335, mul_62, )
  mul_63 = aten.mul.Tensor(add_79, 0.7978845608028654, )
  tanh_15 = aten.tanh.default(mul_63, )
  add_80 = aten.add.Tensor(tanh_15, 1.0, )
  mul_64 = aten.mul.Tensor(mul_61, add_80, )
  view_336 = aten.view.default(mul_64, [9, 4096], )
  t_96 = aten.t.default(arg19_1, )
  addmm_96 = aten.addmm.default(arg20_1, view_336, t_96, )
  view_337 = aten.view.default(addmm_96, [1, 9, 1024], )
  add_81 = aten.add.Tensor(view_337, getitem_93, )
  native_layer_norm_32 = aten.native_layer_norm.default(add_81, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_96 = native_layer_norm_32[0]
  view_338 = aten.view.default(getitem_96, [9, 1024], )
  t_97 = aten.t.default(arg7_1, )
  addmm_97 = aten.addmm.default(arg8_1, view_338, t_97, )
  view_339 = aten.view.default(addmm_97, [1, 9, 1024], )
  view_340 = aten.view.default(getitem_96, [9, 1024], )
  t_98 = aten.t.default(arg9_1, )
  addmm_98 = aten.addmm.default(arg10_1, view_340, t_98, )
  view_341 = aten.view.default(addmm_98, [1, 9, 1024], )
  view_342 = aten.view.default(getitem_96, [9, 1024], )
  t_99 = aten.t.default(arg11_1, )
  addmm_99 = aten.addmm.default(arg12_1, view_342, t_99, )
  view_343 = aten.view.default(addmm_99, [1, 9, 1024], )
  view_344 = aten.view.default(view_339, [1, 9, 16, 64], )
  permute_48 = aten.permute.default(view_344, [0, 2, 1, 3], )
  view_345 = aten.view.default(view_341, [1, 9, 16, 64], )
  permute_49 = aten.permute.default(view_345, [0, 2, 1, 3], )
  view_346 = aten.view.default(view_343, [1, 9, 16, 64], )
  permute_50 = aten.permute.default(view_346, [0, 2, 1, 3], )
  transpose_32 = aten.transpose.int(permute_49, -1, -2, )
  expand_64 = aten.expand.default(permute_48, [1, 16, 9, 64], )
  view_347 = aten.view.default(expand_64, [16, 9, 64], )
  expand_65 = aten.expand.default(transpose_32, [1, 16, 64, 9], )
  view_348 = aten.view.default(expand_65, [16, 64, 9], )
  bmm_32 = aten.bmm.default(view_347, view_348, )
  view_349 = aten.view.default(bmm_32, [1, 16, 9, 9], )
  div_16 = aten.div.Tensor(view_349, 8.0, )
  add_82 = aten.add.Tensor(div_16, mul, )
  _softmax_16 = aten._softmax.default(add_82, -1, False, )
  clone_52 = aten.clone.default(_softmax_16, )
  expand_66 = aten.expand.default(clone_52, [1, 16, 9, 9], )
  view_350 = aten.view.default(expand_66, [16, 9, 9], )
  expand_67 = aten.expand.default(permute_50, [1, 16, 9, 64], )
  view_351 = aten.view.default(expand_67, [16, 9, 64], )
  bmm_33 = aten.bmm.default(view_350, view_351, )
  view_352 = aten.view.default(bmm_33, [1, 16, 9, 64], )
  transpose_33 = aten.transpose.int(view_352, 2, 1, )
  clone_53 = aten.clone.default(transpose_33, memory_format = torch.contiguous_format)
  _unsafe_view_16 = aten._unsafe_view.default(clone_53, [1, 9, 1024], )
  view_353 = aten.view.default(_unsafe_view_16, [9, 1024], )
  t_100 = aten.t.default(arg13_1, )
  addmm_100 = aten.addmm.default(arg14_1, view_353, t_100, )
  view_354 = aten.view.default(addmm_100, [1, 9, 1024], )
  clone_54 = aten.clone.default(view_354, )
  add_83 = aten.add.Tensor(getitem_96, clone_54, )
  native_layer_norm_33 = aten.native_layer_norm.default(add_83, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_99 = native_layer_norm_33[0]
  view_355 = aten.view.default(getitem_99, [9, 1024], )
  t_101 = aten.t.default(arg17_1, )
  addmm_101 = aten.addmm.default(arg18_1, view_355, t_101, )
  view_356 = aten.view.default(addmm_101, [1, 9, 4096], )
  mul_65 = aten.mul.Tensor(view_356, 0.5, )
  pow_17 = aten.pow.Tensor_Scalar(view_356, 3.0, )
  mul_66 = aten.mul.Tensor(pow_17, 0.044715, )
  add_84 = aten.add.Tensor(view_356, mul_66, )
  mul_67 = aten.mul.Tensor(add_84, 0.7978845608028654, )
  tanh_16 = aten.tanh.default(mul_67, )
  add_85 = aten.add.Tensor(tanh_16, 1.0, )
  mul_68 = aten.mul.Tensor(mul_65, add_85, )
  view_357 = aten.view.default(mul_68, [9, 4096], )
  t_102 = aten.t.default(arg19_1, )
  addmm_102 = aten.addmm.default(arg20_1, view_357, t_102, )
  view_358 = aten.view.default(addmm_102, [1, 9, 1024], )
  add_86 = aten.add.Tensor(view_358, getitem_99, )
  native_layer_norm_34 = aten.native_layer_norm.default(add_86, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_102 = native_layer_norm_34[0]
  view_359 = aten.view.default(getitem_102, [9, 1024], )
  t_103 = aten.t.default(arg7_1, )
  addmm_103 = aten.addmm.default(arg8_1, view_359, t_103, )
  view_360 = aten.view.default(addmm_103, [1, 9, 1024], )
  view_361 = aten.view.default(getitem_102, [9, 1024], )
  t_104 = aten.t.default(arg9_1, )
  addmm_104 = aten.addmm.default(arg10_1, view_361, t_104, )
  view_362 = aten.view.default(addmm_104, [1, 9, 1024], )
  view_363 = aten.view.default(getitem_102, [9, 1024], )
  t_105 = aten.t.default(arg11_1, )
  addmm_105 = aten.addmm.default(arg12_1, view_363, t_105, )
  view_364 = aten.view.default(addmm_105, [1, 9, 1024], )
  view_365 = aten.view.default(view_360, [1, 9, 16, 64], )
  permute_51 = aten.permute.default(view_365, [0, 2, 1, 3], )
  view_366 = aten.view.default(view_362, [1, 9, 16, 64], )
  permute_52 = aten.permute.default(view_366, [0, 2, 1, 3], )
  view_367 = aten.view.default(view_364, [1, 9, 16, 64], )
  permute_53 = aten.permute.default(view_367, [0, 2, 1, 3], )
  transpose_34 = aten.transpose.int(permute_52, -1, -2, )
  expand_68 = aten.expand.default(permute_51, [1, 16, 9, 64], )
  view_368 = aten.view.default(expand_68, [16, 9, 64], )
  expand_69 = aten.expand.default(transpose_34, [1, 16, 64, 9], )
  view_369 = aten.view.default(expand_69, [16, 64, 9], )
  bmm_34 = aten.bmm.default(view_368, view_369, )
  view_370 = aten.view.default(bmm_34, [1, 16, 9, 9], )
  div_17 = aten.div.Tensor(view_370, 8.0, )
  add_87 = aten.add.Tensor(div_17, mul, )
  _softmax_17 = aten._softmax.default(add_87, -1, False, )
  clone_55 = aten.clone.default(_softmax_17, )
  expand_70 = aten.expand.default(clone_55, [1, 16, 9, 9], )
  view_371 = aten.view.default(expand_70, [16, 9, 9], )
  expand_71 = aten.expand.default(permute_53, [1, 16, 9, 64], )
  view_372 = aten.view.default(expand_71, [16, 9, 64], )
  bmm_35 = aten.bmm.default(view_371, view_372, )
  view_373 = aten.view.default(bmm_35, [1, 16, 9, 64], )
  transpose_35 = aten.transpose.int(view_373, 2, 1, )
  clone_56 = aten.clone.default(transpose_35, memory_format = torch.contiguous_format)
  _unsafe_view_17 = aten._unsafe_view.default(clone_56, [1, 9, 1024], )
  view_374 = aten.view.default(_unsafe_view_17, [9, 1024], )
  t_106 = aten.t.default(arg13_1, )
  addmm_106 = aten.addmm.default(arg14_1, view_374, t_106, )
  view_375 = aten.view.default(addmm_106, [1, 9, 1024], )
  clone_57 = aten.clone.default(view_375, )
  add_88 = aten.add.Tensor(getitem_102, clone_57, )
  native_layer_norm_35 = aten.native_layer_norm.default(add_88, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_105 = native_layer_norm_35[0]
  view_376 = aten.view.default(getitem_105, [9, 1024], )
  t_107 = aten.t.default(arg17_1, )
  addmm_107 = aten.addmm.default(arg18_1, view_376, t_107, )
  view_377 = aten.view.default(addmm_107, [1, 9, 4096], )
  mul_69 = aten.mul.Tensor(view_377, 0.5, )
  pow_18 = aten.pow.Tensor_Scalar(view_377, 3.0, )
  mul_70 = aten.mul.Tensor(pow_18, 0.044715, )
  add_89 = aten.add.Tensor(view_377, mul_70, )
  mul_71 = aten.mul.Tensor(add_89, 0.7978845608028654, )
  tanh_17 = aten.tanh.default(mul_71, )
  add_90 = aten.add.Tensor(tanh_17, 1.0, )
  mul_72 = aten.mul.Tensor(mul_69, add_90, )
  view_378 = aten.view.default(mul_72, [9, 4096], )
  t_108 = aten.t.default(arg19_1, )
  addmm_108 = aten.addmm.default(arg20_1, view_378, t_108, )
  view_379 = aten.view.default(addmm_108, [1, 9, 1024], )
  add_91 = aten.add.Tensor(view_379, getitem_105, )
  native_layer_norm_36 = aten.native_layer_norm.default(add_91, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_108 = native_layer_norm_36[0]
  view_380 = aten.view.default(getitem_108, [9, 1024], )
  t_109 = aten.t.default(arg7_1, )
  addmm_109 = aten.addmm.default(arg8_1, view_380, t_109, )
  view_381 = aten.view.default(addmm_109, [1, 9, 1024], )
  view_382 = aten.view.default(getitem_108, [9, 1024], )
  t_110 = aten.t.default(arg9_1, )
  addmm_110 = aten.addmm.default(arg10_1, view_382, t_110, )
  view_383 = aten.view.default(addmm_110, [1, 9, 1024], )
  view_384 = aten.view.default(getitem_108, [9, 1024], )
  t_111 = aten.t.default(arg11_1, )
  addmm_111 = aten.addmm.default(arg12_1, view_384, t_111, )
  view_385 = aten.view.default(addmm_111, [1, 9, 1024], )
  view_386 = aten.view.default(view_381, [1, 9, 16, 64], )
  permute_54 = aten.permute.default(view_386, [0, 2, 1, 3], )
  view_387 = aten.view.default(view_383, [1, 9, 16, 64], )
  permute_55 = aten.permute.default(view_387, [0, 2, 1, 3], )
  view_388 = aten.view.default(view_385, [1, 9, 16, 64], )
  permute_56 = aten.permute.default(view_388, [0, 2, 1, 3], )
  transpose_36 = aten.transpose.int(permute_55, -1, -2, )
  expand_72 = aten.expand.default(permute_54, [1, 16, 9, 64], )
  view_389 = aten.view.default(expand_72, [16, 9, 64], )
  expand_73 = aten.expand.default(transpose_36, [1, 16, 64, 9], )
  view_390 = aten.view.default(expand_73, [16, 64, 9], )
  bmm_36 = aten.bmm.default(view_389, view_390, )
  view_391 = aten.view.default(bmm_36, [1, 16, 9, 9], )
  div_18 = aten.div.Tensor(view_391, 8.0, )
  add_92 = aten.add.Tensor(div_18, mul, )
  _softmax_18 = aten._softmax.default(add_92, -1, False, )
  clone_58 = aten.clone.default(_softmax_18, )
  expand_74 = aten.expand.default(clone_58, [1, 16, 9, 9], )
  view_392 = aten.view.default(expand_74, [16, 9, 9], )
  expand_75 = aten.expand.default(permute_56, [1, 16, 9, 64], )
  view_393 = aten.view.default(expand_75, [16, 9, 64], )
  bmm_37 = aten.bmm.default(view_392, view_393, )
  view_394 = aten.view.default(bmm_37, [1, 16, 9, 64], )
  transpose_37 = aten.transpose.int(view_394, 2, 1, )
  clone_59 = aten.clone.default(transpose_37, memory_format = torch.contiguous_format)
  _unsafe_view_18 = aten._unsafe_view.default(clone_59, [1, 9, 1024], )
  view_395 = aten.view.default(_unsafe_view_18, [9, 1024], )
  t_112 = aten.t.default(arg13_1, )
  addmm_112 = aten.addmm.default(arg14_1, view_395, t_112, )
  view_396 = aten.view.default(addmm_112, [1, 9, 1024], )
  clone_60 = aten.clone.default(view_396, )
  add_93 = aten.add.Tensor(getitem_108, clone_60, )
  native_layer_norm_37 = aten.native_layer_norm.default(add_93, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_111 = native_layer_norm_37[0]
  view_397 = aten.view.default(getitem_111, [9, 1024], )
  t_113 = aten.t.default(arg17_1, )
  addmm_113 = aten.addmm.default(arg18_1, view_397, t_113, )
  view_398 = aten.view.default(addmm_113, [1, 9, 4096], )
  mul_73 = aten.mul.Tensor(view_398, 0.5, )
  pow_19 = aten.pow.Tensor_Scalar(view_398, 3.0, )
  mul_74 = aten.mul.Tensor(pow_19, 0.044715, )
  add_94 = aten.add.Tensor(view_398, mul_74, )
  mul_75 = aten.mul.Tensor(add_94, 0.7978845608028654, )
  tanh_18 = aten.tanh.default(mul_75, )
  add_95 = aten.add.Tensor(tanh_18, 1.0, )
  mul_76 = aten.mul.Tensor(mul_73, add_95, )
  view_399 = aten.view.default(mul_76, [9, 4096], )
  t_114 = aten.t.default(arg19_1, )
  addmm_114 = aten.addmm.default(arg20_1, view_399, t_114, )
  view_400 = aten.view.default(addmm_114, [1, 9, 1024], )
  add_96 = aten.add.Tensor(view_400, getitem_111, )
  native_layer_norm_38 = aten.native_layer_norm.default(add_96, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_114 = native_layer_norm_38[0]
  view_401 = aten.view.default(getitem_114, [9, 1024], )
  t_115 = aten.t.default(arg7_1, )
  addmm_115 = aten.addmm.default(arg8_1, view_401, t_115, )
  view_402 = aten.view.default(addmm_115, [1, 9, 1024], )
  view_403 = aten.view.default(getitem_114, [9, 1024], )
  t_116 = aten.t.default(arg9_1, )
  addmm_116 = aten.addmm.default(arg10_1, view_403, t_116, )
  view_404 = aten.view.default(addmm_116, [1, 9, 1024], )
  view_405 = aten.view.default(getitem_114, [9, 1024], )
  t_117 = aten.t.default(arg11_1, )
  addmm_117 = aten.addmm.default(arg12_1, view_405, t_117, )
  view_406 = aten.view.default(addmm_117, [1, 9, 1024], )
  view_407 = aten.view.default(view_402, [1, 9, 16, 64], )
  permute_57 = aten.permute.default(view_407, [0, 2, 1, 3], )
  view_408 = aten.view.default(view_404, [1, 9, 16, 64], )
  permute_58 = aten.permute.default(view_408, [0, 2, 1, 3], )
  view_409 = aten.view.default(view_406, [1, 9, 16, 64], )
  permute_59 = aten.permute.default(view_409, [0, 2, 1, 3], )
  transpose_38 = aten.transpose.int(permute_58, -1, -2, )
  expand_76 = aten.expand.default(permute_57, [1, 16, 9, 64], )
  view_410 = aten.view.default(expand_76, [16, 9, 64], )
  expand_77 = aten.expand.default(transpose_38, [1, 16, 64, 9], )
  view_411 = aten.view.default(expand_77, [16, 64, 9], )
  bmm_38 = aten.bmm.default(view_410, view_411, )
  view_412 = aten.view.default(bmm_38, [1, 16, 9, 9], )
  div_19 = aten.div.Tensor(view_412, 8.0, )
  add_97 = aten.add.Tensor(div_19, mul, )
  _softmax_19 = aten._softmax.default(add_97, -1, False, )
  clone_61 = aten.clone.default(_softmax_19, )
  expand_78 = aten.expand.default(clone_61, [1, 16, 9, 9], )
  view_413 = aten.view.default(expand_78, [16, 9, 9], )
  expand_79 = aten.expand.default(permute_59, [1, 16, 9, 64], )
  view_414 = aten.view.default(expand_79, [16, 9, 64], )
  bmm_39 = aten.bmm.default(view_413, view_414, )
  view_415 = aten.view.default(bmm_39, [1, 16, 9, 64], )
  transpose_39 = aten.transpose.int(view_415, 2, 1, )
  clone_62 = aten.clone.default(transpose_39, memory_format = torch.contiguous_format)
  _unsafe_view_19 = aten._unsafe_view.default(clone_62, [1, 9, 1024], )
  view_416 = aten.view.default(_unsafe_view_19, [9, 1024], )
  t_118 = aten.t.default(arg13_1, )
  addmm_118 = aten.addmm.default(arg14_1, view_416, t_118, )
  view_417 = aten.view.default(addmm_118, [1, 9, 1024], )
  clone_63 = aten.clone.default(view_417, )
  add_98 = aten.add.Tensor(getitem_114, clone_63, )
  native_layer_norm_39 = aten.native_layer_norm.default(add_98, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_117 = native_layer_norm_39[0]
  view_418 = aten.view.default(getitem_117, [9, 1024], )
  t_119 = aten.t.default(arg17_1, )
  addmm_119 = aten.addmm.default(arg18_1, view_418, t_119, )
  view_419 = aten.view.default(addmm_119, [1, 9, 4096], )
  mul_77 = aten.mul.Tensor(view_419, 0.5, )
  pow_20 = aten.pow.Tensor_Scalar(view_419, 3.0, )
  mul_78 = aten.mul.Tensor(pow_20, 0.044715, )
  add_99 = aten.add.Tensor(view_419, mul_78, )
  mul_79 = aten.mul.Tensor(add_99, 0.7978845608028654, )
  tanh_19 = aten.tanh.default(mul_79, )
  add_100 = aten.add.Tensor(tanh_19, 1.0, )
  mul_80 = aten.mul.Tensor(mul_77, add_100, )
  view_420 = aten.view.default(mul_80, [9, 4096], )
  t_120 = aten.t.default(arg19_1, )
  addmm_120 = aten.addmm.default(arg20_1, view_420, t_120, )
  view_421 = aten.view.default(addmm_120, [1, 9, 1024], )
  add_101 = aten.add.Tensor(view_421, getitem_117, )
  native_layer_norm_40 = aten.native_layer_norm.default(add_101, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_120 = native_layer_norm_40[0]
  view_422 = aten.view.default(getitem_120, [9, 1024], )
  t_121 = aten.t.default(arg7_1, )
  addmm_121 = aten.addmm.default(arg8_1, view_422, t_121, )
  view_423 = aten.view.default(addmm_121, [1, 9, 1024], )
  view_424 = aten.view.default(getitem_120, [9, 1024], )
  t_122 = aten.t.default(arg9_1, )
  addmm_122 = aten.addmm.default(arg10_1, view_424, t_122, )
  view_425 = aten.view.default(addmm_122, [1, 9, 1024], )
  view_426 = aten.view.default(getitem_120, [9, 1024], )
  t_123 = aten.t.default(arg11_1, )
  addmm_123 = aten.addmm.default(arg12_1, view_426, t_123, )
  view_427 = aten.view.default(addmm_123, [1, 9, 1024], )
  view_428 = aten.view.default(view_423, [1, 9, 16, 64], )
  permute_60 = aten.permute.default(view_428, [0, 2, 1, 3], )
  view_429 = aten.view.default(view_425, [1, 9, 16, 64], )
  permute_61 = aten.permute.default(view_429, [0, 2, 1, 3], )
  view_430 = aten.view.default(view_427, [1, 9, 16, 64], )
  permute_62 = aten.permute.default(view_430, [0, 2, 1, 3], )
  transpose_40 = aten.transpose.int(permute_61, -1, -2, )
  expand_80 = aten.expand.default(permute_60, [1, 16, 9, 64], )
  view_431 = aten.view.default(expand_80, [16, 9, 64], )
  expand_81 = aten.expand.default(transpose_40, [1, 16, 64, 9], )
  view_432 = aten.view.default(expand_81, [16, 64, 9], )
  bmm_40 = aten.bmm.default(view_431, view_432, )
  view_433 = aten.view.default(bmm_40, [1, 16, 9, 9], )
  div_20 = aten.div.Tensor(view_433, 8.0, )
  add_102 = aten.add.Tensor(div_20, mul, )
  _softmax_20 = aten._softmax.default(add_102, -1, False, )
  clone_64 = aten.clone.default(_softmax_20, )
  expand_82 = aten.expand.default(clone_64, [1, 16, 9, 9], )
  view_434 = aten.view.default(expand_82, [16, 9, 9], )
  expand_83 = aten.expand.default(permute_62, [1, 16, 9, 64], )
  view_435 = aten.view.default(expand_83, [16, 9, 64], )
  bmm_41 = aten.bmm.default(view_434, view_435, )
  view_436 = aten.view.default(bmm_41, [1, 16, 9, 64], )
  transpose_41 = aten.transpose.int(view_436, 2, 1, )
  clone_65 = aten.clone.default(transpose_41, memory_format = torch.contiguous_format)
  _unsafe_view_20 = aten._unsafe_view.default(clone_65, [1, 9, 1024], )
  view_437 = aten.view.default(_unsafe_view_20, [9, 1024], )
  t_124 = aten.t.default(arg13_1, )
  addmm_124 = aten.addmm.default(arg14_1, view_437, t_124, )
  view_438 = aten.view.default(addmm_124, [1, 9, 1024], )
  clone_66 = aten.clone.default(view_438, )
  add_103 = aten.add.Tensor(getitem_120, clone_66, )
  native_layer_norm_41 = aten.native_layer_norm.default(add_103, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_123 = native_layer_norm_41[0]
  view_439 = aten.view.default(getitem_123, [9, 1024], )
  t_125 = aten.t.default(arg17_1, )
  addmm_125 = aten.addmm.default(arg18_1, view_439, t_125, )
  view_440 = aten.view.default(addmm_125, [1, 9, 4096], )
  mul_81 = aten.mul.Tensor(view_440, 0.5, )
  pow_21 = aten.pow.Tensor_Scalar(view_440, 3.0, )
  mul_82 = aten.mul.Tensor(pow_21, 0.044715, )
  add_104 = aten.add.Tensor(view_440, mul_82, )
  mul_83 = aten.mul.Tensor(add_104, 0.7978845608028654, )
  tanh_20 = aten.tanh.default(mul_83, )
  add_105 = aten.add.Tensor(tanh_20, 1.0, )
  mul_84 = aten.mul.Tensor(mul_81, add_105, )
  view_441 = aten.view.default(mul_84, [9, 4096], )
  t_126 = aten.t.default(arg19_1, )
  addmm_126 = aten.addmm.default(arg20_1, view_441, t_126, )
  view_442 = aten.view.default(addmm_126, [1, 9, 1024], )
  add_106 = aten.add.Tensor(view_442, getitem_123, )
  native_layer_norm_42 = aten.native_layer_norm.default(add_106, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_126 = native_layer_norm_42[0]
  view_443 = aten.view.default(getitem_126, [9, 1024], )
  t_127 = aten.t.default(arg7_1, )
  addmm_127 = aten.addmm.default(arg8_1, view_443, t_127, )
  view_444 = aten.view.default(addmm_127, [1, 9, 1024], )
  view_445 = aten.view.default(getitem_126, [9, 1024], )
  t_128 = aten.t.default(arg9_1, )
  addmm_128 = aten.addmm.default(arg10_1, view_445, t_128, )
  view_446 = aten.view.default(addmm_128, [1, 9, 1024], )
  view_447 = aten.view.default(getitem_126, [9, 1024], )
  t_129 = aten.t.default(arg11_1, )
  addmm_129 = aten.addmm.default(arg12_1, view_447, t_129, )
  view_448 = aten.view.default(addmm_129, [1, 9, 1024], )
  view_449 = aten.view.default(view_444, [1, 9, 16, 64], )
  permute_63 = aten.permute.default(view_449, [0, 2, 1, 3], )
  view_450 = aten.view.default(view_446, [1, 9, 16, 64], )
  permute_64 = aten.permute.default(view_450, [0, 2, 1, 3], )
  view_451 = aten.view.default(view_448, [1, 9, 16, 64], )
  permute_65 = aten.permute.default(view_451, [0, 2, 1, 3], )
  transpose_42 = aten.transpose.int(permute_64, -1, -2, )
  expand_84 = aten.expand.default(permute_63, [1, 16, 9, 64], )
  view_452 = aten.view.default(expand_84, [16, 9, 64], )
  expand_85 = aten.expand.default(transpose_42, [1, 16, 64, 9], )
  view_453 = aten.view.default(expand_85, [16, 64, 9], )
  bmm_42 = aten.bmm.default(view_452, view_453, )
  view_454 = aten.view.default(bmm_42, [1, 16, 9, 9], )
  div_21 = aten.div.Tensor(view_454, 8.0, )
  add_107 = aten.add.Tensor(div_21, mul, )
  _softmax_21 = aten._softmax.default(add_107, -1, False, )
  clone_67 = aten.clone.default(_softmax_21, )
  expand_86 = aten.expand.default(clone_67, [1, 16, 9, 9], )
  view_455 = aten.view.default(expand_86, [16, 9, 9], )
  expand_87 = aten.expand.default(permute_65, [1, 16, 9, 64], )
  view_456 = aten.view.default(expand_87, [16, 9, 64], )
  bmm_43 = aten.bmm.default(view_455, view_456, )
  view_457 = aten.view.default(bmm_43, [1, 16, 9, 64], )
  transpose_43 = aten.transpose.int(view_457, 2, 1, )
  clone_68 = aten.clone.default(transpose_43, memory_format = torch.contiguous_format)
  _unsafe_view_21 = aten._unsafe_view.default(clone_68, [1, 9, 1024], )
  view_458 = aten.view.default(_unsafe_view_21, [9, 1024], )
  t_130 = aten.t.default(arg13_1, )
  addmm_130 = aten.addmm.default(arg14_1, view_458, t_130, )
  view_459 = aten.view.default(addmm_130, [1, 9, 1024], )
  clone_69 = aten.clone.default(view_459, )
  add_108 = aten.add.Tensor(getitem_126, clone_69, )
  native_layer_norm_43 = aten.native_layer_norm.default(add_108, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_129 = native_layer_norm_43[0]
  view_460 = aten.view.default(getitem_129, [9, 1024], )
  t_131 = aten.t.default(arg17_1, )
  addmm_131 = aten.addmm.default(arg18_1, view_460, t_131, )
  view_461 = aten.view.default(addmm_131, [1, 9, 4096], )
  mul_85 = aten.mul.Tensor(view_461, 0.5, )
  pow_22 = aten.pow.Tensor_Scalar(view_461, 3.0, )
  mul_86 = aten.mul.Tensor(pow_22, 0.044715, )
  add_109 = aten.add.Tensor(view_461, mul_86, )
  mul_87 = aten.mul.Tensor(add_109, 0.7978845608028654, )
  tanh_21 = aten.tanh.default(mul_87, )
  add_110 = aten.add.Tensor(tanh_21, 1.0, )
  mul_88 = aten.mul.Tensor(mul_85, add_110, )
  view_462 = aten.view.default(mul_88, [9, 4096], )
  t_132 = aten.t.default(arg19_1, )
  addmm_132 = aten.addmm.default(arg20_1, view_462, t_132, )
  view_463 = aten.view.default(addmm_132, [1, 9, 1024], )
  add_111 = aten.add.Tensor(view_463, getitem_129, )
  native_layer_norm_44 = aten.native_layer_norm.default(add_111, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_132 = native_layer_norm_44[0]
  view_464 = aten.view.default(getitem_132, [9, 1024], )
  t_133 = aten.t.default(arg7_1, )
  addmm_133 = aten.addmm.default(arg8_1, view_464, t_133, )
  view_465 = aten.view.default(addmm_133, [1, 9, 1024], )
  view_466 = aten.view.default(getitem_132, [9, 1024], )
  t_134 = aten.t.default(arg9_1, )
  addmm_134 = aten.addmm.default(arg10_1, view_466, t_134, )
  view_467 = aten.view.default(addmm_134, [1, 9, 1024], )
  view_468 = aten.view.default(getitem_132, [9, 1024], )
  t_135 = aten.t.default(arg11_1, )
  addmm_135 = aten.addmm.default(arg12_1, view_468, t_135, )
  view_469 = aten.view.default(addmm_135, [1, 9, 1024], )
  view_470 = aten.view.default(view_465, [1, 9, 16, 64], )
  permute_66 = aten.permute.default(view_470, [0, 2, 1, 3], )
  view_471 = aten.view.default(view_467, [1, 9, 16, 64], )
  permute_67 = aten.permute.default(view_471, [0, 2, 1, 3], )
  view_472 = aten.view.default(view_469, [1, 9, 16, 64], )
  permute_68 = aten.permute.default(view_472, [0, 2, 1, 3], )
  transpose_44 = aten.transpose.int(permute_67, -1, -2, )
  expand_88 = aten.expand.default(permute_66, [1, 16, 9, 64], )
  view_473 = aten.view.default(expand_88, [16, 9, 64], )
  expand_89 = aten.expand.default(transpose_44, [1, 16, 64, 9], )
  view_474 = aten.view.default(expand_89, [16, 64, 9], )
  bmm_44 = aten.bmm.default(view_473, view_474, )
  view_475 = aten.view.default(bmm_44, [1, 16, 9, 9], )
  div_22 = aten.div.Tensor(view_475, 8.0, )
  add_112 = aten.add.Tensor(div_22, mul, )
  _softmax_22 = aten._softmax.default(add_112, -1, False, )
  clone_70 = aten.clone.default(_softmax_22, )
  expand_90 = aten.expand.default(clone_70, [1, 16, 9, 9], )
  view_476 = aten.view.default(expand_90, [16, 9, 9], )
  expand_91 = aten.expand.default(permute_68, [1, 16, 9, 64], )
  view_477 = aten.view.default(expand_91, [16, 9, 64], )
  bmm_45 = aten.bmm.default(view_476, view_477, )
  view_478 = aten.view.default(bmm_45, [1, 16, 9, 64], )
  transpose_45 = aten.transpose.int(view_478, 2, 1, )
  clone_71 = aten.clone.default(transpose_45, memory_format = torch.contiguous_format)
  _unsafe_view_22 = aten._unsafe_view.default(clone_71, [1, 9, 1024], )
  view_479 = aten.view.default(_unsafe_view_22, [9, 1024], )
  t_136 = aten.t.default(arg13_1, )
  addmm_136 = aten.addmm.default(arg14_1, view_479, t_136, )
  view_480 = aten.view.default(addmm_136, [1, 9, 1024], )
  clone_72 = aten.clone.default(view_480, )
  add_113 = aten.add.Tensor(getitem_132, clone_72, )
  native_layer_norm_45 = aten.native_layer_norm.default(add_113, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_135 = native_layer_norm_45[0]
  view_481 = aten.view.default(getitem_135, [9, 1024], )
  t_137 = aten.t.default(arg17_1, )
  addmm_137 = aten.addmm.default(arg18_1, view_481, t_137, )
  view_482 = aten.view.default(addmm_137, [1, 9, 4096], )
  mul_89 = aten.mul.Tensor(view_482, 0.5, )
  pow_23 = aten.pow.Tensor_Scalar(view_482, 3.0, )
  mul_90 = aten.mul.Tensor(pow_23, 0.044715, )
  add_114 = aten.add.Tensor(view_482, mul_90, )
  mul_91 = aten.mul.Tensor(add_114, 0.7978845608028654, )
  tanh_22 = aten.tanh.default(mul_91, )
  add_115 = aten.add.Tensor(tanh_22, 1.0, )
  mul_92 = aten.mul.Tensor(mul_89, add_115, )
  view_483 = aten.view.default(mul_92, [9, 4096], )
  t_138 = aten.t.default(arg19_1, )
  addmm_138 = aten.addmm.default(arg20_1, view_483, t_138, )
  view_484 = aten.view.default(addmm_138, [1, 9, 1024], )
  add_116 = aten.add.Tensor(view_484, getitem_135, )
  native_layer_norm_46 = aten.native_layer_norm.default(add_116, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_138 = native_layer_norm_46[0]
  view_485 = aten.view.default(getitem_138, [9, 1024], )
  t_139 = aten.t.default(arg7_1, )
  addmm_139 = aten.addmm.default(arg8_1, view_485, t_139, )
  view_486 = aten.view.default(addmm_139, [1, 9, 1024], )
  view_487 = aten.view.default(getitem_138, [9, 1024], )
  t_140 = aten.t.default(arg9_1, )
  addmm_140 = aten.addmm.default(arg10_1, view_487, t_140, )
  view_488 = aten.view.default(addmm_140, [1, 9, 1024], )
  view_489 = aten.view.default(getitem_138, [9, 1024], )
  t_141 = aten.t.default(arg11_1, )
  addmm_141 = aten.addmm.default(arg12_1, view_489, t_141, )
  view_490 = aten.view.default(addmm_141, [1, 9, 1024], )
  view_491 = aten.view.default(view_486, [1, 9, 16, 64], )
  permute_69 = aten.permute.default(view_491, [0, 2, 1, 3], )
  view_492 = aten.view.default(view_488, [1, 9, 16, 64], )
  permute_70 = aten.permute.default(view_492, [0, 2, 1, 3], )
  view_493 = aten.view.default(view_490, [1, 9, 16, 64], )
  permute_71 = aten.permute.default(view_493, [0, 2, 1, 3], )
  transpose_46 = aten.transpose.int(permute_70, -1, -2, )
  expand_92 = aten.expand.default(permute_69, [1, 16, 9, 64], )
  view_494 = aten.view.default(expand_92, [16, 9, 64], )
  expand_93 = aten.expand.default(transpose_46, [1, 16, 64, 9], )
  view_495 = aten.view.default(expand_93, [16, 64, 9], )
  bmm_46 = aten.bmm.default(view_494, view_495, )
  view_496 = aten.view.default(bmm_46, [1, 16, 9, 9], )
  div_23 = aten.div.Tensor(view_496, 8.0, )
  add_117 = aten.add.Tensor(div_23, mul, )
  _softmax_23 = aten._softmax.default(add_117, -1, False, )
  clone_73 = aten.clone.default(_softmax_23, )
  expand_94 = aten.expand.default(clone_73, [1, 16, 9, 9], )
  view_497 = aten.view.default(expand_94, [16, 9, 9], )
  expand_95 = aten.expand.default(permute_71, [1, 16, 9, 64], )
  view_498 = aten.view.default(expand_95, [16, 9, 64], )
  bmm_47 = aten.bmm.default(view_497, view_498, )
  view_499 = aten.view.default(bmm_47, [1, 16, 9, 64], )
  transpose_47 = aten.transpose.int(view_499, 2, 1, )
  clone_74 = aten.clone.default(transpose_47, memory_format = torch.contiguous_format)
  _unsafe_view_23 = aten._unsafe_view.default(clone_74, [1, 9, 1024], )
  view_500 = aten.view.default(_unsafe_view_23, [9, 1024], )
  t_142 = aten.t.default(arg13_1, )
  addmm_142 = aten.addmm.default(arg14_1, view_500, t_142, )
  view_501 = aten.view.default(addmm_142, [1, 9, 1024], )
  clone_75 = aten.clone.default(view_501, )
  add_118 = aten.add.Tensor(getitem_138, clone_75, )
  native_layer_norm_47 = aten.native_layer_norm.default(add_118, [1024], arg15_1, arg16_1, 1e-12, )
  getitem_141 = native_layer_norm_47[0]
  view_502 = aten.view.default(getitem_141, [9, 1024], )
  t_143 = aten.t.default(arg17_1, )
  addmm_143 = aten.addmm.default(arg18_1, view_502, t_143, )
  view_503 = aten.view.default(addmm_143, [1, 9, 4096], )
  mul_93 = aten.mul.Tensor(view_503, 0.5, )
  pow_24 = aten.pow.Tensor_Scalar(view_503, 3.0, )
  mul_94 = aten.mul.Tensor(pow_24, 0.044715, )
  add_119 = aten.add.Tensor(view_503, mul_94, )
  mul_95 = aten.mul.Tensor(add_119, 0.7978845608028654, )
  tanh_23 = aten.tanh.default(mul_95, )
  add_120 = aten.add.Tensor(tanh_23, 1.0, )
  mul_96 = aten.mul.Tensor(mul_93, add_120, )
  view_504 = aten.view.default(mul_96, [9, 4096], )
  t_144 = aten.t.default(arg19_1, )
  addmm_144 = aten.addmm.default(arg20_1, view_504, t_144, )
  view_505 = aten.view.default(addmm_144, [1, 9, 1024], )
  add_121 = aten.add.Tensor(view_505, getitem_141, )
  native_layer_norm_48 = aten.native_layer_norm.default(add_121, [1024], arg21_1, arg22_1, 1e-12, )
  getitem_144 = native_layer_norm_48[0]
  view_506 = aten.view.default(getitem_144, [9, 1024], )
  t_145 = aten.t.default(arg23_1, )
  addmm_145 = aten.addmm.default(arg24_1, view_506, t_145, )
  view_507 = aten.view.default(addmm_145, [1, 9, 128], )
  mul_97 = aten.mul.Tensor(view_507, 0.5, )
  pow_25 = aten.pow.Tensor_Scalar(view_507, 3.0, )
  mul_98 = aten.mul.Tensor(pow_25, 0.044715, )
  add_122 = aten.add.Tensor(view_507, mul_98, )
  mul_99 = aten.mul.Tensor(add_122, 0.7978845608028654, )
  tanh_24 = aten.tanh.default(mul_99, )
  add_123 = aten.add.Tensor(tanh_24, 1.0, )
  mul_100 = aten.mul.Tensor(mul_97, add_123, )
  native_layer_norm_49 = aten.native_layer_norm.default(mul_100, [128], arg25_1, arg26_1, 1e-12, )
  getitem_147 = native_layer_norm_49[0]
  view_508 = aten.view.default(getitem_147, [9, 128], )
  t_146 = aten.t.default(arg27_1, )
  addmm_146 = aten.addmm.default(arg28_1, view_508, t_146, )
  view_509 = aten.view.default(addmm_146, [1, 9, 30000], )
  # return (view_509,)
  ttnn_from_torch = ttnn.from_torch(arg31_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_reshape = ttnn.reshape(ttnn_from_torch, (1, 1, 9), )
  test_accuracy(unsqueeze, ttnn_reshape)
  ttnn_from_device = ttnn.from_device(ttnn_reshape, )
  ttnn_to_layout_3 = ttnn.to_layout(ttnn_from_device, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_1 = ttnn.reshape(ttnn_to_layout_3, (1, 1, 1, 9), )
  test_accuracy(unsqueeze_1, ttnn_reshape_1)
  ttnn_from_device_1 = ttnn.from_device(ttnn_reshape_1, )
  ttnn_to_layout_4 = ttnn.to_layout(ttnn_from_device_1, ttnn.TILE_LAYOUT, )
  ttnn_to_device = ttnn.to_device(ttnn_to_layout_4, device = device)
  ttnn_typecast = ttnn.typecast(ttnn_to_device, ttnn.bfloat16, )
  test_accuracy(_to_copy, ttnn_typecast)
  ttnn_rsub = ttnn.rsub(ttnn_typecast, 1.0, )
  test_accuracy(rsub, ttnn_rsub)
  ttnn_multiply = ttnn.multiply(ttnn_rsub, -3.3895313892515355e+38, )
  test_accuracy(mul, ttnn_multiply)
  ttnn_from_torch_1 = ttnn.from_torch(arg29_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_slice = ttnn.slice(ttnn_from_torch_1, [0, 0], [1, 9], )
  test_accuracy(slice_2, ttnn_slice)
  ttnn_from_torch_2 = ttnn.from_torch(arg30_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_from_torch_3 = ttnn.from_torch(arg0_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_embedding = ttnn.embedding(ttnn_from_torch_2, ttnn_from_torch_3, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout = ttnn.to_layout(ttnn_embedding, ttnn.TILE_LAYOUT, )
  ttnn_from_torch_4 = ttnn.from_torch(arg32_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_from_torch_5 = ttnn.from_torch(arg1_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_embedding_1 = ttnn.embedding(ttnn_from_torch_4, ttnn_from_torch_5, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout_1 = ttnn.to_layout(ttnn_embedding_1, ttnn.TILE_LAYOUT, )
  ttnn_add_147 = ttnn.add(ttnn_to_layout, ttnn_to_layout_1, )
  ttnn_from_device_2 = ttnn.from_device(ttnn_slice, )
  ttnn_to_layout_5 = ttnn.to_layout(ttnn_from_device_2, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_1 = ttnn.to_device(ttnn_to_layout_5, device = device)
  ttnn_from_torch_6 = ttnn.from_torch(arg2_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_embedding_2 = ttnn.embedding(ttnn_to_device_1, ttnn_from_torch_6, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout_2 = ttnn.to_layout(ttnn_embedding_2, ttnn.TILE_LAYOUT, )
  test_accuracy(embedding_2, ttnn_to_layout_2)
  ttnn_add_148 = ttnn.add(ttnn_add_147, ttnn_to_layout_2, )
  ttnn_from_torch_7 = ttnn.from_torch(arg3_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_8 = ttnn.from_torch(arg4_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_ = ttnn.layer_norm(ttnn_add_148, epsilon = 1e-12, weight = ttnn_from_torch_7, bias = ttnn_from_torch_8)
  ttnn_layer_norm_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_148), [128], ttnn.to_torch(ttnn_from_torch_7), 
                                                                ttnn.to_torch(ttnn_from_torch_8), 1e-12, )[0]
  ttnn_layer_norm = ttnn.from_torch(ttnn_layer_norm_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_), ttnn_layer_norm))
  ttnn_prefix_clone = clone_wrapper(ttnn_layer_norm, )
  test_accuracy(clone_3, ttnn_prefix_clone)
  ttnn_from_device_3 = ttnn.from_device(ttnn_prefix_clone, )
  ttnn_to_layout_6 = ttnn.to_layout(ttnn_from_device_3, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_2 = ttnn.reshape(ttnn_to_layout_6, (9, 128), )
  ttnn_from_torch_9 = ttnn.from_torch(arg5_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose = ttnn.transpose(ttnn_from_torch_9, 0, 1, )
  test_accuracy(t, ttnn_transpose)
  ttnn_from_device_4 = ttnn.from_device(ttnn_reshape_2, )
  ttnn_to_layout_7 = ttnn.to_layout(ttnn_from_device_4, ttnn.TILE_LAYOUT, )
  ttnn_to_device_2 = ttnn.to_device(ttnn_to_layout_7, device = device)
  ttnn_matmul = ttnn.matmul(ttnn_to_device_2, ttnn_transpose, )
  ttnn_from_torch_10 = ttnn.from_torch(arg6_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add = ttnn.add(ttnn_from_torch_10, ttnn_matmul, )
  ttnn_from_device_5 = ttnn.from_device(ttnn_add, )
  ttnn_to_layout_8 = ttnn.to_layout(ttnn_from_device_5, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_3 = ttnn.reshape(ttnn_to_layout_8, (1, 9, 1024), )
  ttnn_from_device_6 = ttnn.from_device(ttnn_reshape_3, )
  ttnn_to_layout_9 = ttnn.to_layout(ttnn_from_device_6, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_4 = ttnn.reshape(ttnn_to_layout_9, (9, 1024), )
  ttnn_from_torch_11 = ttnn.from_torch(arg7_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_1 = ttnn.transpose(ttnn_from_torch_11, 0, 1, )
  ttnn_from_device_7 = ttnn.from_device(ttnn_reshape_4, )
  ttnn_to_layout_10 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_3 = ttnn.to_device(ttnn_to_layout_10, device = device)
  ttnn_matmul_1 = ttnn.matmul(ttnn_to_device_3, ttnn_transpose_1, )
  ttnn_from_torch_12 = ttnn.from_torch(arg8_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_1 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_1, )
  ttnn_from_device_8 = ttnn.from_device(ttnn_add_1, )
  ttnn_to_layout_11 = ttnn.to_layout(ttnn_from_device_8, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_5 = ttnn.reshape(ttnn_to_layout_11, (1, 9, 1024), )
  ttnn_from_torch_13 = ttnn.from_torch(arg9_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_2 = ttnn.transpose(ttnn_from_torch_13, 0, 1, )
  ttnn_to_layout_12 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_4 = ttnn.to_device(ttnn_to_layout_12, device = device)
  ttnn_matmul_2 = ttnn.matmul(ttnn_to_device_4, ttnn_transpose_2, )
  ttnn_from_torch_14 = ttnn.from_torch(arg10_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_2 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_2, )
  ttnn_from_device_10 = ttnn.from_device(ttnn_add_2, )
  ttnn_to_layout_13 = ttnn.to_layout(ttnn_from_device_10, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_7 = ttnn.reshape(ttnn_to_layout_13, (1, 9, 1024), )
  ttnn_from_torch_15 = ttnn.from_torch(arg11_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_3 = ttnn.transpose(ttnn_from_torch_15, 0, 1, )
  ttnn_to_layout_14 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_5 = ttnn.to_device(ttnn_to_layout_14, device = device)
  ttnn_matmul_3 = ttnn.matmul(ttnn_to_device_5, ttnn_transpose_3, )
  ttnn_from_torch_16 = ttnn.from_torch(arg12_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_3 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_3, )
  ttnn_from_device_12 = ttnn.from_device(ttnn_add_3, )
  ttnn_to_layout_15 = ttnn.to_layout(ttnn_from_device_12, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_9 = ttnn.reshape(ttnn_to_layout_15, (1, 9, 1024), )
  ttnn_from_device_13 = ttnn.from_device(ttnn_reshape_5, )
  ttnn_to_layout_16 = ttnn.to_layout(ttnn_from_device_13, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_10 = ttnn.reshape(ttnn_to_layout_16, (1, 9, 16, 64), )
  ttnn_from_device_14 = ttnn.from_device(ttnn_reshape_10, )
  ttnn_to_layout_17 = ttnn.to_layout(ttnn_from_device_14, ttnn.TILE_LAYOUT, )
  ttnn_to_device_6 = ttnn.to_device(ttnn_to_layout_17, device = device)
  ttnn_permute = ttnn.permute(ttnn_to_device_6, (0, 2, 1, 3), )
  ttnn_from_device_15 = ttnn.from_device(ttnn_reshape_7, )
  ttnn_to_layout_18 = ttnn.to_layout(ttnn_from_device_15, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_11 = ttnn.reshape(ttnn_to_layout_18, (1, 9, 16, 64), )
  ttnn_from_device_16 = ttnn.from_device(ttnn_reshape_11, )
  ttnn_to_layout_19 = ttnn.to_layout(ttnn_from_device_16, ttnn.TILE_LAYOUT, )
  ttnn_to_device_7 = ttnn.to_device(ttnn_to_layout_19, device = device)
  ttnn_permute_1 = ttnn.permute(ttnn_to_device_7, (0, 2, 1, 3), )
  ttnn_from_device_17 = ttnn.from_device(ttnn_reshape_9, )
  ttnn_to_layout_20 = ttnn.to_layout(ttnn_from_device_17, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_12 = ttnn.reshape(ttnn_to_layout_20, (1, 9, 16, 64), )
  ttnn_from_device_18 = ttnn.from_device(ttnn_reshape_12, )
  ttnn_to_layout_21 = ttnn.to_layout(ttnn_from_device_18, ttnn.TILE_LAYOUT, )
  ttnn_to_device_8 = ttnn.to_device(ttnn_to_layout_21, device = device)
  ttnn_permute_2 = ttnn.permute(ttnn_to_device_8, (0, 2, 1, 3), )
  ttnn_transpose_4 = ttnn.transpose(ttnn_permute_1, 3, 2, )
  ttnn_from_device_19 = ttnn.from_device(ttnn_permute, )
  ttnn_to_layout_22 = ttnn.to_layout(ttnn_from_device_19, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_13 = ttnn.reshape(ttnn_to_layout_22, (16, 9, 64), )
  ttnn_from_device_20 = ttnn.from_device(ttnn_transpose_4, )
  ttnn_to_layout_23 = ttnn.to_layout(ttnn_from_device_20, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_14 = ttnn.reshape(ttnn_to_layout_23, (16, 64, 9), )
  ttnn_from_device_21 = ttnn.from_device(ttnn_reshape_13, )
  ttnn_to_layout_24 = ttnn.to_layout(ttnn_from_device_21, ttnn.TILE_LAYOUT, )
  ttnn_to_device_9 = ttnn.to_device(ttnn_to_layout_24, device = device)
  ttnn_from_device_22 = ttnn.from_device(ttnn_reshape_14, )
  ttnn_to_layout_25 = ttnn.to_layout(ttnn_from_device_22, ttnn.TILE_LAYOUT, )
  ttnn_to_device_10 = ttnn.to_device(ttnn_to_layout_25, device = device)
  ttnn_matmul_4 = ttnn.matmul(ttnn_to_device_9, ttnn_to_device_10, )
  ttnn_from_device_23 = ttnn.from_device(ttnn_matmul_4, )
  ttnn_to_layout_26 = ttnn.to_layout(ttnn_from_device_23, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_15 = ttnn.reshape(ttnn_to_layout_26, (1, 16, 9, 9), )
  ttnn_from_device_24 = ttnn.from_device(ttnn_reshape_15, )
  ttnn_to_layout_27 = ttnn.to_layout(ttnn_from_device_24, ttnn.TILE_LAYOUT, )
  ttnn_to_device_11 = ttnn.to_device(ttnn_to_layout_27, device = device)
  ttnn_multiply_1 = ttnn.multiply(ttnn_to_device_11, 0.125, )
  ttnn_add_149 = ttnn.add(ttnn_multiply_1, ttnn_multiply, )
  ttnn_softmax = ttnn.softmax(ttnn_add_149, -1, numeric_stable = True)
  test_accuracy(_softmax, ttnn_softmax)
  ttnn_prefix_clone_1 = clone_wrapper(ttnn_softmax, )
  ttnn_from_device_25 = ttnn.from_device(ttnn_prefix_clone_1, )
  ttnn_to_layout_28 = ttnn.to_layout(ttnn_from_device_25, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_16 = ttnn.reshape(ttnn_to_layout_28, (16, 9, 9), )
  ttnn_from_device_26 = ttnn.from_device(ttnn_permute_2, )
  ttnn_to_layout_29 = ttnn.to_layout(ttnn_from_device_26, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_17 = ttnn.reshape(ttnn_to_layout_29, (16, 9, 64), )
  ttnn_from_device_27 = ttnn.from_device(ttnn_reshape_16, )
  ttnn_to_layout_30 = ttnn.to_layout(ttnn_from_device_27, ttnn.TILE_LAYOUT, )
  ttnn_to_device_12 = ttnn.to_device(ttnn_to_layout_30, device = device)
  ttnn_from_device_28 = ttnn.from_device(ttnn_reshape_17, )
  ttnn_to_layout_31 = ttnn.to_layout(ttnn_from_device_28, ttnn.TILE_LAYOUT, )
  ttnn_to_device_13 = ttnn.to_device(ttnn_to_layout_31, device = device)
  ttnn_matmul_5 = ttnn.matmul(ttnn_to_device_12, ttnn_to_device_13, )
  ttnn_from_device_29 = ttnn.from_device(ttnn_matmul_5, )
  ttnn_to_layout_32 = ttnn.to_layout(ttnn_from_device_29, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_18 = ttnn.reshape(ttnn_to_layout_32, (1, 16, 9, 64), )
  ttnn_from_device_30 = ttnn.from_device(ttnn_reshape_18, )
  ttnn_to_layout_33 = ttnn.to_layout(ttnn_from_device_30, ttnn.TILE_LAYOUT, )
  ttnn_to_device_14 = ttnn.to_device(ttnn_to_layout_33, device = device)
  ttnn_transpose_5 = ttnn.transpose(ttnn_to_device_14, 2, 1, )
  ttnn_prefix_clone_2 = clone_wrapper(ttnn_transpose_5, )
  ttnn_from_device_31 = ttnn.from_device(ttnn_prefix_clone_2, )
  ttnn_to_layout_34 = ttnn.to_layout(ttnn_from_device_31, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_19 = ttnn.reshape(ttnn_to_layout_34, (1, 9, 1024), )
  ttnn_from_device_32 = ttnn.from_device(ttnn_reshape_19, )
  ttnn_to_layout_35 = ttnn.to_layout(ttnn_from_device_32, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_20 = ttnn.reshape(ttnn_to_layout_35, (9, 1024), )
  ttnn_from_torch_17 = ttnn.from_torch(arg13_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_6 = ttnn.transpose(ttnn_from_torch_17, 0, 1, )
  test_accuracy(t_142, ttnn_transpose_6)
  ttnn_from_device_33 = ttnn.from_device(ttnn_reshape_20, )
  ttnn_to_layout_36 = ttnn.to_layout(ttnn_from_device_33, ttnn.TILE_LAYOUT, )
  ttnn_to_device_15 = ttnn.to_device(ttnn_to_layout_36, device = device)
  ttnn_matmul_6 = ttnn.matmul(ttnn_to_device_15, ttnn_transpose_6, )
  ttnn_from_torch_18 = ttnn.from_torch(arg14_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_4 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_6, )
  ttnn_from_device_34 = ttnn.from_device(ttnn_add_4, )
  ttnn_to_layout_37 = ttnn.to_layout(ttnn_from_device_34, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_21 = ttnn.reshape(ttnn_to_layout_37, (1, 9, 1024), )
  ttnn_from_device_35 = ttnn.from_device(ttnn_reshape_21, )
  ttnn_to_layout_38 = ttnn.to_layout(ttnn_from_device_35, ttnn.TILE_LAYOUT, )
  ttnn_to_device_16 = ttnn.to_device(ttnn_to_layout_38, device = device)
  ttnn_prefix_clone_3 = clone_wrapper(ttnn_to_device_16, )
  ttnn_to_layout_39 = ttnn.to_layout(ttnn_from_device_6, ttnn.TILE_LAYOUT, )
  ttnn_to_device_17 = ttnn.to_device(ttnn_to_layout_39, device = device)
  ttnn_add_150 = ttnn.add(ttnn_to_device_17, ttnn_prefix_clone_3, )
  ttnn_from_torch_19 = ttnn.from_torch(arg15_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_20 = ttnn.from_torch(arg16_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_1_ = ttnn.layer_norm(ttnn_add_150, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_1_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_150), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_1 = ttnn.from_torch(ttnn_layer_norm_1_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_1_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_1_), ttnn_layer_norm_1))
  ttnn_from_device_37 = ttnn.from_device(ttnn_layer_norm_1, )
  ttnn_to_layout_40 = ttnn.to_layout(ttnn_from_device_37, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_22 = ttnn.reshape(ttnn_to_layout_40, (9, 1024), )
  ttnn_from_torch_21 = ttnn.from_torch(arg17_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_7 = ttnn.transpose(ttnn_from_torch_21, 0, 1, )
  test_accuracy(t_143, ttnn_transpose_7)
  ttnn_from_device_38 = ttnn.from_device(ttnn_reshape_22, )
  ttnn_to_layout_41 = ttnn.to_layout(ttnn_from_device_38, ttnn.TILE_LAYOUT, )
  ttnn_to_device_18 = ttnn.to_device(ttnn_to_layout_41, device = device)
  ttnn_matmul_7 = ttnn.matmul(ttnn_to_device_18, ttnn_transpose_7, )
  ttnn_from_torch_22 = ttnn.from_torch(arg18_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_5 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_7, )
  ttnn_from_device_39 = ttnn.from_device(ttnn_add_5, )
  ttnn_to_layout_42 = ttnn.to_layout(ttnn_from_device_39, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_23 = ttnn.reshape(ttnn_to_layout_42, (1, 9, 4096), )
  ttnn_from_device_40 = ttnn.from_device(ttnn_reshape_23, )
  ttnn_to_layout_43 = ttnn.to_layout(ttnn_from_device_40, ttnn.TILE_LAYOUT, )
  ttnn_to_device_19 = ttnn.to_device(ttnn_to_layout_43, device = device)
  ttnn_multiply_2 = ttnn.multiply(ttnn_to_device_19, 0.5, )
  ttnn_pow = ttnn.pow(ttnn_to_device_19, 3.0, )
  ttnn_multiply_3 = ttnn.multiply(ttnn_pow, 0.044715, )
  ttnn_add_151 = ttnn.add(ttnn_to_device_19, ttnn_multiply_3, )
  ttnn_multiply_4 = ttnn.multiply(ttnn_add_151, 0.7978845608028654, )
  ttnn_tanh_ = ttnn.tanh(ttnn_multiply_4, )
  ttnn_tanh_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_4), )
  ttnn_tanh = ttnn.from_torch(ttnn_tanh_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_), ttnn_tanh))
  test_accuracy(tanh, ttnn_tanh)
  ttnn_add_152 = ttnn.add(ttnn_tanh, 1.0, )
  ttnn_multiply_5 = ttnn.multiply(ttnn_multiply_2, ttnn_add_152, )
  ttnn_from_device_41 = ttnn.from_device(ttnn_multiply_5, )
  ttnn_to_layout_44 = ttnn.to_layout(ttnn_from_device_41, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_24 = ttnn.reshape(ttnn_to_layout_44, (9, 4096), )
  ttnn_from_torch_23 = ttnn.from_torch(arg19_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_8 = ttnn.transpose(ttnn_from_torch_23, 0, 1, )
  test_accuracy(t_144, ttnn_transpose_8)
  ttnn_from_device_42 = ttnn.from_device(ttnn_reshape_24, )
  ttnn_to_layout_45 = ttnn.to_layout(ttnn_from_device_42, ttnn.TILE_LAYOUT, )
  ttnn_to_device_20 = ttnn.to_device(ttnn_to_layout_45, device = device)
  ttnn_matmul_8 = ttnn.matmul(ttnn_to_device_20, ttnn_transpose_8, )
  ttnn_from_torch_24 = ttnn.from_torch(arg20_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_6 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_8, )
  ttnn_from_device_43 = ttnn.from_device(ttnn_add_6, )
  ttnn_to_layout_46 = ttnn.to_layout(ttnn_from_device_43, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_25 = ttnn.reshape(ttnn_to_layout_46, (1, 9, 1024), )
  ttnn_from_device_44 = ttnn.from_device(ttnn_reshape_25, )
  ttnn_to_layout_47 = ttnn.to_layout(ttnn_from_device_44, ttnn.TILE_LAYOUT, )
  ttnn_to_device_21 = ttnn.to_device(ttnn_to_layout_47, device = device)
  ttnn_add_153 = ttnn.add(ttnn_to_device_21, ttnn_layer_norm_1, )
  ttnn_from_torch_25 = ttnn.from_torch(arg21_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_26 = ttnn.from_torch(arg22_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_2_ = ttnn.layer_norm(ttnn_add_153, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_2_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_153), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_2 = ttnn.from_torch(ttnn_layer_norm_2_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_2_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_2_), ttnn_layer_norm_2))
  ttnn_from_device_45 = ttnn.from_device(ttnn_layer_norm_2, )
  ttnn_to_layout_48 = ttnn.to_layout(ttnn_from_device_45, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_26 = ttnn.reshape(ttnn_to_layout_48, (9, 1024), )
  ttnn_from_device_46 = ttnn.from_device(ttnn_reshape_26, )
  ttnn_to_layout_49 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_22 = ttnn.to_device(ttnn_to_layout_49, device = device)
  ttnn_matmul_9 = ttnn.matmul(ttnn_to_device_22, ttnn_transpose_1, )
  ttnn_add_7 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_9, )
  ttnn_from_device_47 = ttnn.from_device(ttnn_add_7, )
  ttnn_to_layout_50 = ttnn.to_layout(ttnn_from_device_47, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_27 = ttnn.reshape(ttnn_to_layout_50, (1, 9, 1024), )
  ttnn_to_layout_51 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_23 = ttnn.to_device(ttnn_to_layout_51, device = device)
  ttnn_matmul_10 = ttnn.matmul(ttnn_to_device_23, ttnn_transpose_2, )
  ttnn_add_8 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_10, )
  ttnn_from_device_49 = ttnn.from_device(ttnn_add_8, )
  ttnn_to_layout_52 = ttnn.to_layout(ttnn_from_device_49, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_29 = ttnn.reshape(ttnn_to_layout_52, (1, 9, 1024), )
  ttnn_to_layout_53 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_24 = ttnn.to_device(ttnn_to_layout_53, device = device)
  ttnn_matmul_11 = ttnn.matmul(ttnn_to_device_24, ttnn_transpose_3, )
  ttnn_add_9 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_11, )
  ttnn_from_device_51 = ttnn.from_device(ttnn_add_9, )
  ttnn_to_layout_54 = ttnn.to_layout(ttnn_from_device_51, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_31 = ttnn.reshape(ttnn_to_layout_54, (1, 9, 1024), )
  ttnn_from_device_52 = ttnn.from_device(ttnn_reshape_27, )
  ttnn_to_layout_55 = ttnn.to_layout(ttnn_from_device_52, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_32 = ttnn.reshape(ttnn_to_layout_55, (1, 9, 16, 64), )
  ttnn_from_device_53 = ttnn.from_device(ttnn_reshape_32, )
  ttnn_to_layout_56 = ttnn.to_layout(ttnn_from_device_53, ttnn.TILE_LAYOUT, )
  ttnn_to_device_25 = ttnn.to_device(ttnn_to_layout_56, device = device)
  ttnn_permute_3 = ttnn.permute(ttnn_to_device_25, (0, 2, 1, 3), )
  ttnn_from_device_54 = ttnn.from_device(ttnn_reshape_29, )
  ttnn_to_layout_57 = ttnn.to_layout(ttnn_from_device_54, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_33 = ttnn.reshape(ttnn_to_layout_57, (1, 9, 16, 64), )
  ttnn_from_device_55 = ttnn.from_device(ttnn_reshape_33, )
  ttnn_to_layout_58 = ttnn.to_layout(ttnn_from_device_55, ttnn.TILE_LAYOUT, )
  ttnn_to_device_26 = ttnn.to_device(ttnn_to_layout_58, device = device)
  ttnn_permute_4 = ttnn.permute(ttnn_to_device_26, (0, 2, 1, 3), )
  ttnn_from_device_56 = ttnn.from_device(ttnn_reshape_31, )
  ttnn_to_layout_59 = ttnn.to_layout(ttnn_from_device_56, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_34 = ttnn.reshape(ttnn_to_layout_59, (1, 9, 16, 64), )
  ttnn_from_device_57 = ttnn.from_device(ttnn_reshape_34, )
  ttnn_to_layout_60 = ttnn.to_layout(ttnn_from_device_57, ttnn.TILE_LAYOUT, )
  ttnn_to_device_27 = ttnn.to_device(ttnn_to_layout_60, device = device)
  ttnn_permute_5 = ttnn.permute(ttnn_to_device_27, (0, 2, 1, 3), )
  ttnn_transpose_12 = ttnn.transpose(ttnn_permute_4, 3, 2, )
  ttnn_from_device_58 = ttnn.from_device(ttnn_permute_3, )
  ttnn_to_layout_61 = ttnn.to_layout(ttnn_from_device_58, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_35 = ttnn.reshape(ttnn_to_layout_61, (16, 9, 64), )
  ttnn_from_device_59 = ttnn.from_device(ttnn_transpose_12, )
  ttnn_to_layout_62 = ttnn.to_layout(ttnn_from_device_59, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_36 = ttnn.reshape(ttnn_to_layout_62, (16, 64, 9), )
  ttnn_from_device_60 = ttnn.from_device(ttnn_reshape_35, )
  ttnn_to_layout_63 = ttnn.to_layout(ttnn_from_device_60, ttnn.TILE_LAYOUT, )
  ttnn_to_device_28 = ttnn.to_device(ttnn_to_layout_63, device = device)
  ttnn_from_device_61 = ttnn.from_device(ttnn_reshape_36, )
  ttnn_to_layout_64 = ttnn.to_layout(ttnn_from_device_61, ttnn.TILE_LAYOUT, )
  ttnn_to_device_29 = ttnn.to_device(ttnn_to_layout_64, device = device)
  ttnn_matmul_12 = ttnn.matmul(ttnn_to_device_28, ttnn_to_device_29, )
  ttnn_from_device_62 = ttnn.from_device(ttnn_matmul_12, )
  ttnn_to_layout_65 = ttnn.to_layout(ttnn_from_device_62, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_37 = ttnn.reshape(ttnn_to_layout_65, (1, 16, 9, 9), )
  ttnn_from_device_63 = ttnn.from_device(ttnn_reshape_37, )
  ttnn_to_layout_66 = ttnn.to_layout(ttnn_from_device_63, ttnn.TILE_LAYOUT, )
  ttnn_to_device_30 = ttnn.to_device(ttnn_to_layout_66, device = device)
  ttnn_multiply_6 = ttnn.multiply(ttnn_to_device_30, 0.125, )
  ttnn_add_154 = ttnn.add(ttnn_multiply_6, ttnn_multiply, )
  ttnn_softmax_1 = ttnn.softmax(ttnn_add_154, -1, numeric_stable = True)
  test_accuracy(_softmax_1, ttnn_softmax_1)
  ttnn_prefix_clone_4 = clone_wrapper(ttnn_softmax_1, )
  ttnn_from_device_64 = ttnn.from_device(ttnn_prefix_clone_4, )
  ttnn_to_layout_67 = ttnn.to_layout(ttnn_from_device_64, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_38 = ttnn.reshape(ttnn_to_layout_67, (16, 9, 9), )
  ttnn_from_device_65 = ttnn.from_device(ttnn_permute_5, )
  ttnn_to_layout_68 = ttnn.to_layout(ttnn_from_device_65, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_39 = ttnn.reshape(ttnn_to_layout_68, (16, 9, 64), )
  ttnn_from_device_66 = ttnn.from_device(ttnn_reshape_38, )
  ttnn_to_layout_69 = ttnn.to_layout(ttnn_from_device_66, ttnn.TILE_LAYOUT, )
  ttnn_to_device_31 = ttnn.to_device(ttnn_to_layout_69, device = device)
  ttnn_from_device_67 = ttnn.from_device(ttnn_reshape_39, )
  ttnn_to_layout_70 = ttnn.to_layout(ttnn_from_device_67, ttnn.TILE_LAYOUT, )
  ttnn_to_device_32 = ttnn.to_device(ttnn_to_layout_70, device = device)
  ttnn_matmul_13 = ttnn.matmul(ttnn_to_device_31, ttnn_to_device_32, )
  ttnn_from_device_68 = ttnn.from_device(ttnn_matmul_13, )
  ttnn_to_layout_71 = ttnn.to_layout(ttnn_from_device_68, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_40 = ttnn.reshape(ttnn_to_layout_71, (1, 16, 9, 64), )
  ttnn_from_device_69 = ttnn.from_device(ttnn_reshape_40, )
  ttnn_to_layout_72 = ttnn.to_layout(ttnn_from_device_69, ttnn.TILE_LAYOUT, )
  ttnn_to_device_33 = ttnn.to_device(ttnn_to_layout_72, device = device)
  ttnn_transpose_13 = ttnn.transpose(ttnn_to_device_33, 2, 1, )
  ttnn_prefix_clone_5 = clone_wrapper(ttnn_transpose_13, )
  ttnn_from_device_70 = ttnn.from_device(ttnn_prefix_clone_5, )
  ttnn_to_layout_73 = ttnn.to_layout(ttnn_from_device_70, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_41 = ttnn.reshape(ttnn_to_layout_73, (1, 9, 1024), )
  ttnn_from_device_71 = ttnn.from_device(ttnn_reshape_41, )
  ttnn_to_layout_74 = ttnn.to_layout(ttnn_from_device_71, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_42 = ttnn.reshape(ttnn_to_layout_74, (9, 1024), )
  ttnn_from_device_72 = ttnn.from_device(ttnn_reshape_42, )
  ttnn_to_layout_75 = ttnn.to_layout(ttnn_from_device_72, ttnn.TILE_LAYOUT, )
  ttnn_to_device_34 = ttnn.to_device(ttnn_to_layout_75, device = device)
  ttnn_matmul_14 = ttnn.matmul(ttnn_to_device_34, ttnn_transpose_6, )
  ttnn_add_10 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_14, )
  ttnn_from_device_73 = ttnn.from_device(ttnn_add_10, )
  ttnn_to_layout_76 = ttnn.to_layout(ttnn_from_device_73, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_43 = ttnn.reshape(ttnn_to_layout_76, (1, 9, 1024), )
  ttnn_from_device_74 = ttnn.from_device(ttnn_reshape_43, )
  ttnn_to_layout_77 = ttnn.to_layout(ttnn_from_device_74, ttnn.TILE_LAYOUT, )
  ttnn_to_device_35 = ttnn.to_device(ttnn_to_layout_77, device = device)
  ttnn_prefix_clone_6 = clone_wrapper(ttnn_to_device_35, )
  ttnn_add_155 = ttnn.add(ttnn_layer_norm_2, ttnn_prefix_clone_6, )
  ttnn_layer_norm_3_ = ttnn.layer_norm(ttnn_add_155, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_3_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_155), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_3 = ttnn.from_torch(ttnn_layer_norm_3_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_3_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_3_), ttnn_layer_norm_3))
  ttnn_from_device_75 = ttnn.from_device(ttnn_layer_norm_3, )
  ttnn_to_layout_78 = ttnn.to_layout(ttnn_from_device_75, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_44 = ttnn.reshape(ttnn_to_layout_78, (9, 1024), )
  ttnn_from_device_76 = ttnn.from_device(ttnn_reshape_44, )
  ttnn_to_layout_79 = ttnn.to_layout(ttnn_from_device_76, ttnn.TILE_LAYOUT, )
  ttnn_to_device_36 = ttnn.to_device(ttnn_to_layout_79, device = device)
  ttnn_matmul_15 = ttnn.matmul(ttnn_to_device_36, ttnn_transpose_7, )
  ttnn_add_11 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_15, )
  ttnn_from_device_77 = ttnn.from_device(ttnn_add_11, )
  ttnn_to_layout_80 = ttnn.to_layout(ttnn_from_device_77, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_45 = ttnn.reshape(ttnn_to_layout_80, (1, 9, 4096), )
  ttnn_from_device_78 = ttnn.from_device(ttnn_reshape_45, )
  ttnn_to_layout_81 = ttnn.to_layout(ttnn_from_device_78, ttnn.TILE_LAYOUT, )
  ttnn_to_device_37 = ttnn.to_device(ttnn_to_layout_81, device = device)
  ttnn_multiply_7 = ttnn.multiply(ttnn_to_device_37, 0.5, )
  ttnn_pow_1 = ttnn.pow(ttnn_to_device_37, 3.0, )
  ttnn_multiply_8 = ttnn.multiply(ttnn_pow_1, 0.044715, )
  ttnn_add_156 = ttnn.add(ttnn_to_device_37, ttnn_multiply_8, )
  ttnn_multiply_9 = ttnn.multiply(ttnn_add_156, 0.7978845608028654, )
  ttnn_tanh_1_ = ttnn.tanh(ttnn_multiply_9, )
  ttnn_tanh_1_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_9), )
  ttnn_tanh_1 = ttnn.from_torch(ttnn_tanh_1_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_1_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_1_), ttnn_tanh_1))
  test_accuracy(tanh_1, ttnn_tanh_1)
  ttnn_add_157 = ttnn.add(ttnn_tanh_1, 1.0, )
  ttnn_multiply_10 = ttnn.multiply(ttnn_multiply_7, ttnn_add_157, )
  ttnn_from_device_79 = ttnn.from_device(ttnn_multiply_10, )
  ttnn_to_layout_82 = ttnn.to_layout(ttnn_from_device_79, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_46 = ttnn.reshape(ttnn_to_layout_82, (9, 4096), )
  ttnn_from_device_80 = ttnn.from_device(ttnn_reshape_46, )
  ttnn_to_layout_83 = ttnn.to_layout(ttnn_from_device_80, ttnn.TILE_LAYOUT, )
  ttnn_to_device_38 = ttnn.to_device(ttnn_to_layout_83, device = device)
  ttnn_matmul_16 = ttnn.matmul(ttnn_to_device_38, ttnn_transpose_8, )
  ttnn_add_12 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_16, )
  ttnn_from_device_81 = ttnn.from_device(ttnn_add_12, )
  ttnn_to_layout_84 = ttnn.to_layout(ttnn_from_device_81, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_47 = ttnn.reshape(ttnn_to_layout_84, (1, 9, 1024), )
  ttnn_from_device_82 = ttnn.from_device(ttnn_reshape_47, )
  ttnn_to_layout_85 = ttnn.to_layout(ttnn_from_device_82, ttnn.TILE_LAYOUT, )
  ttnn_to_device_39 = ttnn.to_device(ttnn_to_layout_85, device = device)
  ttnn_add_158 = ttnn.add(ttnn_to_device_39, ttnn_layer_norm_3, )
  ttnn_layer_norm_4_ = ttnn.layer_norm(ttnn_add_158, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_4_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_158), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_4 = ttnn.from_torch(ttnn_layer_norm_4_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_4_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_4_), ttnn_layer_norm_4))
  ttnn_from_device_83 = ttnn.from_device(ttnn_layer_norm_4, )
  ttnn_to_layout_86 = ttnn.to_layout(ttnn_from_device_83, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_48 = ttnn.reshape(ttnn_to_layout_86, (9, 1024), )
  ttnn_from_device_84 = ttnn.from_device(ttnn_reshape_48, )
  ttnn_to_layout_87 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_40 = ttnn.to_device(ttnn_to_layout_87, device = device)
  ttnn_matmul_17 = ttnn.matmul(ttnn_to_device_40, ttnn_transpose_1, )
  ttnn_add_13 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_17, )
  ttnn_from_device_85 = ttnn.from_device(ttnn_add_13, )
  ttnn_to_layout_88 = ttnn.to_layout(ttnn_from_device_85, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_49 = ttnn.reshape(ttnn_to_layout_88, (1, 9, 1024), )
  ttnn_to_layout_89 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_41 = ttnn.to_device(ttnn_to_layout_89, device = device)
  ttnn_matmul_18 = ttnn.matmul(ttnn_to_device_41, ttnn_transpose_2, )
  ttnn_add_14 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_18, )
  ttnn_from_device_87 = ttnn.from_device(ttnn_add_14, )
  ttnn_to_layout_90 = ttnn.to_layout(ttnn_from_device_87, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_51 = ttnn.reshape(ttnn_to_layout_90, (1, 9, 1024), )
  ttnn_to_layout_91 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_42 = ttnn.to_device(ttnn_to_layout_91, device = device)
  ttnn_matmul_19 = ttnn.matmul(ttnn_to_device_42, ttnn_transpose_3, )
  ttnn_add_15 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_19, )
  ttnn_from_device_89 = ttnn.from_device(ttnn_add_15, )
  ttnn_to_layout_92 = ttnn.to_layout(ttnn_from_device_89, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_53 = ttnn.reshape(ttnn_to_layout_92, (1, 9, 1024), )
  ttnn_from_device_90 = ttnn.from_device(ttnn_reshape_49, )
  ttnn_to_layout_93 = ttnn.to_layout(ttnn_from_device_90, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_54 = ttnn.reshape(ttnn_to_layout_93, (1, 9, 16, 64), )
  ttnn_from_device_91 = ttnn.from_device(ttnn_reshape_54, )
  ttnn_to_layout_94 = ttnn.to_layout(ttnn_from_device_91, ttnn.TILE_LAYOUT, )
  ttnn_to_device_43 = ttnn.to_device(ttnn_to_layout_94, device = device)
  ttnn_permute_6 = ttnn.permute(ttnn_to_device_43, (0, 2, 1, 3), )
  ttnn_from_device_92 = ttnn.from_device(ttnn_reshape_51, )
  ttnn_to_layout_95 = ttnn.to_layout(ttnn_from_device_92, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_55 = ttnn.reshape(ttnn_to_layout_95, (1, 9, 16, 64), )
  ttnn_from_device_93 = ttnn.from_device(ttnn_reshape_55, )
  ttnn_to_layout_96 = ttnn.to_layout(ttnn_from_device_93, ttnn.TILE_LAYOUT, )
  ttnn_to_device_44 = ttnn.to_device(ttnn_to_layout_96, device = device)
  ttnn_permute_7 = ttnn.permute(ttnn_to_device_44, (0, 2, 1, 3), )
  ttnn_from_device_94 = ttnn.from_device(ttnn_reshape_53, )
  ttnn_to_layout_97 = ttnn.to_layout(ttnn_from_device_94, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_56 = ttnn.reshape(ttnn_to_layout_97, (1, 9, 16, 64), )
  ttnn_from_device_95 = ttnn.from_device(ttnn_reshape_56, )
  ttnn_to_layout_98 = ttnn.to_layout(ttnn_from_device_95, ttnn.TILE_LAYOUT, )
  ttnn_to_device_45 = ttnn.to_device(ttnn_to_layout_98, device = device)
  ttnn_permute_8 = ttnn.permute(ttnn_to_device_45, (0, 2, 1, 3), )
  ttnn_transpose_20 = ttnn.transpose(ttnn_permute_7, 3, 2, )
  ttnn_from_device_96 = ttnn.from_device(ttnn_permute_6, )
  ttnn_to_layout_99 = ttnn.to_layout(ttnn_from_device_96, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_57 = ttnn.reshape(ttnn_to_layout_99, (16, 9, 64), )
  ttnn_from_device_97 = ttnn.from_device(ttnn_transpose_20, )
  ttnn_to_layout_100 = ttnn.to_layout(ttnn_from_device_97, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_58 = ttnn.reshape(ttnn_to_layout_100, (16, 64, 9), )
  ttnn_from_device_98 = ttnn.from_device(ttnn_reshape_57, )
  ttnn_to_layout_101 = ttnn.to_layout(ttnn_from_device_98, ttnn.TILE_LAYOUT, )
  ttnn_to_device_46 = ttnn.to_device(ttnn_to_layout_101, device = device)
  ttnn_from_device_99 = ttnn.from_device(ttnn_reshape_58, )
  ttnn_to_layout_102 = ttnn.to_layout(ttnn_from_device_99, ttnn.TILE_LAYOUT, )
  ttnn_to_device_47 = ttnn.to_device(ttnn_to_layout_102, device = device)
  ttnn_matmul_20 = ttnn.matmul(ttnn_to_device_46, ttnn_to_device_47, )
  ttnn_from_device_100 = ttnn.from_device(ttnn_matmul_20, )
  ttnn_to_layout_103 = ttnn.to_layout(ttnn_from_device_100, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_59 = ttnn.reshape(ttnn_to_layout_103, (1, 16, 9, 9), )
  ttnn_from_device_101 = ttnn.from_device(ttnn_reshape_59, )
  ttnn_to_layout_104 = ttnn.to_layout(ttnn_from_device_101, ttnn.TILE_LAYOUT, )
  ttnn_to_device_48 = ttnn.to_device(ttnn_to_layout_104, device = device)
  ttnn_multiply_11 = ttnn.multiply(ttnn_to_device_48, 0.125, )
  ttnn_add_159 = ttnn.add(ttnn_multiply_11, ttnn_multiply, )
  ttnn_softmax_2 = ttnn.softmax(ttnn_add_159, -1, numeric_stable = True)
  test_accuracy(_softmax_2, ttnn_softmax_2)
  ttnn_prefix_clone_7 = clone_wrapper(ttnn_softmax_2, )
  ttnn_from_device_102 = ttnn.from_device(ttnn_prefix_clone_7, )
  ttnn_to_layout_105 = ttnn.to_layout(ttnn_from_device_102, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_60 = ttnn.reshape(ttnn_to_layout_105, (16, 9, 9), )
  ttnn_from_device_103 = ttnn.from_device(ttnn_permute_8, )
  ttnn_to_layout_106 = ttnn.to_layout(ttnn_from_device_103, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_61 = ttnn.reshape(ttnn_to_layout_106, (16, 9, 64), )
  ttnn_from_device_104 = ttnn.from_device(ttnn_reshape_60, )
  ttnn_to_layout_107 = ttnn.to_layout(ttnn_from_device_104, ttnn.TILE_LAYOUT, )
  ttnn_to_device_49 = ttnn.to_device(ttnn_to_layout_107, device = device)
  ttnn_from_device_105 = ttnn.from_device(ttnn_reshape_61, )
  ttnn_to_layout_108 = ttnn.to_layout(ttnn_from_device_105, ttnn.TILE_LAYOUT, )
  ttnn_to_device_50 = ttnn.to_device(ttnn_to_layout_108, device = device)
  ttnn_matmul_21 = ttnn.matmul(ttnn_to_device_49, ttnn_to_device_50, )
  ttnn_from_device_106 = ttnn.from_device(ttnn_matmul_21, )
  ttnn_to_layout_109 = ttnn.to_layout(ttnn_from_device_106, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_62 = ttnn.reshape(ttnn_to_layout_109, (1, 16, 9, 64), )
  ttnn_from_device_107 = ttnn.from_device(ttnn_reshape_62, )
  ttnn_to_layout_110 = ttnn.to_layout(ttnn_from_device_107, ttnn.TILE_LAYOUT, )
  ttnn_to_device_51 = ttnn.to_device(ttnn_to_layout_110, device = device)
  ttnn_transpose_21 = ttnn.transpose(ttnn_to_device_51, 2, 1, )
  ttnn_prefix_clone_8 = clone_wrapper(ttnn_transpose_21, )
  ttnn_from_device_108 = ttnn.from_device(ttnn_prefix_clone_8, )
  ttnn_to_layout_111 = ttnn.to_layout(ttnn_from_device_108, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_63 = ttnn.reshape(ttnn_to_layout_111, (1, 9, 1024), )
  ttnn_from_device_109 = ttnn.from_device(ttnn_reshape_63, )
  ttnn_to_layout_112 = ttnn.to_layout(ttnn_from_device_109, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_64 = ttnn.reshape(ttnn_to_layout_112, (9, 1024), )
  ttnn_from_device_110 = ttnn.from_device(ttnn_reshape_64, )
  ttnn_to_layout_113 = ttnn.to_layout(ttnn_from_device_110, ttnn.TILE_LAYOUT, )
  ttnn_to_device_52 = ttnn.to_device(ttnn_to_layout_113, device = device)
  ttnn_matmul_22 = ttnn.matmul(ttnn_to_device_52, ttnn_transpose_6, )
  ttnn_add_16 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_22, )
  ttnn_from_device_111 = ttnn.from_device(ttnn_add_16, )
  ttnn_to_layout_114 = ttnn.to_layout(ttnn_from_device_111, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_65 = ttnn.reshape(ttnn_to_layout_114, (1, 9, 1024), )
  ttnn_from_device_112 = ttnn.from_device(ttnn_reshape_65, )
  ttnn_to_layout_115 = ttnn.to_layout(ttnn_from_device_112, ttnn.TILE_LAYOUT, )
  ttnn_to_device_53 = ttnn.to_device(ttnn_to_layout_115, device = device)
  ttnn_prefix_clone_9 = clone_wrapper(ttnn_to_device_53, )
  ttnn_add_160 = ttnn.add(ttnn_layer_norm_4, ttnn_prefix_clone_9, )
  ttnn_layer_norm_5_ = ttnn.layer_norm(ttnn_add_160, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_5_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_160), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_5 = ttnn.from_torch(ttnn_layer_norm_5_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_5_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_5_), ttnn_layer_norm_5))
  ttnn_from_device_113 = ttnn.from_device(ttnn_layer_norm_5, )
  ttnn_to_layout_116 = ttnn.to_layout(ttnn_from_device_113, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_66 = ttnn.reshape(ttnn_to_layout_116, (9, 1024), )
  ttnn_from_device_114 = ttnn.from_device(ttnn_reshape_66, )
  ttnn_to_layout_117 = ttnn.to_layout(ttnn_from_device_114, ttnn.TILE_LAYOUT, )
  ttnn_to_device_54 = ttnn.to_device(ttnn_to_layout_117, device = device)
  ttnn_matmul_23 = ttnn.matmul(ttnn_to_device_54, ttnn_transpose_7, )
  ttnn_add_17 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_23, )
  ttnn_from_device_115 = ttnn.from_device(ttnn_add_17, )
  ttnn_to_layout_118 = ttnn.to_layout(ttnn_from_device_115, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_67 = ttnn.reshape(ttnn_to_layout_118, (1, 9, 4096), )
  ttnn_from_device_116 = ttnn.from_device(ttnn_reshape_67, )
  ttnn_to_layout_119 = ttnn.to_layout(ttnn_from_device_116, ttnn.TILE_LAYOUT, )
  ttnn_to_device_55 = ttnn.to_device(ttnn_to_layout_119, device = device)
  ttnn_multiply_12 = ttnn.multiply(ttnn_to_device_55, 0.5, )
  ttnn_pow_2 = ttnn.pow(ttnn_to_device_55, 3.0, )
  ttnn_multiply_13 = ttnn.multiply(ttnn_pow_2, 0.044715, )
  ttnn_add_161 = ttnn.add(ttnn_to_device_55, ttnn_multiply_13, )
  ttnn_multiply_14 = ttnn.multiply(ttnn_add_161, 0.7978845608028654, )
  ttnn_tanh_2_ = ttnn.tanh(ttnn_multiply_14, )
  ttnn_tanh_2_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_14), )
  ttnn_tanh_2 = ttnn.from_torch(ttnn_tanh_2_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_2_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_2_), ttnn_tanh_2))
  test_accuracy(tanh_2, ttnn_tanh_2)
  ttnn_add_162 = ttnn.add(ttnn_tanh_2, 1.0, )
  ttnn_multiply_15 = ttnn.multiply(ttnn_multiply_12, ttnn_add_162, )
  ttnn_from_device_117 = ttnn.from_device(ttnn_multiply_15, )
  ttnn_to_layout_120 = ttnn.to_layout(ttnn_from_device_117, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_68 = ttnn.reshape(ttnn_to_layout_120, (9, 4096), )
  ttnn_from_device_118 = ttnn.from_device(ttnn_reshape_68, )
  ttnn_to_layout_121 = ttnn.to_layout(ttnn_from_device_118, ttnn.TILE_LAYOUT, )
  ttnn_to_device_56 = ttnn.to_device(ttnn_to_layout_121, device = device)
  ttnn_matmul_24 = ttnn.matmul(ttnn_to_device_56, ttnn_transpose_8, )
  ttnn_add_18 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_24, )
  ttnn_from_device_119 = ttnn.from_device(ttnn_add_18, )
  ttnn_to_layout_122 = ttnn.to_layout(ttnn_from_device_119, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_69 = ttnn.reshape(ttnn_to_layout_122, (1, 9, 1024), )
  ttnn_from_device_120 = ttnn.from_device(ttnn_reshape_69, )
  ttnn_to_layout_123 = ttnn.to_layout(ttnn_from_device_120, ttnn.TILE_LAYOUT, )
  ttnn_to_device_57 = ttnn.to_device(ttnn_to_layout_123, device = device)
  ttnn_add_163 = ttnn.add(ttnn_to_device_57, ttnn_layer_norm_5, )
  ttnn_layer_norm_6_ = ttnn.layer_norm(ttnn_add_163, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_6_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_163), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_6 = ttnn.from_torch(ttnn_layer_norm_6_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_6_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_6_), ttnn_layer_norm_6))
  ttnn_from_device_121 = ttnn.from_device(ttnn_layer_norm_6, )
  ttnn_to_layout_124 = ttnn.to_layout(ttnn_from_device_121, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_70 = ttnn.reshape(ttnn_to_layout_124, (9, 1024), )
  ttnn_from_device_122 = ttnn.from_device(ttnn_reshape_70, )
  ttnn_to_layout_125 = ttnn.to_layout(ttnn_from_device_122, ttnn.TILE_LAYOUT, )
  ttnn_to_device_58 = ttnn.to_device(ttnn_to_layout_125, device = device)
  ttnn_matmul_25 = ttnn.matmul(ttnn_to_device_58, ttnn_transpose_1, )
  ttnn_add_19 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_25, )
  ttnn_from_device_123 = ttnn.from_device(ttnn_add_19, )
  ttnn_to_layout_126 = ttnn.to_layout(ttnn_from_device_123, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_71 = ttnn.reshape(ttnn_to_layout_126, (1, 9, 1024), )
  ttnn_to_layout_127 = ttnn.to_layout(ttnn_from_device_122, ttnn.TILE_LAYOUT, )
  ttnn_to_device_59 = ttnn.to_device(ttnn_to_layout_127, device = device)
  ttnn_matmul_26 = ttnn.matmul(ttnn_to_device_59, ttnn_transpose_2, )
  ttnn_add_20 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_26, )
  ttnn_from_device_125 = ttnn.from_device(ttnn_add_20, )
  ttnn_to_layout_128 = ttnn.to_layout(ttnn_from_device_125, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_73 = ttnn.reshape(ttnn_to_layout_128, (1, 9, 1024), )
  ttnn_to_layout_129 = ttnn.to_layout(ttnn_from_device_122, ttnn.TILE_LAYOUT, )
  ttnn_to_device_60 = ttnn.to_device(ttnn_to_layout_129, device = device)
  ttnn_matmul_27 = ttnn.matmul(ttnn_to_device_60, ttnn_transpose_3, )
  ttnn_add_21 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_27, )
  ttnn_from_device_127 = ttnn.from_device(ttnn_add_21, )
  ttnn_to_layout_130 = ttnn.to_layout(ttnn_from_device_127, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_75 = ttnn.reshape(ttnn_to_layout_130, (1, 9, 1024), )
  ttnn_from_device_128 = ttnn.from_device(ttnn_reshape_71, )
  ttnn_to_layout_131 = ttnn.to_layout(ttnn_from_device_128, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_76 = ttnn.reshape(ttnn_to_layout_131, (1, 9, 16, 64), )
  ttnn_from_device_129 = ttnn.from_device(ttnn_reshape_76, )
  ttnn_to_layout_132 = ttnn.to_layout(ttnn_from_device_129, ttnn.TILE_LAYOUT, )
  ttnn_to_device_61 = ttnn.to_device(ttnn_to_layout_132, device = device)
  ttnn_permute_9 = ttnn.permute(ttnn_to_device_61, (0, 2, 1, 3), )
  ttnn_from_device_130 = ttnn.from_device(ttnn_reshape_73, )
  ttnn_to_layout_133 = ttnn.to_layout(ttnn_from_device_130, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_77 = ttnn.reshape(ttnn_to_layout_133, (1, 9, 16, 64), )
  ttnn_from_device_131 = ttnn.from_device(ttnn_reshape_77, )
  ttnn_to_layout_134 = ttnn.to_layout(ttnn_from_device_131, ttnn.TILE_LAYOUT, )
  ttnn_to_device_62 = ttnn.to_device(ttnn_to_layout_134, device = device)
  ttnn_permute_10 = ttnn.permute(ttnn_to_device_62, (0, 2, 1, 3), )
  ttnn_from_device_132 = ttnn.from_device(ttnn_reshape_75, )
  ttnn_to_layout_135 = ttnn.to_layout(ttnn_from_device_132, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_78 = ttnn.reshape(ttnn_to_layout_135, (1, 9, 16, 64), )
  ttnn_from_device_133 = ttnn.from_device(ttnn_reshape_78, )
  ttnn_to_layout_136 = ttnn.to_layout(ttnn_from_device_133, ttnn.TILE_LAYOUT, )
  ttnn_to_device_63 = ttnn.to_device(ttnn_to_layout_136, device = device)
  ttnn_permute_11 = ttnn.permute(ttnn_to_device_63, (0, 2, 1, 3), )
  ttnn_transpose_28 = ttnn.transpose(ttnn_permute_10, 3, 2, )
  ttnn_from_device_134 = ttnn.from_device(ttnn_permute_9, )
  ttnn_to_layout_137 = ttnn.to_layout(ttnn_from_device_134, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_79 = ttnn.reshape(ttnn_to_layout_137, (16, 9, 64), )
  ttnn_from_device_135 = ttnn.from_device(ttnn_transpose_28, )
  ttnn_to_layout_138 = ttnn.to_layout(ttnn_from_device_135, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_80 = ttnn.reshape(ttnn_to_layout_138, (16, 64, 9), )
  ttnn_from_device_136 = ttnn.from_device(ttnn_reshape_79, )
  ttnn_to_layout_139 = ttnn.to_layout(ttnn_from_device_136, ttnn.TILE_LAYOUT, )
  ttnn_to_device_64 = ttnn.to_device(ttnn_to_layout_139, device = device)
  ttnn_from_device_137 = ttnn.from_device(ttnn_reshape_80, )
  ttnn_to_layout_140 = ttnn.to_layout(ttnn_from_device_137, ttnn.TILE_LAYOUT, )
  ttnn_to_device_65 = ttnn.to_device(ttnn_to_layout_140, device = device)
  ttnn_matmul_28 = ttnn.matmul(ttnn_to_device_64, ttnn_to_device_65, )
  ttnn_from_device_138 = ttnn.from_device(ttnn_matmul_28, )
  ttnn_to_layout_141 = ttnn.to_layout(ttnn_from_device_138, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_81 = ttnn.reshape(ttnn_to_layout_141, (1, 16, 9, 9), )
  ttnn_from_device_139 = ttnn.from_device(ttnn_reshape_81, )
  ttnn_to_layout_142 = ttnn.to_layout(ttnn_from_device_139, ttnn.TILE_LAYOUT, )
  ttnn_to_device_66 = ttnn.to_device(ttnn_to_layout_142, device = device)
  ttnn_multiply_16 = ttnn.multiply(ttnn_to_device_66, 0.125, )
  ttnn_add_164 = ttnn.add(ttnn_multiply_16, ttnn_multiply, )
  ttnn_softmax_3 = ttnn.softmax(ttnn_add_164, -1, numeric_stable = True)
  test_accuracy(_softmax_3, ttnn_softmax_3)
  ttnn_prefix_clone_10 = clone_wrapper(ttnn_softmax_3, )
  ttnn_from_device_140 = ttnn.from_device(ttnn_prefix_clone_10, )
  ttnn_to_layout_143 = ttnn.to_layout(ttnn_from_device_140, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_82 = ttnn.reshape(ttnn_to_layout_143, (16, 9, 9), )
  ttnn_from_device_141 = ttnn.from_device(ttnn_permute_11, )
  ttnn_to_layout_144 = ttnn.to_layout(ttnn_from_device_141, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_83 = ttnn.reshape(ttnn_to_layout_144, (16, 9, 64), )
  ttnn_from_device_142 = ttnn.from_device(ttnn_reshape_82, )
  ttnn_to_layout_145 = ttnn.to_layout(ttnn_from_device_142, ttnn.TILE_LAYOUT, )
  ttnn_to_device_67 = ttnn.to_device(ttnn_to_layout_145, device = device)
  ttnn_from_device_143 = ttnn.from_device(ttnn_reshape_83, )
  ttnn_to_layout_146 = ttnn.to_layout(ttnn_from_device_143, ttnn.TILE_LAYOUT, )
  ttnn_to_device_68 = ttnn.to_device(ttnn_to_layout_146, device = device)
  ttnn_matmul_29 = ttnn.matmul(ttnn_to_device_67, ttnn_to_device_68, )
  ttnn_from_device_144 = ttnn.from_device(ttnn_matmul_29, )
  ttnn_to_layout_147 = ttnn.to_layout(ttnn_from_device_144, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_84 = ttnn.reshape(ttnn_to_layout_147, (1, 16, 9, 64), )
  ttnn_from_device_145 = ttnn.from_device(ttnn_reshape_84, )
  ttnn_to_layout_148 = ttnn.to_layout(ttnn_from_device_145, ttnn.TILE_LAYOUT, )
  ttnn_to_device_69 = ttnn.to_device(ttnn_to_layout_148, device = device)
  ttnn_transpose_29 = ttnn.transpose(ttnn_to_device_69, 2, 1, )
  ttnn_prefix_clone_11 = clone_wrapper(ttnn_transpose_29, )
  ttnn_from_device_146 = ttnn.from_device(ttnn_prefix_clone_11, )
  ttnn_to_layout_149 = ttnn.to_layout(ttnn_from_device_146, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_85 = ttnn.reshape(ttnn_to_layout_149, (1, 9, 1024), )
  ttnn_from_device_147 = ttnn.from_device(ttnn_reshape_85, )
  ttnn_to_layout_150 = ttnn.to_layout(ttnn_from_device_147, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_86 = ttnn.reshape(ttnn_to_layout_150, (9, 1024), )
  ttnn_from_device_148 = ttnn.from_device(ttnn_reshape_86, )
  ttnn_to_layout_151 = ttnn.to_layout(ttnn_from_device_148, ttnn.TILE_LAYOUT, )
  ttnn_to_device_70 = ttnn.to_device(ttnn_to_layout_151, device = device)
  ttnn_matmul_30 = ttnn.matmul(ttnn_to_device_70, ttnn_transpose_6, )
  ttnn_add_22 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_30, )
  ttnn_from_device_149 = ttnn.from_device(ttnn_add_22, )
  ttnn_to_layout_152 = ttnn.to_layout(ttnn_from_device_149, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_87 = ttnn.reshape(ttnn_to_layout_152, (1, 9, 1024), )
  ttnn_from_device_150 = ttnn.from_device(ttnn_reshape_87, )
  ttnn_to_layout_153 = ttnn.to_layout(ttnn_from_device_150, ttnn.TILE_LAYOUT, )
  ttnn_to_device_71 = ttnn.to_device(ttnn_to_layout_153, device = device)
  ttnn_prefix_clone_12 = clone_wrapper(ttnn_to_device_71, )
  ttnn_add_165 = ttnn.add(ttnn_layer_norm_6, ttnn_prefix_clone_12, )
  ttnn_layer_norm_7_ = ttnn.layer_norm(ttnn_add_165, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_7_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_165), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_7 = ttnn.from_torch(ttnn_layer_norm_7_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_7_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_7_), ttnn_layer_norm_7))
  ttnn_from_device_151 = ttnn.from_device(ttnn_layer_norm_7, )
  ttnn_to_layout_154 = ttnn.to_layout(ttnn_from_device_151, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_88 = ttnn.reshape(ttnn_to_layout_154, (9, 1024), )
  ttnn_from_device_152 = ttnn.from_device(ttnn_reshape_88, )
  ttnn_to_layout_155 = ttnn.to_layout(ttnn_from_device_152, ttnn.TILE_LAYOUT, )
  ttnn_to_device_72 = ttnn.to_device(ttnn_to_layout_155, device = device)
  ttnn_matmul_31 = ttnn.matmul(ttnn_to_device_72, ttnn_transpose_7, )
  ttnn_add_23 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_31, )
  ttnn_from_device_153 = ttnn.from_device(ttnn_add_23, )
  ttnn_to_layout_156 = ttnn.to_layout(ttnn_from_device_153, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_89 = ttnn.reshape(ttnn_to_layout_156, (1, 9, 4096), )
  ttnn_from_device_154 = ttnn.from_device(ttnn_reshape_89, )
  ttnn_to_layout_157 = ttnn.to_layout(ttnn_from_device_154, ttnn.TILE_LAYOUT, )
  ttnn_to_device_73 = ttnn.to_device(ttnn_to_layout_157, device = device)
  ttnn_multiply_17 = ttnn.multiply(ttnn_to_device_73, 0.5, )
  ttnn_pow_3 = ttnn.pow(ttnn_to_device_73, 3.0, )
  ttnn_multiply_18 = ttnn.multiply(ttnn_pow_3, 0.044715, )
  ttnn_add_166 = ttnn.add(ttnn_to_device_73, ttnn_multiply_18, )
  ttnn_multiply_19 = ttnn.multiply(ttnn_add_166, 0.7978845608028654, )
  ttnn_tanh_3_ = ttnn.tanh(ttnn_multiply_19, )
  ttnn_tanh_3_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_19), )
  ttnn_tanh_3 = ttnn.from_torch(ttnn_tanh_3_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_3_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_3_), ttnn_tanh_3))
  test_accuracy(tanh_3, ttnn_tanh_3)
  ttnn_add_167 = ttnn.add(ttnn_tanh_3, 1.0, )
  ttnn_multiply_20 = ttnn.multiply(ttnn_multiply_17, ttnn_add_167, )
  ttnn_from_device_155 = ttnn.from_device(ttnn_multiply_20, )
  ttnn_to_layout_158 = ttnn.to_layout(ttnn_from_device_155, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_90 = ttnn.reshape(ttnn_to_layout_158, (9, 4096), )
  ttnn_from_device_156 = ttnn.from_device(ttnn_reshape_90, )
  ttnn_to_layout_159 = ttnn.to_layout(ttnn_from_device_156, ttnn.TILE_LAYOUT, )
  ttnn_to_device_74 = ttnn.to_device(ttnn_to_layout_159, device = device)
  ttnn_matmul_32 = ttnn.matmul(ttnn_to_device_74, ttnn_transpose_8, )
  ttnn_add_24 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_32, )
  ttnn_from_device_157 = ttnn.from_device(ttnn_add_24, )
  ttnn_to_layout_160 = ttnn.to_layout(ttnn_from_device_157, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_91 = ttnn.reshape(ttnn_to_layout_160, (1, 9, 1024), )
  ttnn_from_device_158 = ttnn.from_device(ttnn_reshape_91, )
  ttnn_to_layout_161 = ttnn.to_layout(ttnn_from_device_158, ttnn.TILE_LAYOUT, )
  ttnn_to_device_75 = ttnn.to_device(ttnn_to_layout_161, device = device)
  ttnn_add_168 = ttnn.add(ttnn_to_device_75, ttnn_layer_norm_7, )
  ttnn_layer_norm_8_ = ttnn.layer_norm(ttnn_add_168, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_8_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_168), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_8 = ttnn.from_torch(ttnn_layer_norm_8_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_8_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_8_), ttnn_layer_norm_8))
  ttnn_from_device_159 = ttnn.from_device(ttnn_layer_norm_8, )
  ttnn_to_layout_162 = ttnn.to_layout(ttnn_from_device_159, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_92 = ttnn.reshape(ttnn_to_layout_162, (9, 1024), )
  ttnn_from_device_160 = ttnn.from_device(ttnn_reshape_92, )
  ttnn_to_layout_163 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_76 = ttnn.to_device(ttnn_to_layout_163, device = device)
  ttnn_matmul_33 = ttnn.matmul(ttnn_to_device_76, ttnn_transpose_1, )
  ttnn_add_25 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_33, )
  ttnn_from_device_161 = ttnn.from_device(ttnn_add_25, )
  ttnn_to_layout_164 = ttnn.to_layout(ttnn_from_device_161, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_93 = ttnn.reshape(ttnn_to_layout_164, (1, 9, 1024), )
  ttnn_to_layout_165 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_77 = ttnn.to_device(ttnn_to_layout_165, device = device)
  ttnn_matmul_34 = ttnn.matmul(ttnn_to_device_77, ttnn_transpose_2, )
  ttnn_add_26 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_34, )
  ttnn_from_device_163 = ttnn.from_device(ttnn_add_26, )
  ttnn_to_layout_166 = ttnn.to_layout(ttnn_from_device_163, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_95 = ttnn.reshape(ttnn_to_layout_166, (1, 9, 1024), )
  ttnn_to_layout_167 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_78 = ttnn.to_device(ttnn_to_layout_167, device = device)
  ttnn_matmul_35 = ttnn.matmul(ttnn_to_device_78, ttnn_transpose_3, )
  ttnn_add_27 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_35, )
  ttnn_from_device_165 = ttnn.from_device(ttnn_add_27, )
  ttnn_to_layout_168 = ttnn.to_layout(ttnn_from_device_165, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_97 = ttnn.reshape(ttnn_to_layout_168, (1, 9, 1024), )
  ttnn_from_device_166 = ttnn.from_device(ttnn_reshape_93, )
  ttnn_to_layout_169 = ttnn.to_layout(ttnn_from_device_166, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_98 = ttnn.reshape(ttnn_to_layout_169, (1, 9, 16, 64), )
  ttnn_from_device_167 = ttnn.from_device(ttnn_reshape_98, )
  ttnn_to_layout_170 = ttnn.to_layout(ttnn_from_device_167, ttnn.TILE_LAYOUT, )
  ttnn_to_device_79 = ttnn.to_device(ttnn_to_layout_170, device = device)
  ttnn_permute_12 = ttnn.permute(ttnn_to_device_79, (0, 2, 1, 3), )
  ttnn_from_device_168 = ttnn.from_device(ttnn_reshape_95, )
  ttnn_to_layout_171 = ttnn.to_layout(ttnn_from_device_168, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_99 = ttnn.reshape(ttnn_to_layout_171, (1, 9, 16, 64), )
  ttnn_from_device_169 = ttnn.from_device(ttnn_reshape_99, )
  ttnn_to_layout_172 = ttnn.to_layout(ttnn_from_device_169, ttnn.TILE_LAYOUT, )
  ttnn_to_device_80 = ttnn.to_device(ttnn_to_layout_172, device = device)
  ttnn_permute_13 = ttnn.permute(ttnn_to_device_80, (0, 2, 1, 3), )
  ttnn_from_device_170 = ttnn.from_device(ttnn_reshape_97, )
  ttnn_to_layout_173 = ttnn.to_layout(ttnn_from_device_170, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_100 = ttnn.reshape(ttnn_to_layout_173, (1, 9, 16, 64), )
  ttnn_from_device_171 = ttnn.from_device(ttnn_reshape_100, )
  ttnn_to_layout_174 = ttnn.to_layout(ttnn_from_device_171, ttnn.TILE_LAYOUT, )
  ttnn_to_device_81 = ttnn.to_device(ttnn_to_layout_174, device = device)
  ttnn_permute_14 = ttnn.permute(ttnn_to_device_81, (0, 2, 1, 3), )
  ttnn_transpose_36 = ttnn.transpose(ttnn_permute_13, 3, 2, )
  ttnn_from_device_172 = ttnn.from_device(ttnn_permute_12, )
  ttnn_to_layout_175 = ttnn.to_layout(ttnn_from_device_172, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_101 = ttnn.reshape(ttnn_to_layout_175, (16, 9, 64), )
  ttnn_from_device_173 = ttnn.from_device(ttnn_transpose_36, )
  ttnn_to_layout_176 = ttnn.to_layout(ttnn_from_device_173, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_102 = ttnn.reshape(ttnn_to_layout_176, (16, 64, 9), )
  ttnn_from_device_174 = ttnn.from_device(ttnn_reshape_101, )
  ttnn_to_layout_177 = ttnn.to_layout(ttnn_from_device_174, ttnn.TILE_LAYOUT, )
  ttnn_to_device_82 = ttnn.to_device(ttnn_to_layout_177, device = device)
  ttnn_from_device_175 = ttnn.from_device(ttnn_reshape_102, )
  ttnn_to_layout_178 = ttnn.to_layout(ttnn_from_device_175, ttnn.TILE_LAYOUT, )
  ttnn_to_device_83 = ttnn.to_device(ttnn_to_layout_178, device = device)
  ttnn_matmul_36 = ttnn.matmul(ttnn_to_device_82, ttnn_to_device_83, )
  ttnn_from_device_176 = ttnn.from_device(ttnn_matmul_36, )
  ttnn_to_layout_179 = ttnn.to_layout(ttnn_from_device_176, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_103 = ttnn.reshape(ttnn_to_layout_179, (1, 16, 9, 9), )
  ttnn_from_device_177 = ttnn.from_device(ttnn_reshape_103, )
  ttnn_to_layout_180 = ttnn.to_layout(ttnn_from_device_177, ttnn.TILE_LAYOUT, )
  ttnn_to_device_84 = ttnn.to_device(ttnn_to_layout_180, device = device)
  ttnn_multiply_21 = ttnn.multiply(ttnn_to_device_84, 0.125, )
  ttnn_add_169 = ttnn.add(ttnn_multiply_21, ttnn_multiply, )
  ttnn_softmax_4 = ttnn.softmax(ttnn_add_169, -1, numeric_stable = True)
  test_accuracy(_softmax_4, ttnn_softmax_4)
  ttnn_prefix_clone_13 = clone_wrapper(ttnn_softmax_4, )
  ttnn_from_device_178 = ttnn.from_device(ttnn_prefix_clone_13, )
  ttnn_to_layout_181 = ttnn.to_layout(ttnn_from_device_178, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_104 = ttnn.reshape(ttnn_to_layout_181, (16, 9, 9), )
  ttnn_from_device_179 = ttnn.from_device(ttnn_permute_14, )
  ttnn_to_layout_182 = ttnn.to_layout(ttnn_from_device_179, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_105 = ttnn.reshape(ttnn_to_layout_182, (16, 9, 64), )
  ttnn_from_device_180 = ttnn.from_device(ttnn_reshape_104, )
  ttnn_to_layout_183 = ttnn.to_layout(ttnn_from_device_180, ttnn.TILE_LAYOUT, )
  ttnn_to_device_85 = ttnn.to_device(ttnn_to_layout_183, device = device)
  ttnn_from_device_181 = ttnn.from_device(ttnn_reshape_105, )
  ttnn_to_layout_184 = ttnn.to_layout(ttnn_from_device_181, ttnn.TILE_LAYOUT, )
  ttnn_to_device_86 = ttnn.to_device(ttnn_to_layout_184, device = device)
  ttnn_matmul_37 = ttnn.matmul(ttnn_to_device_85, ttnn_to_device_86, )
  ttnn_from_device_182 = ttnn.from_device(ttnn_matmul_37, )
  ttnn_to_layout_185 = ttnn.to_layout(ttnn_from_device_182, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_106 = ttnn.reshape(ttnn_to_layout_185, (1, 16, 9, 64), )
  ttnn_from_device_183 = ttnn.from_device(ttnn_reshape_106, )
  ttnn_to_layout_186 = ttnn.to_layout(ttnn_from_device_183, ttnn.TILE_LAYOUT, )
  ttnn_to_device_87 = ttnn.to_device(ttnn_to_layout_186, device = device)
  ttnn_transpose_37 = ttnn.transpose(ttnn_to_device_87, 2, 1, )
  ttnn_prefix_clone_14 = clone_wrapper(ttnn_transpose_37, )
  ttnn_from_device_184 = ttnn.from_device(ttnn_prefix_clone_14, )
  ttnn_to_layout_187 = ttnn.to_layout(ttnn_from_device_184, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_107 = ttnn.reshape(ttnn_to_layout_187, (1, 9, 1024), )
  ttnn_from_device_185 = ttnn.from_device(ttnn_reshape_107, )
  ttnn_to_layout_188 = ttnn.to_layout(ttnn_from_device_185, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_108 = ttnn.reshape(ttnn_to_layout_188, (9, 1024), )
  ttnn_from_device_186 = ttnn.from_device(ttnn_reshape_108, )
  ttnn_to_layout_189 = ttnn.to_layout(ttnn_from_device_186, ttnn.TILE_LAYOUT, )
  ttnn_to_device_88 = ttnn.to_device(ttnn_to_layout_189, device = device)
  ttnn_matmul_38 = ttnn.matmul(ttnn_to_device_88, ttnn_transpose_6, )
  ttnn_add_28 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_38, )
  ttnn_from_device_187 = ttnn.from_device(ttnn_add_28, )
  ttnn_to_layout_190 = ttnn.to_layout(ttnn_from_device_187, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_109 = ttnn.reshape(ttnn_to_layout_190, (1, 9, 1024), )
  ttnn_from_device_188 = ttnn.from_device(ttnn_reshape_109, )
  ttnn_to_layout_191 = ttnn.to_layout(ttnn_from_device_188, ttnn.TILE_LAYOUT, )
  ttnn_to_device_89 = ttnn.to_device(ttnn_to_layout_191, device = device)
  ttnn_prefix_clone_15 = clone_wrapper(ttnn_to_device_89, )
  ttnn_add_170 = ttnn.add(ttnn_layer_norm_8, ttnn_prefix_clone_15, )
  ttnn_layer_norm_9_ = ttnn.layer_norm(ttnn_add_170, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_9_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_170), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_9 = ttnn.from_torch(ttnn_layer_norm_9_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_9_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_9_), ttnn_layer_norm_9))
  ttnn_from_device_189 = ttnn.from_device(ttnn_layer_norm_9, )
  ttnn_to_layout_192 = ttnn.to_layout(ttnn_from_device_189, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_110 = ttnn.reshape(ttnn_to_layout_192, (9, 1024), )
  ttnn_from_device_190 = ttnn.from_device(ttnn_reshape_110, )
  ttnn_to_layout_193 = ttnn.to_layout(ttnn_from_device_190, ttnn.TILE_LAYOUT, )
  ttnn_to_device_90 = ttnn.to_device(ttnn_to_layout_193, device = device)
  ttnn_matmul_39 = ttnn.matmul(ttnn_to_device_90, ttnn_transpose_7, )
  ttnn_add_29 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_39, )
  ttnn_from_device_191 = ttnn.from_device(ttnn_add_29, )
  ttnn_to_layout_194 = ttnn.to_layout(ttnn_from_device_191, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_111 = ttnn.reshape(ttnn_to_layout_194, (1, 9, 4096), )
  ttnn_from_device_192 = ttnn.from_device(ttnn_reshape_111, )
  ttnn_to_layout_195 = ttnn.to_layout(ttnn_from_device_192, ttnn.TILE_LAYOUT, )
  ttnn_to_device_91 = ttnn.to_device(ttnn_to_layout_195, device = device)
  ttnn_multiply_22 = ttnn.multiply(ttnn_to_device_91, 0.5, )
  ttnn_pow_4 = ttnn.pow(ttnn_to_device_91, 3.0, )
  ttnn_multiply_23 = ttnn.multiply(ttnn_pow_4, 0.044715, )
  ttnn_add_171 = ttnn.add(ttnn_to_device_91, ttnn_multiply_23, )
  ttnn_multiply_24 = ttnn.multiply(ttnn_add_171, 0.7978845608028654, )
  ttnn_tanh_4_ = ttnn.tanh(ttnn_multiply_24, )
  ttnn_tanh_4_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_24), )
  ttnn_tanh_4 = ttnn.from_torch(ttnn_tanh_4_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_4_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_4_), ttnn_tanh_4))
  test_accuracy(tanh_4, ttnn_tanh_4)
  ttnn_add_172 = ttnn.add(ttnn_tanh_4, 1.0, )
  ttnn_multiply_25 = ttnn.multiply(ttnn_multiply_22, ttnn_add_172, )
  ttnn_from_device_193 = ttnn.from_device(ttnn_multiply_25, )
  ttnn_to_layout_196 = ttnn.to_layout(ttnn_from_device_193, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_112 = ttnn.reshape(ttnn_to_layout_196, (9, 4096), )
  ttnn_from_device_194 = ttnn.from_device(ttnn_reshape_112, )
  ttnn_to_layout_197 = ttnn.to_layout(ttnn_from_device_194, ttnn.TILE_LAYOUT, )
  ttnn_to_device_92 = ttnn.to_device(ttnn_to_layout_197, device = device)
  ttnn_matmul_40 = ttnn.matmul(ttnn_to_device_92, ttnn_transpose_8, )
  ttnn_add_30 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_40, )
  ttnn_from_device_195 = ttnn.from_device(ttnn_add_30, )
  ttnn_to_layout_198 = ttnn.to_layout(ttnn_from_device_195, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_113 = ttnn.reshape(ttnn_to_layout_198, (1, 9, 1024), )
  ttnn_from_device_196 = ttnn.from_device(ttnn_reshape_113, )
  ttnn_to_layout_199 = ttnn.to_layout(ttnn_from_device_196, ttnn.TILE_LAYOUT, )
  ttnn_to_device_93 = ttnn.to_device(ttnn_to_layout_199, device = device)
  ttnn_add_173 = ttnn.add(ttnn_to_device_93, ttnn_layer_norm_9, )
  ttnn_layer_norm_10_ = ttnn.layer_norm(ttnn_add_173, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_10_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_173), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_10 = ttnn.from_torch(ttnn_layer_norm_10_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_10_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_10_), ttnn_layer_norm_10))
  ttnn_from_device_197 = ttnn.from_device(ttnn_layer_norm_10, )
  ttnn_to_layout_200 = ttnn.to_layout(ttnn_from_device_197, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_114 = ttnn.reshape(ttnn_to_layout_200, (9, 1024), )
  ttnn_from_device_198 = ttnn.from_device(ttnn_reshape_114, )
  ttnn_to_layout_201 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_94 = ttnn.to_device(ttnn_to_layout_201, device = device)
  ttnn_matmul_41 = ttnn.matmul(ttnn_to_device_94, ttnn_transpose_1, )
  ttnn_add_31 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_41, )
  ttnn_from_device_199 = ttnn.from_device(ttnn_add_31, )
  ttnn_to_layout_202 = ttnn.to_layout(ttnn_from_device_199, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_115 = ttnn.reshape(ttnn_to_layout_202, (1, 9, 1024), )
  ttnn_to_layout_203 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_95 = ttnn.to_device(ttnn_to_layout_203, device = device)
  ttnn_matmul_42 = ttnn.matmul(ttnn_to_device_95, ttnn_transpose_2, )
  ttnn_add_32 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_42, )
  ttnn_from_device_201 = ttnn.from_device(ttnn_add_32, )
  ttnn_to_layout_204 = ttnn.to_layout(ttnn_from_device_201, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_117 = ttnn.reshape(ttnn_to_layout_204, (1, 9, 1024), )
  ttnn_to_layout_205 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_96 = ttnn.to_device(ttnn_to_layout_205, device = device)
  ttnn_matmul_43 = ttnn.matmul(ttnn_to_device_96, ttnn_transpose_3, )
  ttnn_add_33 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_43, )
  ttnn_from_device_203 = ttnn.from_device(ttnn_add_33, )
  ttnn_to_layout_206 = ttnn.to_layout(ttnn_from_device_203, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_119 = ttnn.reshape(ttnn_to_layout_206, (1, 9, 1024), )
  ttnn_from_device_204 = ttnn.from_device(ttnn_reshape_115, )
  ttnn_to_layout_207 = ttnn.to_layout(ttnn_from_device_204, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_120 = ttnn.reshape(ttnn_to_layout_207, (1, 9, 16, 64), )
  ttnn_from_device_205 = ttnn.from_device(ttnn_reshape_120, )
  ttnn_to_layout_208 = ttnn.to_layout(ttnn_from_device_205, ttnn.TILE_LAYOUT, )
  ttnn_to_device_97 = ttnn.to_device(ttnn_to_layout_208, device = device)
  ttnn_permute_15 = ttnn.permute(ttnn_to_device_97, (0, 2, 1, 3), )
  ttnn_from_device_206 = ttnn.from_device(ttnn_reshape_117, )
  ttnn_to_layout_209 = ttnn.to_layout(ttnn_from_device_206, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_121 = ttnn.reshape(ttnn_to_layout_209, (1, 9, 16, 64), )
  ttnn_from_device_207 = ttnn.from_device(ttnn_reshape_121, )
  ttnn_to_layout_210 = ttnn.to_layout(ttnn_from_device_207, ttnn.TILE_LAYOUT, )
  ttnn_to_device_98 = ttnn.to_device(ttnn_to_layout_210, device = device)
  ttnn_permute_16 = ttnn.permute(ttnn_to_device_98, (0, 2, 1, 3), )
  ttnn_from_device_208 = ttnn.from_device(ttnn_reshape_119, )
  ttnn_to_layout_211 = ttnn.to_layout(ttnn_from_device_208, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_122 = ttnn.reshape(ttnn_to_layout_211, (1, 9, 16, 64), )
  ttnn_from_device_209 = ttnn.from_device(ttnn_reshape_122, )
  ttnn_to_layout_212 = ttnn.to_layout(ttnn_from_device_209, ttnn.TILE_LAYOUT, )
  ttnn_to_device_99 = ttnn.to_device(ttnn_to_layout_212, device = device)
  ttnn_permute_17 = ttnn.permute(ttnn_to_device_99, (0, 2, 1, 3), )
  ttnn_transpose_44 = ttnn.transpose(ttnn_permute_16, 3, 2, )
  ttnn_from_device_210 = ttnn.from_device(ttnn_permute_15, )
  ttnn_to_layout_213 = ttnn.to_layout(ttnn_from_device_210, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_123 = ttnn.reshape(ttnn_to_layout_213, (16, 9, 64), )
  ttnn_from_device_211 = ttnn.from_device(ttnn_transpose_44, )
  ttnn_to_layout_214 = ttnn.to_layout(ttnn_from_device_211, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_124 = ttnn.reshape(ttnn_to_layout_214, (16, 64, 9), )
  ttnn_from_device_212 = ttnn.from_device(ttnn_reshape_123, )
  ttnn_to_layout_215 = ttnn.to_layout(ttnn_from_device_212, ttnn.TILE_LAYOUT, )
  ttnn_to_device_100 = ttnn.to_device(ttnn_to_layout_215, device = device)
  ttnn_from_device_213 = ttnn.from_device(ttnn_reshape_124, )
  ttnn_to_layout_216 = ttnn.to_layout(ttnn_from_device_213, ttnn.TILE_LAYOUT, )
  ttnn_to_device_101 = ttnn.to_device(ttnn_to_layout_216, device = device)
  ttnn_matmul_44 = ttnn.matmul(ttnn_to_device_100, ttnn_to_device_101, )
  ttnn_from_device_214 = ttnn.from_device(ttnn_matmul_44, )
  ttnn_to_layout_217 = ttnn.to_layout(ttnn_from_device_214, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_125 = ttnn.reshape(ttnn_to_layout_217, (1, 16, 9, 9), )
  ttnn_from_device_215 = ttnn.from_device(ttnn_reshape_125, )
  ttnn_to_layout_218 = ttnn.to_layout(ttnn_from_device_215, ttnn.TILE_LAYOUT, )
  ttnn_to_device_102 = ttnn.to_device(ttnn_to_layout_218, device = device)
  ttnn_multiply_26 = ttnn.multiply(ttnn_to_device_102, 0.125, )
  ttnn_add_174 = ttnn.add(ttnn_multiply_26, ttnn_multiply, )
  ttnn_softmax_5 = ttnn.softmax(ttnn_add_174, -1, numeric_stable = True)
  test_accuracy(_softmax_5, ttnn_softmax_5)
  ttnn_prefix_clone_16 = clone_wrapper(ttnn_softmax_5, )
  ttnn_from_device_216 = ttnn.from_device(ttnn_prefix_clone_16, )
  ttnn_to_layout_219 = ttnn.to_layout(ttnn_from_device_216, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_126 = ttnn.reshape(ttnn_to_layout_219, (16, 9, 9), )
  ttnn_from_device_217 = ttnn.from_device(ttnn_permute_17, )
  ttnn_to_layout_220 = ttnn.to_layout(ttnn_from_device_217, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_127 = ttnn.reshape(ttnn_to_layout_220, (16, 9, 64), )
  ttnn_from_device_218 = ttnn.from_device(ttnn_reshape_126, )
  ttnn_to_layout_221 = ttnn.to_layout(ttnn_from_device_218, ttnn.TILE_LAYOUT, )
  ttnn_to_device_103 = ttnn.to_device(ttnn_to_layout_221, device = device)
  ttnn_from_device_219 = ttnn.from_device(ttnn_reshape_127, )
  ttnn_to_layout_222 = ttnn.to_layout(ttnn_from_device_219, ttnn.TILE_LAYOUT, )
  ttnn_to_device_104 = ttnn.to_device(ttnn_to_layout_222, device = device)
  ttnn_matmul_45 = ttnn.matmul(ttnn_to_device_103, ttnn_to_device_104, )
  ttnn_from_device_220 = ttnn.from_device(ttnn_matmul_45, )
  ttnn_to_layout_223 = ttnn.to_layout(ttnn_from_device_220, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_128 = ttnn.reshape(ttnn_to_layout_223, (1, 16, 9, 64), )
  ttnn_from_device_221 = ttnn.from_device(ttnn_reshape_128, )
  ttnn_to_layout_224 = ttnn.to_layout(ttnn_from_device_221, ttnn.TILE_LAYOUT, )
  ttnn_to_device_105 = ttnn.to_device(ttnn_to_layout_224, device = device)
  ttnn_transpose_45 = ttnn.transpose(ttnn_to_device_105, 2, 1, )
  ttnn_prefix_clone_17 = clone_wrapper(ttnn_transpose_45, )
  ttnn_from_device_222 = ttnn.from_device(ttnn_prefix_clone_17, )
  ttnn_to_layout_225 = ttnn.to_layout(ttnn_from_device_222, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_129 = ttnn.reshape(ttnn_to_layout_225, (1, 9, 1024), )
  ttnn_from_device_223 = ttnn.from_device(ttnn_reshape_129, )
  ttnn_to_layout_226 = ttnn.to_layout(ttnn_from_device_223, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_130 = ttnn.reshape(ttnn_to_layout_226, (9, 1024), )
  ttnn_from_device_224 = ttnn.from_device(ttnn_reshape_130, )
  ttnn_to_layout_227 = ttnn.to_layout(ttnn_from_device_224, ttnn.TILE_LAYOUT, )
  ttnn_to_device_106 = ttnn.to_device(ttnn_to_layout_227, device = device)
  ttnn_matmul_46 = ttnn.matmul(ttnn_to_device_106, ttnn_transpose_6, )
  ttnn_add_34 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_46, )
  ttnn_from_device_225 = ttnn.from_device(ttnn_add_34, )
  ttnn_to_layout_228 = ttnn.to_layout(ttnn_from_device_225, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_131 = ttnn.reshape(ttnn_to_layout_228, (1, 9, 1024), )
  ttnn_from_device_226 = ttnn.from_device(ttnn_reshape_131, )
  ttnn_to_layout_229 = ttnn.to_layout(ttnn_from_device_226, ttnn.TILE_LAYOUT, )
  ttnn_to_device_107 = ttnn.to_device(ttnn_to_layout_229, device = device)
  ttnn_prefix_clone_18 = clone_wrapper(ttnn_to_device_107, )
  ttnn_add_175 = ttnn.add(ttnn_layer_norm_10, ttnn_prefix_clone_18, )
  ttnn_layer_norm_11_ = ttnn.layer_norm(ttnn_add_175, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_11_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_175), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_11 = ttnn.from_torch(ttnn_layer_norm_11_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_11_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_11_), ttnn_layer_norm_11))
  ttnn_from_device_227 = ttnn.from_device(ttnn_layer_norm_11, )
  ttnn_to_layout_230 = ttnn.to_layout(ttnn_from_device_227, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_132 = ttnn.reshape(ttnn_to_layout_230, (9, 1024), )
  ttnn_from_device_228 = ttnn.from_device(ttnn_reshape_132, )
  ttnn_to_layout_231 = ttnn.to_layout(ttnn_from_device_228, ttnn.TILE_LAYOUT, )
  ttnn_to_device_108 = ttnn.to_device(ttnn_to_layout_231, device = device)
  ttnn_matmul_47 = ttnn.matmul(ttnn_to_device_108, ttnn_transpose_7, )
  ttnn_add_35 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_47, )
  ttnn_from_device_229 = ttnn.from_device(ttnn_add_35, )
  ttnn_to_layout_232 = ttnn.to_layout(ttnn_from_device_229, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_133 = ttnn.reshape(ttnn_to_layout_232, (1, 9, 4096), )
  ttnn_from_device_230 = ttnn.from_device(ttnn_reshape_133, )
  ttnn_to_layout_233 = ttnn.to_layout(ttnn_from_device_230, ttnn.TILE_LAYOUT, )
  ttnn_to_device_109 = ttnn.to_device(ttnn_to_layout_233, device = device)
  ttnn_multiply_27 = ttnn.multiply(ttnn_to_device_109, 0.5, )
  ttnn_pow_5 = ttnn.pow(ttnn_to_device_109, 3.0, )
  ttnn_multiply_28 = ttnn.multiply(ttnn_pow_5, 0.044715, )
  ttnn_add_176 = ttnn.add(ttnn_to_device_109, ttnn_multiply_28, )
  ttnn_multiply_29 = ttnn.multiply(ttnn_add_176, 0.7978845608028654, )
  ttnn_tanh_5_ = ttnn.tanh(ttnn_multiply_29, )
  ttnn_tanh_5_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_29), )
  ttnn_tanh_5 = ttnn.from_torch(ttnn_tanh_5_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_5_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_5_), ttnn_tanh_5))
  test_accuracy(tanh_5, ttnn_tanh_5)
  ttnn_add_177 = ttnn.add(ttnn_tanh_5, 1.0, )
  ttnn_multiply_30 = ttnn.multiply(ttnn_multiply_27, ttnn_add_177, )
  ttnn_from_device_231 = ttnn.from_device(ttnn_multiply_30, )
  ttnn_to_layout_234 = ttnn.to_layout(ttnn_from_device_231, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_134 = ttnn.reshape(ttnn_to_layout_234, (9, 4096), )
  ttnn_from_device_232 = ttnn.from_device(ttnn_reshape_134, )
  ttnn_to_layout_235 = ttnn.to_layout(ttnn_from_device_232, ttnn.TILE_LAYOUT, )
  ttnn_to_device_110 = ttnn.to_device(ttnn_to_layout_235, device = device)
  ttnn_matmul_48 = ttnn.matmul(ttnn_to_device_110, ttnn_transpose_8, )
  ttnn_add_36 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_48, )
  ttnn_from_device_233 = ttnn.from_device(ttnn_add_36, )
  ttnn_to_layout_236 = ttnn.to_layout(ttnn_from_device_233, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_135 = ttnn.reshape(ttnn_to_layout_236, (1, 9, 1024), )
  ttnn_from_device_234 = ttnn.from_device(ttnn_reshape_135, )
  ttnn_to_layout_237 = ttnn.to_layout(ttnn_from_device_234, ttnn.TILE_LAYOUT, )
  ttnn_to_device_111 = ttnn.to_device(ttnn_to_layout_237, device = device)
  ttnn_add_178 = ttnn.add(ttnn_to_device_111, ttnn_layer_norm_11, )
  ttnn_layer_norm_12_ = ttnn.layer_norm(ttnn_add_178, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_12_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_178), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_12 = ttnn.from_torch(ttnn_layer_norm_12_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_12_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_12_), ttnn_layer_norm_12))
  ttnn_from_device_235 = ttnn.from_device(ttnn_layer_norm_12, )
  ttnn_to_layout_238 = ttnn.to_layout(ttnn_from_device_235, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_136 = ttnn.reshape(ttnn_to_layout_238, (9, 1024), )
  ttnn_from_device_236 = ttnn.from_device(ttnn_reshape_136, )
  ttnn_to_layout_239 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_112 = ttnn.to_device(ttnn_to_layout_239, device = device)
  ttnn_matmul_49 = ttnn.matmul(ttnn_to_device_112, ttnn_transpose_1, )
  ttnn_add_37 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_49, )
  ttnn_from_device_237 = ttnn.from_device(ttnn_add_37, )
  ttnn_to_layout_240 = ttnn.to_layout(ttnn_from_device_237, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_137 = ttnn.reshape(ttnn_to_layout_240, (1, 9, 1024), )
  ttnn_to_layout_241 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_113 = ttnn.to_device(ttnn_to_layout_241, device = device)
  ttnn_matmul_50 = ttnn.matmul(ttnn_to_device_113, ttnn_transpose_2, )
  ttnn_add_38 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_50, )
  ttnn_from_device_239 = ttnn.from_device(ttnn_add_38, )
  ttnn_to_layout_242 = ttnn.to_layout(ttnn_from_device_239, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_139 = ttnn.reshape(ttnn_to_layout_242, (1, 9, 1024), )
  ttnn_to_layout_243 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_114 = ttnn.to_device(ttnn_to_layout_243, device = device)
  ttnn_matmul_51 = ttnn.matmul(ttnn_to_device_114, ttnn_transpose_3, )
  ttnn_add_39 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_51, )
  ttnn_from_device_241 = ttnn.from_device(ttnn_add_39, )
  ttnn_to_layout_244 = ttnn.to_layout(ttnn_from_device_241, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_141 = ttnn.reshape(ttnn_to_layout_244, (1, 9, 1024), )
  ttnn_from_device_242 = ttnn.from_device(ttnn_reshape_137, )
  ttnn_to_layout_245 = ttnn.to_layout(ttnn_from_device_242, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_142 = ttnn.reshape(ttnn_to_layout_245, (1, 9, 16, 64), )
  ttnn_from_device_243 = ttnn.from_device(ttnn_reshape_142, )
  ttnn_to_layout_246 = ttnn.to_layout(ttnn_from_device_243, ttnn.TILE_LAYOUT, )
  ttnn_to_device_115 = ttnn.to_device(ttnn_to_layout_246, device = device)
  ttnn_permute_18 = ttnn.permute(ttnn_to_device_115, (0, 2, 1, 3), )
  ttnn_from_device_244 = ttnn.from_device(ttnn_reshape_139, )
  ttnn_to_layout_247 = ttnn.to_layout(ttnn_from_device_244, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_143 = ttnn.reshape(ttnn_to_layout_247, (1, 9, 16, 64), )
  ttnn_from_device_245 = ttnn.from_device(ttnn_reshape_143, )
  ttnn_to_layout_248 = ttnn.to_layout(ttnn_from_device_245, ttnn.TILE_LAYOUT, )
  ttnn_to_device_116 = ttnn.to_device(ttnn_to_layout_248, device = device)
  ttnn_permute_19 = ttnn.permute(ttnn_to_device_116, (0, 2, 1, 3), )
  ttnn_from_device_246 = ttnn.from_device(ttnn_reshape_141, )
  ttnn_to_layout_249 = ttnn.to_layout(ttnn_from_device_246, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_144 = ttnn.reshape(ttnn_to_layout_249, (1, 9, 16, 64), )
  ttnn_from_device_247 = ttnn.from_device(ttnn_reshape_144, )
  ttnn_to_layout_250 = ttnn.to_layout(ttnn_from_device_247, ttnn.TILE_LAYOUT, )
  ttnn_to_device_117 = ttnn.to_device(ttnn_to_layout_250, device = device)
  ttnn_permute_20 = ttnn.permute(ttnn_to_device_117, (0, 2, 1, 3), )
  ttnn_transpose_52 = ttnn.transpose(ttnn_permute_19, 3, 2, )
  ttnn_from_device_248 = ttnn.from_device(ttnn_permute_18, )
  ttnn_to_layout_251 = ttnn.to_layout(ttnn_from_device_248, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_145 = ttnn.reshape(ttnn_to_layout_251, (16, 9, 64), )
  ttnn_from_device_249 = ttnn.from_device(ttnn_transpose_52, )
  ttnn_to_layout_252 = ttnn.to_layout(ttnn_from_device_249, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_146 = ttnn.reshape(ttnn_to_layout_252, (16, 64, 9), )
  ttnn_from_device_250 = ttnn.from_device(ttnn_reshape_145, )
  ttnn_to_layout_253 = ttnn.to_layout(ttnn_from_device_250, ttnn.TILE_LAYOUT, )
  ttnn_to_device_118 = ttnn.to_device(ttnn_to_layout_253, device = device)
  ttnn_from_device_251 = ttnn.from_device(ttnn_reshape_146, )
  ttnn_to_layout_254 = ttnn.to_layout(ttnn_from_device_251, ttnn.TILE_LAYOUT, )
  ttnn_to_device_119 = ttnn.to_device(ttnn_to_layout_254, device = device)
  ttnn_matmul_52 = ttnn.matmul(ttnn_to_device_118, ttnn_to_device_119, )
  ttnn_from_device_252 = ttnn.from_device(ttnn_matmul_52, )
  ttnn_to_layout_255 = ttnn.to_layout(ttnn_from_device_252, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_147 = ttnn.reshape(ttnn_to_layout_255, (1, 16, 9, 9), )
  ttnn_from_device_253 = ttnn.from_device(ttnn_reshape_147, )
  ttnn_to_layout_256 = ttnn.to_layout(ttnn_from_device_253, ttnn.TILE_LAYOUT, )
  ttnn_to_device_120 = ttnn.to_device(ttnn_to_layout_256, device = device)
  ttnn_multiply_31 = ttnn.multiply(ttnn_to_device_120, 0.125, )
  ttnn_add_179 = ttnn.add(ttnn_multiply_31, ttnn_multiply, )
  ttnn_softmax_6 = ttnn.softmax(ttnn_add_179, -1, numeric_stable = True)
  test_accuracy(_softmax_6, ttnn_softmax_6)
  ttnn_prefix_clone_19 = clone_wrapper(ttnn_softmax_6, )
  ttnn_from_device_254 = ttnn.from_device(ttnn_prefix_clone_19, )
  ttnn_to_layout_257 = ttnn.to_layout(ttnn_from_device_254, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_148 = ttnn.reshape(ttnn_to_layout_257, (16, 9, 9), )
  ttnn_from_device_255 = ttnn.from_device(ttnn_permute_20, )
  ttnn_to_layout_258 = ttnn.to_layout(ttnn_from_device_255, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_149 = ttnn.reshape(ttnn_to_layout_258, (16, 9, 64), )
  ttnn_from_device_256 = ttnn.from_device(ttnn_reshape_148, )
  ttnn_to_layout_259 = ttnn.to_layout(ttnn_from_device_256, ttnn.TILE_LAYOUT, )
  ttnn_to_device_121 = ttnn.to_device(ttnn_to_layout_259, device = device)
  ttnn_from_device_257 = ttnn.from_device(ttnn_reshape_149, )
  ttnn_to_layout_260 = ttnn.to_layout(ttnn_from_device_257, ttnn.TILE_LAYOUT, )
  ttnn_to_device_122 = ttnn.to_device(ttnn_to_layout_260, device = device)
  ttnn_matmul_53 = ttnn.matmul(ttnn_to_device_121, ttnn_to_device_122, )
  ttnn_from_device_258 = ttnn.from_device(ttnn_matmul_53, )
  ttnn_to_layout_261 = ttnn.to_layout(ttnn_from_device_258, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_150 = ttnn.reshape(ttnn_to_layout_261, (1, 16, 9, 64), )
  ttnn_from_device_259 = ttnn.from_device(ttnn_reshape_150, )
  ttnn_to_layout_262 = ttnn.to_layout(ttnn_from_device_259, ttnn.TILE_LAYOUT, )
  ttnn_to_device_123 = ttnn.to_device(ttnn_to_layout_262, device = device)
  ttnn_transpose_53 = ttnn.transpose(ttnn_to_device_123, 2, 1, )
  ttnn_prefix_clone_20 = clone_wrapper(ttnn_transpose_53, )
  ttnn_from_device_260 = ttnn.from_device(ttnn_prefix_clone_20, )
  ttnn_to_layout_263 = ttnn.to_layout(ttnn_from_device_260, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_151 = ttnn.reshape(ttnn_to_layout_263, (1, 9, 1024), )
  ttnn_from_device_261 = ttnn.from_device(ttnn_reshape_151, )
  ttnn_to_layout_264 = ttnn.to_layout(ttnn_from_device_261, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_152 = ttnn.reshape(ttnn_to_layout_264, (9, 1024), )
  ttnn_from_device_262 = ttnn.from_device(ttnn_reshape_152, )
  ttnn_to_layout_265 = ttnn.to_layout(ttnn_from_device_262, ttnn.TILE_LAYOUT, )
  ttnn_to_device_124 = ttnn.to_device(ttnn_to_layout_265, device = device)
  ttnn_matmul_54 = ttnn.matmul(ttnn_to_device_124, ttnn_transpose_6, )
  ttnn_add_40 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_54, )
  ttnn_from_device_263 = ttnn.from_device(ttnn_add_40, )
  ttnn_to_layout_266 = ttnn.to_layout(ttnn_from_device_263, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_153 = ttnn.reshape(ttnn_to_layout_266, (1, 9, 1024), )
  ttnn_from_device_264 = ttnn.from_device(ttnn_reshape_153, )
  ttnn_to_layout_267 = ttnn.to_layout(ttnn_from_device_264, ttnn.TILE_LAYOUT, )
  ttnn_to_device_125 = ttnn.to_device(ttnn_to_layout_267, device = device)
  ttnn_prefix_clone_21 = clone_wrapper(ttnn_to_device_125, )
  ttnn_add_180 = ttnn.add(ttnn_layer_norm_12, ttnn_prefix_clone_21, )
  ttnn_layer_norm_13_ = ttnn.layer_norm(ttnn_add_180, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_13_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_180), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_13 = ttnn.from_torch(ttnn_layer_norm_13_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_13_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_13_), ttnn_layer_norm_13))
  ttnn_from_device_265 = ttnn.from_device(ttnn_layer_norm_13, )
  ttnn_to_layout_268 = ttnn.to_layout(ttnn_from_device_265, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_154 = ttnn.reshape(ttnn_to_layout_268, (9, 1024), )
  ttnn_from_device_266 = ttnn.from_device(ttnn_reshape_154, )
  ttnn_to_layout_269 = ttnn.to_layout(ttnn_from_device_266, ttnn.TILE_LAYOUT, )
  ttnn_to_device_126 = ttnn.to_device(ttnn_to_layout_269, device = device)
  ttnn_matmul_55 = ttnn.matmul(ttnn_to_device_126, ttnn_transpose_7, )
  ttnn_add_41 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_55, )
  ttnn_from_device_267 = ttnn.from_device(ttnn_add_41, )
  ttnn_to_layout_270 = ttnn.to_layout(ttnn_from_device_267, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_155 = ttnn.reshape(ttnn_to_layout_270, (1, 9, 4096), )
  ttnn_from_device_268 = ttnn.from_device(ttnn_reshape_155, )
  ttnn_to_layout_271 = ttnn.to_layout(ttnn_from_device_268, ttnn.TILE_LAYOUT, )
  ttnn_to_device_127 = ttnn.to_device(ttnn_to_layout_271, device = device)
  ttnn_multiply_32 = ttnn.multiply(ttnn_to_device_127, 0.5, )
  ttnn_pow_6 = ttnn.pow(ttnn_to_device_127, 3.0, )
  ttnn_multiply_33 = ttnn.multiply(ttnn_pow_6, 0.044715, )
  ttnn_add_181 = ttnn.add(ttnn_to_device_127, ttnn_multiply_33, )
  ttnn_multiply_34 = ttnn.multiply(ttnn_add_181, 0.7978845608028654, )
  ttnn_tanh_6_ = ttnn.tanh(ttnn_multiply_34, )
  ttnn_tanh_6_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_34), )
  ttnn_tanh_6 = ttnn.from_torch(ttnn_tanh_6_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_6_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_6_), ttnn_tanh_6))
  test_accuracy(tanh_6, ttnn_tanh_6)
  ttnn_add_182 = ttnn.add(ttnn_tanh_6, 1.0, )
  ttnn_multiply_35 = ttnn.multiply(ttnn_multiply_32, ttnn_add_182, )
  ttnn_from_device_269 = ttnn.from_device(ttnn_multiply_35, )
  ttnn_to_layout_272 = ttnn.to_layout(ttnn_from_device_269, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_156 = ttnn.reshape(ttnn_to_layout_272, (9, 4096), )
  ttnn_from_device_270 = ttnn.from_device(ttnn_reshape_156, )
  ttnn_to_layout_273 = ttnn.to_layout(ttnn_from_device_270, ttnn.TILE_LAYOUT, )
  ttnn_to_device_128 = ttnn.to_device(ttnn_to_layout_273, device = device)
  ttnn_matmul_56 = ttnn.matmul(ttnn_to_device_128, ttnn_transpose_8, )
  ttnn_add_42 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_56, )
  ttnn_from_device_271 = ttnn.from_device(ttnn_add_42, )
  ttnn_to_layout_274 = ttnn.to_layout(ttnn_from_device_271, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_157 = ttnn.reshape(ttnn_to_layout_274, (1, 9, 1024), )
  ttnn_from_device_272 = ttnn.from_device(ttnn_reshape_157, )
  ttnn_to_layout_275 = ttnn.to_layout(ttnn_from_device_272, ttnn.TILE_LAYOUT, )
  ttnn_to_device_129 = ttnn.to_device(ttnn_to_layout_275, device = device)
  ttnn_add_183 = ttnn.add(ttnn_to_device_129, ttnn_layer_norm_13, )
  ttnn_layer_norm_14_ = ttnn.layer_norm(ttnn_add_183, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_14_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_183), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_14 = ttnn.from_torch(ttnn_layer_norm_14_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_14_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_14_), ttnn_layer_norm_14))
  ttnn_from_device_273 = ttnn.from_device(ttnn_layer_norm_14, )
  ttnn_to_layout_276 = ttnn.to_layout(ttnn_from_device_273, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_158 = ttnn.reshape(ttnn_to_layout_276, (9, 1024), )
  ttnn_from_device_274 = ttnn.from_device(ttnn_reshape_158, )
  ttnn_to_layout_277 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_130 = ttnn.to_device(ttnn_to_layout_277, device = device)
  ttnn_matmul_57 = ttnn.matmul(ttnn_to_device_130, ttnn_transpose_1, )
  ttnn_add_43 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_57, )
  ttnn_from_device_275 = ttnn.from_device(ttnn_add_43, )
  ttnn_to_layout_278 = ttnn.to_layout(ttnn_from_device_275, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_159 = ttnn.reshape(ttnn_to_layout_278, (1, 9, 1024), )
  ttnn_to_layout_279 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_131 = ttnn.to_device(ttnn_to_layout_279, device = device)
  ttnn_matmul_58 = ttnn.matmul(ttnn_to_device_131, ttnn_transpose_2, )
  ttnn_add_44 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_58, )
  ttnn_from_device_277 = ttnn.from_device(ttnn_add_44, )
  ttnn_to_layout_280 = ttnn.to_layout(ttnn_from_device_277, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_161 = ttnn.reshape(ttnn_to_layout_280, (1, 9, 1024), )
  ttnn_to_layout_281 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_132 = ttnn.to_device(ttnn_to_layout_281, device = device)
  ttnn_matmul_59 = ttnn.matmul(ttnn_to_device_132, ttnn_transpose_3, )
  ttnn_add_45 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_59, )
  ttnn_from_device_279 = ttnn.from_device(ttnn_add_45, )
  ttnn_to_layout_282 = ttnn.to_layout(ttnn_from_device_279, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_163 = ttnn.reshape(ttnn_to_layout_282, (1, 9, 1024), )
  ttnn_from_device_280 = ttnn.from_device(ttnn_reshape_159, )
  ttnn_to_layout_283 = ttnn.to_layout(ttnn_from_device_280, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_164 = ttnn.reshape(ttnn_to_layout_283, (1, 9, 16, 64), )
  ttnn_from_device_281 = ttnn.from_device(ttnn_reshape_164, )
  ttnn_to_layout_284 = ttnn.to_layout(ttnn_from_device_281, ttnn.TILE_LAYOUT, )
  ttnn_to_device_133 = ttnn.to_device(ttnn_to_layout_284, device = device)
  ttnn_permute_21 = ttnn.permute(ttnn_to_device_133, (0, 2, 1, 3), )
  ttnn_from_device_282 = ttnn.from_device(ttnn_reshape_161, )
  ttnn_to_layout_285 = ttnn.to_layout(ttnn_from_device_282, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_165 = ttnn.reshape(ttnn_to_layout_285, (1, 9, 16, 64), )
  ttnn_from_device_283 = ttnn.from_device(ttnn_reshape_165, )
  ttnn_to_layout_286 = ttnn.to_layout(ttnn_from_device_283, ttnn.TILE_LAYOUT, )
  ttnn_to_device_134 = ttnn.to_device(ttnn_to_layout_286, device = device)
  ttnn_permute_22 = ttnn.permute(ttnn_to_device_134, (0, 2, 1, 3), )
  ttnn_from_device_284 = ttnn.from_device(ttnn_reshape_163, )
  ttnn_to_layout_287 = ttnn.to_layout(ttnn_from_device_284, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_166 = ttnn.reshape(ttnn_to_layout_287, (1, 9, 16, 64), )
  ttnn_from_device_285 = ttnn.from_device(ttnn_reshape_166, )
  ttnn_to_layout_288 = ttnn.to_layout(ttnn_from_device_285, ttnn.TILE_LAYOUT, )
  ttnn_to_device_135 = ttnn.to_device(ttnn_to_layout_288, device = device)
  ttnn_permute_23 = ttnn.permute(ttnn_to_device_135, (0, 2, 1, 3), )
  ttnn_transpose_60 = ttnn.transpose(ttnn_permute_22, 3, 2, )
  ttnn_from_device_286 = ttnn.from_device(ttnn_permute_21, )
  ttnn_to_layout_289 = ttnn.to_layout(ttnn_from_device_286, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_167 = ttnn.reshape(ttnn_to_layout_289, (16, 9, 64), )
  ttnn_from_device_287 = ttnn.from_device(ttnn_transpose_60, )
  ttnn_to_layout_290 = ttnn.to_layout(ttnn_from_device_287, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_168 = ttnn.reshape(ttnn_to_layout_290, (16, 64, 9), )
  ttnn_from_device_288 = ttnn.from_device(ttnn_reshape_167, )
  ttnn_to_layout_291 = ttnn.to_layout(ttnn_from_device_288, ttnn.TILE_LAYOUT, )
  ttnn_to_device_136 = ttnn.to_device(ttnn_to_layout_291, device = device)
  ttnn_from_device_289 = ttnn.from_device(ttnn_reshape_168, )
  ttnn_to_layout_292 = ttnn.to_layout(ttnn_from_device_289, ttnn.TILE_LAYOUT, )
  ttnn_to_device_137 = ttnn.to_device(ttnn_to_layout_292, device = device)
  ttnn_matmul_60 = ttnn.matmul(ttnn_to_device_136, ttnn_to_device_137, )
  ttnn_from_device_290 = ttnn.from_device(ttnn_matmul_60, )
  ttnn_to_layout_293 = ttnn.to_layout(ttnn_from_device_290, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_169 = ttnn.reshape(ttnn_to_layout_293, (1, 16, 9, 9), )
  ttnn_from_device_291 = ttnn.from_device(ttnn_reshape_169, )
  ttnn_to_layout_294 = ttnn.to_layout(ttnn_from_device_291, ttnn.TILE_LAYOUT, )
  ttnn_to_device_138 = ttnn.to_device(ttnn_to_layout_294, device = device)
  ttnn_multiply_36 = ttnn.multiply(ttnn_to_device_138, 0.125, )
  ttnn_add_184 = ttnn.add(ttnn_multiply_36, ttnn_multiply, )
  ttnn_softmax_7 = ttnn.softmax(ttnn_add_184, -1, numeric_stable = True)
  test_accuracy(_softmax_7, ttnn_softmax_7)
  ttnn_prefix_clone_22 = clone_wrapper(ttnn_softmax_7, )
  ttnn_from_device_292 = ttnn.from_device(ttnn_prefix_clone_22, )
  ttnn_to_layout_295 = ttnn.to_layout(ttnn_from_device_292, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_170 = ttnn.reshape(ttnn_to_layout_295, (16, 9, 9), )
  ttnn_from_device_293 = ttnn.from_device(ttnn_permute_23, )
  ttnn_to_layout_296 = ttnn.to_layout(ttnn_from_device_293, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_171 = ttnn.reshape(ttnn_to_layout_296, (16, 9, 64), )
  ttnn_from_device_294 = ttnn.from_device(ttnn_reshape_170, )
  ttnn_to_layout_297 = ttnn.to_layout(ttnn_from_device_294, ttnn.TILE_LAYOUT, )
  ttnn_to_device_139 = ttnn.to_device(ttnn_to_layout_297, device = device)
  ttnn_from_device_295 = ttnn.from_device(ttnn_reshape_171, )
  ttnn_to_layout_298 = ttnn.to_layout(ttnn_from_device_295, ttnn.TILE_LAYOUT, )
  ttnn_to_device_140 = ttnn.to_device(ttnn_to_layout_298, device = device)
  ttnn_matmul_61 = ttnn.matmul(ttnn_to_device_139, ttnn_to_device_140, )
  ttnn_from_device_296 = ttnn.from_device(ttnn_matmul_61, )
  ttnn_to_layout_299 = ttnn.to_layout(ttnn_from_device_296, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_172 = ttnn.reshape(ttnn_to_layout_299, (1, 16, 9, 64), )
  ttnn_from_device_297 = ttnn.from_device(ttnn_reshape_172, )
  ttnn_to_layout_300 = ttnn.to_layout(ttnn_from_device_297, ttnn.TILE_LAYOUT, )
  ttnn_to_device_141 = ttnn.to_device(ttnn_to_layout_300, device = device)
  ttnn_transpose_61 = ttnn.transpose(ttnn_to_device_141, 2, 1, )
  ttnn_prefix_clone_23 = clone_wrapper(ttnn_transpose_61, )
  ttnn_from_device_298 = ttnn.from_device(ttnn_prefix_clone_23, )
  ttnn_to_layout_301 = ttnn.to_layout(ttnn_from_device_298, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_173 = ttnn.reshape(ttnn_to_layout_301, (1, 9, 1024), )
  ttnn_from_device_299 = ttnn.from_device(ttnn_reshape_173, )
  ttnn_to_layout_302 = ttnn.to_layout(ttnn_from_device_299, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_174 = ttnn.reshape(ttnn_to_layout_302, (9, 1024), )
  ttnn_from_device_300 = ttnn.from_device(ttnn_reshape_174, )
  ttnn_to_layout_303 = ttnn.to_layout(ttnn_from_device_300, ttnn.TILE_LAYOUT, )
  ttnn_to_device_142 = ttnn.to_device(ttnn_to_layout_303, device = device)
  ttnn_matmul_62 = ttnn.matmul(ttnn_to_device_142, ttnn_transpose_6, )
  ttnn_add_46 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_62, )
  ttnn_from_device_301 = ttnn.from_device(ttnn_add_46, )
  ttnn_to_layout_304 = ttnn.to_layout(ttnn_from_device_301, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_175 = ttnn.reshape(ttnn_to_layout_304, (1, 9, 1024), )
  ttnn_from_device_302 = ttnn.from_device(ttnn_reshape_175, )
  ttnn_to_layout_305 = ttnn.to_layout(ttnn_from_device_302, ttnn.TILE_LAYOUT, )
  ttnn_to_device_143 = ttnn.to_device(ttnn_to_layout_305, device = device)
  ttnn_prefix_clone_24 = clone_wrapper(ttnn_to_device_143, )
  ttnn_add_185 = ttnn.add(ttnn_layer_norm_14, ttnn_prefix_clone_24, )
  ttnn_layer_norm_15_ = ttnn.layer_norm(ttnn_add_185, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_15_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_185), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_15 = ttnn.from_torch(ttnn_layer_norm_15_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_15_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_15_), ttnn_layer_norm_15))
  ttnn_from_device_303 = ttnn.from_device(ttnn_layer_norm_15, )
  ttnn_to_layout_306 = ttnn.to_layout(ttnn_from_device_303, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_176 = ttnn.reshape(ttnn_to_layout_306, (9, 1024), )
  ttnn_from_device_304 = ttnn.from_device(ttnn_reshape_176, )
  ttnn_to_layout_307 = ttnn.to_layout(ttnn_from_device_304, ttnn.TILE_LAYOUT, )
  ttnn_to_device_144 = ttnn.to_device(ttnn_to_layout_307, device = device)
  ttnn_matmul_63 = ttnn.matmul(ttnn_to_device_144, ttnn_transpose_7, )
  ttnn_add_47 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_63, )
  ttnn_from_device_305 = ttnn.from_device(ttnn_add_47, )
  ttnn_to_layout_308 = ttnn.to_layout(ttnn_from_device_305, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_177 = ttnn.reshape(ttnn_to_layout_308, (1, 9, 4096), )
  ttnn_from_device_306 = ttnn.from_device(ttnn_reshape_177, )
  ttnn_to_layout_309 = ttnn.to_layout(ttnn_from_device_306, ttnn.TILE_LAYOUT, )
  ttnn_to_device_145 = ttnn.to_device(ttnn_to_layout_309, device = device)
  ttnn_multiply_37 = ttnn.multiply(ttnn_to_device_145, 0.5, )
  ttnn_pow_7 = ttnn.pow(ttnn_to_device_145, 3.0, )
  ttnn_multiply_38 = ttnn.multiply(ttnn_pow_7, 0.044715, )
  ttnn_add_186 = ttnn.add(ttnn_to_device_145, ttnn_multiply_38, )
  ttnn_multiply_39 = ttnn.multiply(ttnn_add_186, 0.7978845608028654, )
  ttnn_tanh_7_ = ttnn.tanh(ttnn_multiply_39, )
  ttnn_tanh_7_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_39), )
  ttnn_tanh_7 = ttnn.from_torch(ttnn_tanh_7_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_7_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_7_), ttnn_tanh_7))
  test_accuracy(tanh_7, ttnn_tanh_7)
  ttnn_add_187 = ttnn.add(ttnn_tanh_7, 1.0, )
  ttnn_multiply_40 = ttnn.multiply(ttnn_multiply_37, ttnn_add_187, )
  ttnn_from_device_307 = ttnn.from_device(ttnn_multiply_40, )
  ttnn_to_layout_310 = ttnn.to_layout(ttnn_from_device_307, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_178 = ttnn.reshape(ttnn_to_layout_310, (9, 4096), )
  ttnn_from_device_308 = ttnn.from_device(ttnn_reshape_178, )
  ttnn_to_layout_311 = ttnn.to_layout(ttnn_from_device_308, ttnn.TILE_LAYOUT, )
  ttnn_to_device_146 = ttnn.to_device(ttnn_to_layout_311, device = device)
  ttnn_matmul_64 = ttnn.matmul(ttnn_to_device_146, ttnn_transpose_8, )
  ttnn_add_48 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_64, )
  ttnn_from_device_309 = ttnn.from_device(ttnn_add_48, )
  ttnn_to_layout_312 = ttnn.to_layout(ttnn_from_device_309, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_179 = ttnn.reshape(ttnn_to_layout_312, (1, 9, 1024), )
  ttnn_from_device_310 = ttnn.from_device(ttnn_reshape_179, )
  ttnn_to_layout_313 = ttnn.to_layout(ttnn_from_device_310, ttnn.TILE_LAYOUT, )
  ttnn_to_device_147 = ttnn.to_device(ttnn_to_layout_313, device = device)
  ttnn_add_188 = ttnn.add(ttnn_to_device_147, ttnn_layer_norm_15, )
  ttnn_layer_norm_16_ = ttnn.layer_norm(ttnn_add_188, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_16_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_188), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_16 = ttnn.from_torch(ttnn_layer_norm_16_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_16_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_16_), ttnn_layer_norm_16))
  ttnn_from_device_311 = ttnn.from_device(ttnn_layer_norm_16, )
  ttnn_to_layout_314 = ttnn.to_layout(ttnn_from_device_311, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_180 = ttnn.reshape(ttnn_to_layout_314, (9, 1024), )
  ttnn_from_device_312 = ttnn.from_device(ttnn_reshape_180, )
  ttnn_to_layout_315 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_148 = ttnn.to_device(ttnn_to_layout_315, device = device)
  ttnn_matmul_65 = ttnn.matmul(ttnn_to_device_148, ttnn_transpose_1, )
  ttnn_add_49 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_65, )
  ttnn_from_device_313 = ttnn.from_device(ttnn_add_49, )
  ttnn_to_layout_316 = ttnn.to_layout(ttnn_from_device_313, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_181 = ttnn.reshape(ttnn_to_layout_316, (1, 9, 1024), )
  ttnn_to_layout_317 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_149 = ttnn.to_device(ttnn_to_layout_317, device = device)
  ttnn_matmul_66 = ttnn.matmul(ttnn_to_device_149, ttnn_transpose_2, )
  ttnn_add_50 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_66, )
  ttnn_from_device_315 = ttnn.from_device(ttnn_add_50, )
  ttnn_to_layout_318 = ttnn.to_layout(ttnn_from_device_315, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_183 = ttnn.reshape(ttnn_to_layout_318, (1, 9, 1024), )
  ttnn_to_layout_319 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_150 = ttnn.to_device(ttnn_to_layout_319, device = device)
  ttnn_matmul_67 = ttnn.matmul(ttnn_to_device_150, ttnn_transpose_3, )
  ttnn_add_51 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_67, )
  ttnn_from_device_317 = ttnn.from_device(ttnn_add_51, )
  ttnn_to_layout_320 = ttnn.to_layout(ttnn_from_device_317, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_185 = ttnn.reshape(ttnn_to_layout_320, (1, 9, 1024), )
  ttnn_from_device_318 = ttnn.from_device(ttnn_reshape_181, )
  ttnn_to_layout_321 = ttnn.to_layout(ttnn_from_device_318, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_186 = ttnn.reshape(ttnn_to_layout_321, (1, 9, 16, 64), )
  ttnn_from_device_319 = ttnn.from_device(ttnn_reshape_186, )
  ttnn_to_layout_322 = ttnn.to_layout(ttnn_from_device_319, ttnn.TILE_LAYOUT, )
  ttnn_to_device_151 = ttnn.to_device(ttnn_to_layout_322, device = device)
  ttnn_permute_24 = ttnn.permute(ttnn_to_device_151, (0, 2, 1, 3), )
  ttnn_from_device_320 = ttnn.from_device(ttnn_reshape_183, )
  ttnn_to_layout_323 = ttnn.to_layout(ttnn_from_device_320, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_187 = ttnn.reshape(ttnn_to_layout_323, (1, 9, 16, 64), )
  ttnn_from_device_321 = ttnn.from_device(ttnn_reshape_187, )
  ttnn_to_layout_324 = ttnn.to_layout(ttnn_from_device_321, ttnn.TILE_LAYOUT, )
  ttnn_to_device_152 = ttnn.to_device(ttnn_to_layout_324, device = device)
  ttnn_permute_25 = ttnn.permute(ttnn_to_device_152, (0, 2, 1, 3), )
  ttnn_from_device_322 = ttnn.from_device(ttnn_reshape_185, )
  ttnn_to_layout_325 = ttnn.to_layout(ttnn_from_device_322, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_188 = ttnn.reshape(ttnn_to_layout_325, (1, 9, 16, 64), )
  ttnn_from_device_323 = ttnn.from_device(ttnn_reshape_188, )
  ttnn_to_layout_326 = ttnn.to_layout(ttnn_from_device_323, ttnn.TILE_LAYOUT, )
  ttnn_to_device_153 = ttnn.to_device(ttnn_to_layout_326, device = device)
  ttnn_permute_26 = ttnn.permute(ttnn_to_device_153, (0, 2, 1, 3), )
  ttnn_transpose_68 = ttnn.transpose(ttnn_permute_25, 3, 2, )
  ttnn_from_device_324 = ttnn.from_device(ttnn_permute_24, )
  ttnn_to_layout_327 = ttnn.to_layout(ttnn_from_device_324, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_189 = ttnn.reshape(ttnn_to_layout_327, (16, 9, 64), )
  ttnn_from_device_325 = ttnn.from_device(ttnn_transpose_68, )
  ttnn_to_layout_328 = ttnn.to_layout(ttnn_from_device_325, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_190 = ttnn.reshape(ttnn_to_layout_328, (16, 64, 9), )
  ttnn_from_device_326 = ttnn.from_device(ttnn_reshape_189, )
  ttnn_to_layout_329 = ttnn.to_layout(ttnn_from_device_326, ttnn.TILE_LAYOUT, )
  ttnn_to_device_154 = ttnn.to_device(ttnn_to_layout_329, device = device)
  ttnn_from_device_327 = ttnn.from_device(ttnn_reshape_190, )
  ttnn_to_layout_330 = ttnn.to_layout(ttnn_from_device_327, ttnn.TILE_LAYOUT, )
  ttnn_to_device_155 = ttnn.to_device(ttnn_to_layout_330, device = device)
  ttnn_matmul_68 = ttnn.matmul(ttnn_to_device_154, ttnn_to_device_155, )
  ttnn_from_device_328 = ttnn.from_device(ttnn_matmul_68, )
  ttnn_to_layout_331 = ttnn.to_layout(ttnn_from_device_328, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_191 = ttnn.reshape(ttnn_to_layout_331, (1, 16, 9, 9), )
  ttnn_from_device_329 = ttnn.from_device(ttnn_reshape_191, )
  ttnn_to_layout_332 = ttnn.to_layout(ttnn_from_device_329, ttnn.TILE_LAYOUT, )
  ttnn_to_device_156 = ttnn.to_device(ttnn_to_layout_332, device = device)
  ttnn_multiply_41 = ttnn.multiply(ttnn_to_device_156, 0.125, )
  ttnn_add_189 = ttnn.add(ttnn_multiply_41, ttnn_multiply, )
  ttnn_softmax_8 = ttnn.softmax(ttnn_add_189, -1, numeric_stable = True)
  test_accuracy(_softmax_8, ttnn_softmax_8)
  ttnn_prefix_clone_25 = clone_wrapper(ttnn_softmax_8, )
  ttnn_from_device_330 = ttnn.from_device(ttnn_prefix_clone_25, )
  ttnn_to_layout_333 = ttnn.to_layout(ttnn_from_device_330, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_192 = ttnn.reshape(ttnn_to_layout_333, (16, 9, 9), )
  ttnn_from_device_331 = ttnn.from_device(ttnn_permute_26, )
  ttnn_to_layout_334 = ttnn.to_layout(ttnn_from_device_331, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_193 = ttnn.reshape(ttnn_to_layout_334, (16, 9, 64), )
  ttnn_from_device_332 = ttnn.from_device(ttnn_reshape_192, )
  ttnn_to_layout_335 = ttnn.to_layout(ttnn_from_device_332, ttnn.TILE_LAYOUT, )
  ttnn_to_device_157 = ttnn.to_device(ttnn_to_layout_335, device = device)
  ttnn_from_device_333 = ttnn.from_device(ttnn_reshape_193, )
  ttnn_to_layout_336 = ttnn.to_layout(ttnn_from_device_333, ttnn.TILE_LAYOUT, )
  ttnn_to_device_158 = ttnn.to_device(ttnn_to_layout_336, device = device)
  ttnn_matmul_69 = ttnn.matmul(ttnn_to_device_157, ttnn_to_device_158, )
  ttnn_from_device_334 = ttnn.from_device(ttnn_matmul_69, )
  ttnn_to_layout_337 = ttnn.to_layout(ttnn_from_device_334, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_194 = ttnn.reshape(ttnn_to_layout_337, (1, 16, 9, 64), )
  ttnn_from_device_335 = ttnn.from_device(ttnn_reshape_194, )
  ttnn_to_layout_338 = ttnn.to_layout(ttnn_from_device_335, ttnn.TILE_LAYOUT, )
  ttnn_to_device_159 = ttnn.to_device(ttnn_to_layout_338, device = device)
  ttnn_transpose_69 = ttnn.transpose(ttnn_to_device_159, 2, 1, )
  ttnn_prefix_clone_26 = clone_wrapper(ttnn_transpose_69, )
  ttnn_from_device_336 = ttnn.from_device(ttnn_prefix_clone_26, )
  ttnn_to_layout_339 = ttnn.to_layout(ttnn_from_device_336, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_195 = ttnn.reshape(ttnn_to_layout_339, (1, 9, 1024), )
  ttnn_from_device_337 = ttnn.from_device(ttnn_reshape_195, )
  ttnn_to_layout_340 = ttnn.to_layout(ttnn_from_device_337, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_196 = ttnn.reshape(ttnn_to_layout_340, (9, 1024), )
  ttnn_from_device_338 = ttnn.from_device(ttnn_reshape_196, )
  ttnn_to_layout_341 = ttnn.to_layout(ttnn_from_device_338, ttnn.TILE_LAYOUT, )
  ttnn_to_device_160 = ttnn.to_device(ttnn_to_layout_341, device = device)
  ttnn_matmul_70 = ttnn.matmul(ttnn_to_device_160, ttnn_transpose_6, )
  ttnn_add_52 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_70, )
  ttnn_from_device_339 = ttnn.from_device(ttnn_add_52, )
  ttnn_to_layout_342 = ttnn.to_layout(ttnn_from_device_339, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_197 = ttnn.reshape(ttnn_to_layout_342, (1, 9, 1024), )
  ttnn_from_device_340 = ttnn.from_device(ttnn_reshape_197, )
  ttnn_to_layout_343 = ttnn.to_layout(ttnn_from_device_340, ttnn.TILE_LAYOUT, )
  ttnn_to_device_161 = ttnn.to_device(ttnn_to_layout_343, device = device)
  ttnn_prefix_clone_27 = clone_wrapper(ttnn_to_device_161, )
  ttnn_add_190 = ttnn.add(ttnn_layer_norm_16, ttnn_prefix_clone_27, )
  ttnn_layer_norm_17_ = ttnn.layer_norm(ttnn_add_190, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_17_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_190), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_17 = ttnn.from_torch(ttnn_layer_norm_17_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_17_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_17_), ttnn_layer_norm_17))
  ttnn_from_device_341 = ttnn.from_device(ttnn_layer_norm_17, )
  ttnn_to_layout_344 = ttnn.to_layout(ttnn_from_device_341, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_198 = ttnn.reshape(ttnn_to_layout_344, (9, 1024), )
  ttnn_from_device_342 = ttnn.from_device(ttnn_reshape_198, )
  ttnn_to_layout_345 = ttnn.to_layout(ttnn_from_device_342, ttnn.TILE_LAYOUT, )
  ttnn_to_device_162 = ttnn.to_device(ttnn_to_layout_345, device = device)
  ttnn_matmul_71 = ttnn.matmul(ttnn_to_device_162, ttnn_transpose_7, )
  ttnn_add_53 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_71, )
  ttnn_from_device_343 = ttnn.from_device(ttnn_add_53, )
  ttnn_to_layout_346 = ttnn.to_layout(ttnn_from_device_343, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_199 = ttnn.reshape(ttnn_to_layout_346, (1, 9, 4096), )
  ttnn_from_device_344 = ttnn.from_device(ttnn_reshape_199, )
  ttnn_to_layout_347 = ttnn.to_layout(ttnn_from_device_344, ttnn.TILE_LAYOUT, )
  ttnn_to_device_163 = ttnn.to_device(ttnn_to_layout_347, device = device)
  ttnn_multiply_42 = ttnn.multiply(ttnn_to_device_163, 0.5, )
  ttnn_pow_8 = ttnn.pow(ttnn_to_device_163, 3.0, )
  ttnn_multiply_43 = ttnn.multiply(ttnn_pow_8, 0.044715, )
  ttnn_add_191 = ttnn.add(ttnn_to_device_163, ttnn_multiply_43, )
  ttnn_multiply_44 = ttnn.multiply(ttnn_add_191, 0.7978845608028654, )
  ttnn_tanh_8_ = ttnn.tanh(ttnn_multiply_44, )
  ttnn_tanh_8_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_44), )
  ttnn_tanh_8 = ttnn.from_torch(ttnn_tanh_8_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_8_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_8_), ttnn_tanh_8))
  test_accuracy(tanh_8, ttnn_tanh_8)
  ttnn_add_192 = ttnn.add(ttnn_tanh_8, 1.0, )
  ttnn_multiply_45 = ttnn.multiply(ttnn_multiply_42, ttnn_add_192, )
  ttnn_from_device_345 = ttnn.from_device(ttnn_multiply_45, )
  ttnn_to_layout_348 = ttnn.to_layout(ttnn_from_device_345, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_200 = ttnn.reshape(ttnn_to_layout_348, (9, 4096), )
  ttnn_from_device_346 = ttnn.from_device(ttnn_reshape_200, )
  ttnn_to_layout_349 = ttnn.to_layout(ttnn_from_device_346, ttnn.TILE_LAYOUT, )
  ttnn_to_device_164 = ttnn.to_device(ttnn_to_layout_349, device = device)
  ttnn_matmul_72 = ttnn.matmul(ttnn_to_device_164, ttnn_transpose_8, )
  ttnn_add_54 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_72, )
  ttnn_from_device_347 = ttnn.from_device(ttnn_add_54, )
  ttnn_to_layout_350 = ttnn.to_layout(ttnn_from_device_347, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_201 = ttnn.reshape(ttnn_to_layout_350, (1, 9, 1024), )
  ttnn_from_device_348 = ttnn.from_device(ttnn_reshape_201, )
  ttnn_to_layout_351 = ttnn.to_layout(ttnn_from_device_348, ttnn.TILE_LAYOUT, )
  ttnn_to_device_165 = ttnn.to_device(ttnn_to_layout_351, device = device)
  ttnn_add_193 = ttnn.add(ttnn_to_device_165, ttnn_layer_norm_17, )
  ttnn_layer_norm_18_ = ttnn.layer_norm(ttnn_add_193, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_18_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_193), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_18 = ttnn.from_torch(ttnn_layer_norm_18_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_18_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_18_), ttnn_layer_norm_18))
  ttnn_from_device_349 = ttnn.from_device(ttnn_layer_norm_18, )
  ttnn_to_layout_352 = ttnn.to_layout(ttnn_from_device_349, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_202 = ttnn.reshape(ttnn_to_layout_352, (9, 1024), )
  ttnn_from_device_350 = ttnn.from_device(ttnn_reshape_202, )
  ttnn_to_layout_353 = ttnn.to_layout(ttnn_from_device_350, ttnn.TILE_LAYOUT, )
  ttnn_to_device_166 = ttnn.to_device(ttnn_to_layout_353, device = device)
  ttnn_matmul_73 = ttnn.matmul(ttnn_to_device_166, ttnn_transpose_1, )
  ttnn_add_55 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_73, )
  ttnn_from_device_351 = ttnn.from_device(ttnn_add_55, )
  ttnn_to_layout_354 = ttnn.to_layout(ttnn_from_device_351, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_203 = ttnn.reshape(ttnn_to_layout_354, (1, 9, 1024), )
  ttnn_to_layout_355 = ttnn.to_layout(ttnn_from_device_350, ttnn.TILE_LAYOUT, )
  ttnn_to_device_167 = ttnn.to_device(ttnn_to_layout_355, device = device)
  ttnn_matmul_74 = ttnn.matmul(ttnn_to_device_167, ttnn_transpose_2, )
  ttnn_add_56 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_74, )
  ttnn_from_device_353 = ttnn.from_device(ttnn_add_56, )
  ttnn_to_layout_356 = ttnn.to_layout(ttnn_from_device_353, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_205 = ttnn.reshape(ttnn_to_layout_356, (1, 9, 1024), )
  ttnn_to_layout_357 = ttnn.to_layout(ttnn_from_device_350, ttnn.TILE_LAYOUT, )
  ttnn_to_device_168 = ttnn.to_device(ttnn_to_layout_357, device = device)
  ttnn_matmul_75 = ttnn.matmul(ttnn_to_device_168, ttnn_transpose_3, )
  ttnn_add_57 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_75, )
  ttnn_from_device_355 = ttnn.from_device(ttnn_add_57, )
  ttnn_to_layout_358 = ttnn.to_layout(ttnn_from_device_355, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_207 = ttnn.reshape(ttnn_to_layout_358, (1, 9, 1024), )
  ttnn_from_device_356 = ttnn.from_device(ttnn_reshape_203, )
  ttnn_to_layout_359 = ttnn.to_layout(ttnn_from_device_356, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_208 = ttnn.reshape(ttnn_to_layout_359, (1, 9, 16, 64), )
  ttnn_from_device_357 = ttnn.from_device(ttnn_reshape_208, )
  ttnn_to_layout_360 = ttnn.to_layout(ttnn_from_device_357, ttnn.TILE_LAYOUT, )
  ttnn_to_device_169 = ttnn.to_device(ttnn_to_layout_360, device = device)
  ttnn_permute_27 = ttnn.permute(ttnn_to_device_169, (0, 2, 1, 3), )
  ttnn_from_device_358 = ttnn.from_device(ttnn_reshape_205, )
  ttnn_to_layout_361 = ttnn.to_layout(ttnn_from_device_358, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_209 = ttnn.reshape(ttnn_to_layout_361, (1, 9, 16, 64), )
  ttnn_from_device_359 = ttnn.from_device(ttnn_reshape_209, )
  ttnn_to_layout_362 = ttnn.to_layout(ttnn_from_device_359, ttnn.TILE_LAYOUT, )
  ttnn_to_device_170 = ttnn.to_device(ttnn_to_layout_362, device = device)
  ttnn_permute_28 = ttnn.permute(ttnn_to_device_170, (0, 2, 1, 3), )
  ttnn_from_device_360 = ttnn.from_device(ttnn_reshape_207, )
  ttnn_to_layout_363 = ttnn.to_layout(ttnn_from_device_360, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_210 = ttnn.reshape(ttnn_to_layout_363, (1, 9, 16, 64), )
  ttnn_from_device_361 = ttnn.from_device(ttnn_reshape_210, )
  ttnn_to_layout_364 = ttnn.to_layout(ttnn_from_device_361, ttnn.TILE_LAYOUT, )
  ttnn_to_device_171 = ttnn.to_device(ttnn_to_layout_364, device = device)
  ttnn_permute_29 = ttnn.permute(ttnn_to_device_171, (0, 2, 1, 3), )
  ttnn_transpose_76 = ttnn.transpose(ttnn_permute_28, 3, 2, )
  ttnn_from_device_362 = ttnn.from_device(ttnn_permute_27, )
  ttnn_to_layout_365 = ttnn.to_layout(ttnn_from_device_362, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_211 = ttnn.reshape(ttnn_to_layout_365, (16, 9, 64), )
  ttnn_from_device_363 = ttnn.from_device(ttnn_transpose_76, )
  ttnn_to_layout_366 = ttnn.to_layout(ttnn_from_device_363, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_212 = ttnn.reshape(ttnn_to_layout_366, (16, 64, 9), )
  ttnn_from_device_364 = ttnn.from_device(ttnn_reshape_211, )
  ttnn_to_layout_367 = ttnn.to_layout(ttnn_from_device_364, ttnn.TILE_LAYOUT, )
  ttnn_to_device_172 = ttnn.to_device(ttnn_to_layout_367, device = device)
  ttnn_from_device_365 = ttnn.from_device(ttnn_reshape_212, )
  ttnn_to_layout_368 = ttnn.to_layout(ttnn_from_device_365, ttnn.TILE_LAYOUT, )
  ttnn_to_device_173 = ttnn.to_device(ttnn_to_layout_368, device = device)
  ttnn_matmul_76 = ttnn.matmul(ttnn_to_device_172, ttnn_to_device_173, )
  ttnn_from_device_366 = ttnn.from_device(ttnn_matmul_76, )
  ttnn_to_layout_369 = ttnn.to_layout(ttnn_from_device_366, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_213 = ttnn.reshape(ttnn_to_layout_369, (1, 16, 9, 9), )
  ttnn_from_device_367 = ttnn.from_device(ttnn_reshape_213, )
  ttnn_to_layout_370 = ttnn.to_layout(ttnn_from_device_367, ttnn.TILE_LAYOUT, )
  ttnn_to_device_174 = ttnn.to_device(ttnn_to_layout_370, device = device)
  ttnn_multiply_46 = ttnn.multiply(ttnn_to_device_174, 0.125, )
  ttnn_add_194 = ttnn.add(ttnn_multiply_46, ttnn_multiply, )
  ttnn_softmax_9 = ttnn.softmax(ttnn_add_194, -1, numeric_stable = True)
  test_accuracy(_softmax_9, ttnn_softmax_9)
  ttnn_prefix_clone_28 = clone_wrapper(ttnn_softmax_9, )
  ttnn_from_device_368 = ttnn.from_device(ttnn_prefix_clone_28, )
  ttnn_to_layout_371 = ttnn.to_layout(ttnn_from_device_368, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_214 = ttnn.reshape(ttnn_to_layout_371, (16, 9, 9), )
  ttnn_from_device_369 = ttnn.from_device(ttnn_permute_29, )
  ttnn_to_layout_372 = ttnn.to_layout(ttnn_from_device_369, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_215 = ttnn.reshape(ttnn_to_layout_372, (16, 9, 64), )
  ttnn_from_device_370 = ttnn.from_device(ttnn_reshape_214, )
  ttnn_to_layout_373 = ttnn.to_layout(ttnn_from_device_370, ttnn.TILE_LAYOUT, )
  ttnn_to_device_175 = ttnn.to_device(ttnn_to_layout_373, device = device)
  ttnn_from_device_371 = ttnn.from_device(ttnn_reshape_215, )
  ttnn_to_layout_374 = ttnn.to_layout(ttnn_from_device_371, ttnn.TILE_LAYOUT, )
  ttnn_to_device_176 = ttnn.to_device(ttnn_to_layout_374, device = device)
  ttnn_matmul_77 = ttnn.matmul(ttnn_to_device_175, ttnn_to_device_176, )
  ttnn_from_device_372 = ttnn.from_device(ttnn_matmul_77, )
  ttnn_to_layout_375 = ttnn.to_layout(ttnn_from_device_372, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_216 = ttnn.reshape(ttnn_to_layout_375, (1, 16, 9, 64), )
  ttnn_from_device_373 = ttnn.from_device(ttnn_reshape_216, )
  ttnn_to_layout_376 = ttnn.to_layout(ttnn_from_device_373, ttnn.TILE_LAYOUT, )
  ttnn_to_device_177 = ttnn.to_device(ttnn_to_layout_376, device = device)
  ttnn_transpose_77 = ttnn.transpose(ttnn_to_device_177, 2, 1, )
  ttnn_prefix_clone_29 = clone_wrapper(ttnn_transpose_77, )
  ttnn_from_device_374 = ttnn.from_device(ttnn_prefix_clone_29, )
  ttnn_to_layout_377 = ttnn.to_layout(ttnn_from_device_374, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_217 = ttnn.reshape(ttnn_to_layout_377, (1, 9, 1024), )
  ttnn_from_device_375 = ttnn.from_device(ttnn_reshape_217, )
  ttnn_to_layout_378 = ttnn.to_layout(ttnn_from_device_375, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_218 = ttnn.reshape(ttnn_to_layout_378, (9, 1024), )
  ttnn_from_device_376 = ttnn.from_device(ttnn_reshape_218, )
  ttnn_to_layout_379 = ttnn.to_layout(ttnn_from_device_376, ttnn.TILE_LAYOUT, )
  ttnn_to_device_178 = ttnn.to_device(ttnn_to_layout_379, device = device)
  ttnn_matmul_78 = ttnn.matmul(ttnn_to_device_178, ttnn_transpose_6, )
  ttnn_add_58 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_78, )
  ttnn_from_device_377 = ttnn.from_device(ttnn_add_58, )
  ttnn_to_layout_380 = ttnn.to_layout(ttnn_from_device_377, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_219 = ttnn.reshape(ttnn_to_layout_380, (1, 9, 1024), )
  ttnn_from_device_378 = ttnn.from_device(ttnn_reshape_219, )
  ttnn_to_layout_381 = ttnn.to_layout(ttnn_from_device_378, ttnn.TILE_LAYOUT, )
  ttnn_to_device_179 = ttnn.to_device(ttnn_to_layout_381, device = device)
  ttnn_prefix_clone_30 = clone_wrapper(ttnn_to_device_179, )
  ttnn_add_195 = ttnn.add(ttnn_layer_norm_18, ttnn_prefix_clone_30, )
  ttnn_layer_norm_19_ = ttnn.layer_norm(ttnn_add_195, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_19_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_195), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_19 = ttnn.from_torch(ttnn_layer_norm_19_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_19_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_19_), ttnn_layer_norm_19))
  ttnn_from_device_379 = ttnn.from_device(ttnn_layer_norm_19, )
  ttnn_to_layout_382 = ttnn.to_layout(ttnn_from_device_379, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_220 = ttnn.reshape(ttnn_to_layout_382, (9, 1024), )
  ttnn_from_device_380 = ttnn.from_device(ttnn_reshape_220, )
  ttnn_to_layout_383 = ttnn.to_layout(ttnn_from_device_380, ttnn.TILE_LAYOUT, )
  ttnn_to_device_180 = ttnn.to_device(ttnn_to_layout_383, device = device)
  ttnn_matmul_79 = ttnn.matmul(ttnn_to_device_180, ttnn_transpose_7, )
  ttnn_add_59 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_79, )
  ttnn_from_device_381 = ttnn.from_device(ttnn_add_59, )
  ttnn_to_layout_384 = ttnn.to_layout(ttnn_from_device_381, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_221 = ttnn.reshape(ttnn_to_layout_384, (1, 9, 4096), )
  ttnn_from_device_382 = ttnn.from_device(ttnn_reshape_221, )
  ttnn_to_layout_385 = ttnn.to_layout(ttnn_from_device_382, ttnn.TILE_LAYOUT, )
  ttnn_to_device_181 = ttnn.to_device(ttnn_to_layout_385, device = device)
  ttnn_multiply_47 = ttnn.multiply(ttnn_to_device_181, 0.5, )
  ttnn_pow_9 = ttnn.pow(ttnn_to_device_181, 3.0, )
  ttnn_multiply_48 = ttnn.multiply(ttnn_pow_9, 0.044715, )
  ttnn_add_196 = ttnn.add(ttnn_to_device_181, ttnn_multiply_48, )
  ttnn_multiply_49 = ttnn.multiply(ttnn_add_196, 0.7978845608028654, )
  ttnn_tanh_9_ = ttnn.tanh(ttnn_multiply_49, )
  ttnn_tanh_9_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_49), )
  ttnn_tanh_9 = ttnn.from_torch(ttnn_tanh_9_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_9_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_9_), ttnn_tanh_9))
  test_accuracy(tanh_9, ttnn_tanh_9)
  ttnn_add_197 = ttnn.add(ttnn_tanh_9, 1.0, )
  ttnn_multiply_50 = ttnn.multiply(ttnn_multiply_47, ttnn_add_197, )
  ttnn_from_device_383 = ttnn.from_device(ttnn_multiply_50, )
  ttnn_to_layout_386 = ttnn.to_layout(ttnn_from_device_383, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_222 = ttnn.reshape(ttnn_to_layout_386, (9, 4096), )
  ttnn_from_device_384 = ttnn.from_device(ttnn_reshape_222, )
  ttnn_to_layout_387 = ttnn.to_layout(ttnn_from_device_384, ttnn.TILE_LAYOUT, )
  ttnn_to_device_182 = ttnn.to_device(ttnn_to_layout_387, device = device)
  ttnn_matmul_80 = ttnn.matmul(ttnn_to_device_182, ttnn_transpose_8, )
  ttnn_add_60 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_80, )
  ttnn_from_device_385 = ttnn.from_device(ttnn_add_60, )
  ttnn_to_layout_388 = ttnn.to_layout(ttnn_from_device_385, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_223 = ttnn.reshape(ttnn_to_layout_388, (1, 9, 1024), )
  ttnn_from_device_386 = ttnn.from_device(ttnn_reshape_223, )
  ttnn_to_layout_389 = ttnn.to_layout(ttnn_from_device_386, ttnn.TILE_LAYOUT, )
  ttnn_to_device_183 = ttnn.to_device(ttnn_to_layout_389, device = device)
  ttnn_add_198 = ttnn.add(ttnn_to_device_183, ttnn_layer_norm_19, )
  ttnn_layer_norm_20_ = ttnn.layer_norm(ttnn_add_198, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_20_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_198), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_20 = ttnn.from_torch(ttnn_layer_norm_20_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_20_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_20_), ttnn_layer_norm_20))
  ttnn_from_device_387 = ttnn.from_device(ttnn_layer_norm_20, )
  ttnn_to_layout_390 = ttnn.to_layout(ttnn_from_device_387, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_224 = ttnn.reshape(ttnn_to_layout_390, (9, 1024), )
  ttnn_from_device_388 = ttnn.from_device(ttnn_reshape_224, )
  ttnn_to_layout_391 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_184 = ttnn.to_device(ttnn_to_layout_391, device = device)
  ttnn_matmul_81 = ttnn.matmul(ttnn_to_device_184, ttnn_transpose_1, )
  ttnn_add_61 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_81, )
  ttnn_from_device_389 = ttnn.from_device(ttnn_add_61, )
  ttnn_to_layout_392 = ttnn.to_layout(ttnn_from_device_389, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_225 = ttnn.reshape(ttnn_to_layout_392, (1, 9, 1024), )
  ttnn_to_layout_393 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_185 = ttnn.to_device(ttnn_to_layout_393, device = device)
  ttnn_matmul_82 = ttnn.matmul(ttnn_to_device_185, ttnn_transpose_2, )
  ttnn_add_62 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_82, )
  ttnn_from_device_391 = ttnn.from_device(ttnn_add_62, )
  ttnn_to_layout_394 = ttnn.to_layout(ttnn_from_device_391, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_227 = ttnn.reshape(ttnn_to_layout_394, (1, 9, 1024), )
  ttnn_to_layout_395 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_186 = ttnn.to_device(ttnn_to_layout_395, device = device)
  ttnn_matmul_83 = ttnn.matmul(ttnn_to_device_186, ttnn_transpose_3, )
  ttnn_add_63 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_83, )
  ttnn_from_device_393 = ttnn.from_device(ttnn_add_63, )
  ttnn_to_layout_396 = ttnn.to_layout(ttnn_from_device_393, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_229 = ttnn.reshape(ttnn_to_layout_396, (1, 9, 1024), )
  ttnn_from_device_394 = ttnn.from_device(ttnn_reshape_225, )
  ttnn_to_layout_397 = ttnn.to_layout(ttnn_from_device_394, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_230 = ttnn.reshape(ttnn_to_layout_397, (1, 9, 16, 64), )
  ttnn_from_device_395 = ttnn.from_device(ttnn_reshape_230, )
  ttnn_to_layout_398 = ttnn.to_layout(ttnn_from_device_395, ttnn.TILE_LAYOUT, )
  ttnn_to_device_187 = ttnn.to_device(ttnn_to_layout_398, device = device)
  ttnn_permute_30 = ttnn.permute(ttnn_to_device_187, (0, 2, 1, 3), )
  ttnn_from_device_396 = ttnn.from_device(ttnn_reshape_227, )
  ttnn_to_layout_399 = ttnn.to_layout(ttnn_from_device_396, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_231 = ttnn.reshape(ttnn_to_layout_399, (1, 9, 16, 64), )
  ttnn_from_device_397 = ttnn.from_device(ttnn_reshape_231, )
  ttnn_to_layout_400 = ttnn.to_layout(ttnn_from_device_397, ttnn.TILE_LAYOUT, )
  ttnn_to_device_188 = ttnn.to_device(ttnn_to_layout_400, device = device)
  ttnn_permute_31 = ttnn.permute(ttnn_to_device_188, (0, 2, 1, 3), )
  ttnn_from_device_398 = ttnn.from_device(ttnn_reshape_229, )
  ttnn_to_layout_401 = ttnn.to_layout(ttnn_from_device_398, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_232 = ttnn.reshape(ttnn_to_layout_401, (1, 9, 16, 64), )
  ttnn_from_device_399 = ttnn.from_device(ttnn_reshape_232, )
  ttnn_to_layout_402 = ttnn.to_layout(ttnn_from_device_399, ttnn.TILE_LAYOUT, )
  ttnn_to_device_189 = ttnn.to_device(ttnn_to_layout_402, device = device)
  ttnn_permute_32 = ttnn.permute(ttnn_to_device_189, (0, 2, 1, 3), )
  ttnn_transpose_84 = ttnn.transpose(ttnn_permute_31, 3, 2, )
  ttnn_from_device_400 = ttnn.from_device(ttnn_permute_30, )
  ttnn_to_layout_403 = ttnn.to_layout(ttnn_from_device_400, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_233 = ttnn.reshape(ttnn_to_layout_403, (16, 9, 64), )
  ttnn_from_device_401 = ttnn.from_device(ttnn_transpose_84, )
  ttnn_to_layout_404 = ttnn.to_layout(ttnn_from_device_401, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_234 = ttnn.reshape(ttnn_to_layout_404, (16, 64, 9), )
  ttnn_from_device_402 = ttnn.from_device(ttnn_reshape_233, )
  ttnn_to_layout_405 = ttnn.to_layout(ttnn_from_device_402, ttnn.TILE_LAYOUT, )
  ttnn_to_device_190 = ttnn.to_device(ttnn_to_layout_405, device = device)
  ttnn_from_device_403 = ttnn.from_device(ttnn_reshape_234, )
  ttnn_to_layout_406 = ttnn.to_layout(ttnn_from_device_403, ttnn.TILE_LAYOUT, )
  ttnn_to_device_191 = ttnn.to_device(ttnn_to_layout_406, device = device)
  ttnn_matmul_84 = ttnn.matmul(ttnn_to_device_190, ttnn_to_device_191, )
  ttnn_from_device_404 = ttnn.from_device(ttnn_matmul_84, )
  ttnn_to_layout_407 = ttnn.to_layout(ttnn_from_device_404, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_235 = ttnn.reshape(ttnn_to_layout_407, (1, 16, 9, 9), )
  ttnn_from_device_405 = ttnn.from_device(ttnn_reshape_235, )
  ttnn_to_layout_408 = ttnn.to_layout(ttnn_from_device_405, ttnn.TILE_LAYOUT, )
  ttnn_to_device_192 = ttnn.to_device(ttnn_to_layout_408, device = device)
  ttnn_multiply_51 = ttnn.multiply(ttnn_to_device_192, 0.125, )
  ttnn_add_199 = ttnn.add(ttnn_multiply_51, ttnn_multiply, )
  ttnn_softmax_10 = ttnn.softmax(ttnn_add_199, -1, numeric_stable = True)
  test_accuracy(_softmax_10, ttnn_softmax_10)
  ttnn_prefix_clone_31 = clone_wrapper(ttnn_softmax_10, )
  ttnn_from_device_406 = ttnn.from_device(ttnn_prefix_clone_31, )
  ttnn_to_layout_409 = ttnn.to_layout(ttnn_from_device_406, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_236 = ttnn.reshape(ttnn_to_layout_409, (16, 9, 9), )
  ttnn_from_device_407 = ttnn.from_device(ttnn_permute_32, )
  ttnn_to_layout_410 = ttnn.to_layout(ttnn_from_device_407, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_237 = ttnn.reshape(ttnn_to_layout_410, (16, 9, 64), )
  ttnn_from_device_408 = ttnn.from_device(ttnn_reshape_236, )
  ttnn_to_layout_411 = ttnn.to_layout(ttnn_from_device_408, ttnn.TILE_LAYOUT, )
  ttnn_to_device_193 = ttnn.to_device(ttnn_to_layout_411, device = device)
  ttnn_from_device_409 = ttnn.from_device(ttnn_reshape_237, )
  ttnn_to_layout_412 = ttnn.to_layout(ttnn_from_device_409, ttnn.TILE_LAYOUT, )
  ttnn_to_device_194 = ttnn.to_device(ttnn_to_layout_412, device = device)
  ttnn_matmul_85 = ttnn.matmul(ttnn_to_device_193, ttnn_to_device_194, )
  ttnn_from_device_410 = ttnn.from_device(ttnn_matmul_85, )
  ttnn_to_layout_413 = ttnn.to_layout(ttnn_from_device_410, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_238 = ttnn.reshape(ttnn_to_layout_413, (1, 16, 9, 64), )
  ttnn_from_device_411 = ttnn.from_device(ttnn_reshape_238, )
  ttnn_to_layout_414 = ttnn.to_layout(ttnn_from_device_411, ttnn.TILE_LAYOUT, )
  ttnn_to_device_195 = ttnn.to_device(ttnn_to_layout_414, device = device)
  ttnn_transpose_85 = ttnn.transpose(ttnn_to_device_195, 2, 1, )
  ttnn_prefix_clone_32 = clone_wrapper(ttnn_transpose_85, )
  ttnn_from_device_412 = ttnn.from_device(ttnn_prefix_clone_32, )
  ttnn_to_layout_415 = ttnn.to_layout(ttnn_from_device_412, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_239 = ttnn.reshape(ttnn_to_layout_415, (1, 9, 1024), )
  ttnn_from_device_413 = ttnn.from_device(ttnn_reshape_239, )
  ttnn_to_layout_416 = ttnn.to_layout(ttnn_from_device_413, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_240 = ttnn.reshape(ttnn_to_layout_416, (9, 1024), )
  ttnn_from_device_414 = ttnn.from_device(ttnn_reshape_240, )
  ttnn_to_layout_417 = ttnn.to_layout(ttnn_from_device_414, ttnn.TILE_LAYOUT, )
  ttnn_to_device_196 = ttnn.to_device(ttnn_to_layout_417, device = device)
  ttnn_matmul_86 = ttnn.matmul(ttnn_to_device_196, ttnn_transpose_6, )
  ttnn_add_64 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_86, )
  ttnn_from_device_415 = ttnn.from_device(ttnn_add_64, )
  ttnn_to_layout_418 = ttnn.to_layout(ttnn_from_device_415, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_241 = ttnn.reshape(ttnn_to_layout_418, (1, 9, 1024), )
  ttnn_from_device_416 = ttnn.from_device(ttnn_reshape_241, )
  ttnn_to_layout_419 = ttnn.to_layout(ttnn_from_device_416, ttnn.TILE_LAYOUT, )
  ttnn_to_device_197 = ttnn.to_device(ttnn_to_layout_419, device = device)
  ttnn_prefix_clone_33 = clone_wrapper(ttnn_to_device_197, )
  ttnn_add_200 = ttnn.add(ttnn_layer_norm_20, ttnn_prefix_clone_33, )
  ttnn_layer_norm_21_ = ttnn.layer_norm(ttnn_add_200, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_21_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_200), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_21 = ttnn.from_torch(ttnn_layer_norm_21_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_21_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_21_), ttnn_layer_norm_21))
  ttnn_from_device_417 = ttnn.from_device(ttnn_layer_norm_21, )
  ttnn_to_layout_420 = ttnn.to_layout(ttnn_from_device_417, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_242 = ttnn.reshape(ttnn_to_layout_420, (9, 1024), )
  ttnn_from_device_418 = ttnn.from_device(ttnn_reshape_242, )
  ttnn_to_layout_421 = ttnn.to_layout(ttnn_from_device_418, ttnn.TILE_LAYOUT, )
  ttnn_to_device_198 = ttnn.to_device(ttnn_to_layout_421, device = device)
  ttnn_matmul_87 = ttnn.matmul(ttnn_to_device_198, ttnn_transpose_7, )
  ttnn_add_65 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_87, )
  ttnn_from_device_419 = ttnn.from_device(ttnn_add_65, )
  ttnn_to_layout_422 = ttnn.to_layout(ttnn_from_device_419, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_243 = ttnn.reshape(ttnn_to_layout_422, (1, 9, 4096), )
  ttnn_from_device_420 = ttnn.from_device(ttnn_reshape_243, )
  ttnn_to_layout_423 = ttnn.to_layout(ttnn_from_device_420, ttnn.TILE_LAYOUT, )
  ttnn_to_device_199 = ttnn.to_device(ttnn_to_layout_423, device = device)
  ttnn_multiply_52 = ttnn.multiply(ttnn_to_device_199, 0.5, )
  ttnn_pow_10 = ttnn.pow(ttnn_to_device_199, 3.0, )
  ttnn_multiply_53 = ttnn.multiply(ttnn_pow_10, 0.044715, )
  ttnn_add_201 = ttnn.add(ttnn_to_device_199, ttnn_multiply_53, )
  ttnn_multiply_54 = ttnn.multiply(ttnn_add_201, 0.7978845608028654, )
  ttnn_tanh_10_ = ttnn.tanh(ttnn_multiply_54, )
  ttnn_tanh_10_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_54), )
  ttnn_tanh_10 = ttnn.from_torch(ttnn_tanh_10_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_10_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_10_), ttnn_tanh_10))
  test_accuracy(tanh_10, ttnn_tanh_10)
  ttnn_add_202 = ttnn.add(ttnn_tanh_10, 1.0, )
  ttnn_multiply_55 = ttnn.multiply(ttnn_multiply_52, ttnn_add_202, )
  ttnn_from_device_421 = ttnn.from_device(ttnn_multiply_55, )
  ttnn_to_layout_424 = ttnn.to_layout(ttnn_from_device_421, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_244 = ttnn.reshape(ttnn_to_layout_424, (9, 4096), )
  ttnn_from_device_422 = ttnn.from_device(ttnn_reshape_244, )
  ttnn_to_layout_425 = ttnn.to_layout(ttnn_from_device_422, ttnn.TILE_LAYOUT, )
  ttnn_to_device_200 = ttnn.to_device(ttnn_to_layout_425, device = device)
  ttnn_matmul_88 = ttnn.matmul(ttnn_to_device_200, ttnn_transpose_8, )
  ttnn_add_66 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_88, )
  ttnn_from_device_423 = ttnn.from_device(ttnn_add_66, )
  ttnn_to_layout_426 = ttnn.to_layout(ttnn_from_device_423, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_245 = ttnn.reshape(ttnn_to_layout_426, (1, 9, 1024), )
  ttnn_from_device_424 = ttnn.from_device(ttnn_reshape_245, )
  ttnn_to_layout_427 = ttnn.to_layout(ttnn_from_device_424, ttnn.TILE_LAYOUT, )
  ttnn_to_device_201 = ttnn.to_device(ttnn_to_layout_427, device = device)
  ttnn_add_203 = ttnn.add(ttnn_to_device_201, ttnn_layer_norm_21, )
  ttnn_layer_norm_22_ = ttnn.layer_norm(ttnn_add_203, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_22_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_203), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_22 = ttnn.from_torch(ttnn_layer_norm_22_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_22_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_22_), ttnn_layer_norm_22))
  ttnn_from_device_425 = ttnn.from_device(ttnn_layer_norm_22, )
  ttnn_to_layout_428 = ttnn.to_layout(ttnn_from_device_425, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_246 = ttnn.reshape(ttnn_to_layout_428, (9, 1024), )
  ttnn_from_device_426 = ttnn.from_device(ttnn_reshape_246, )
  ttnn_to_layout_429 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_202 = ttnn.to_device(ttnn_to_layout_429, device = device)
  ttnn_matmul_89 = ttnn.matmul(ttnn_to_device_202, ttnn_transpose_1, )
  ttnn_add_67 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_89, )
  ttnn_from_device_427 = ttnn.from_device(ttnn_add_67, )
  ttnn_to_layout_430 = ttnn.to_layout(ttnn_from_device_427, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_247 = ttnn.reshape(ttnn_to_layout_430, (1, 9, 1024), )
  ttnn_to_layout_431 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_203 = ttnn.to_device(ttnn_to_layout_431, device = device)
  ttnn_matmul_90 = ttnn.matmul(ttnn_to_device_203, ttnn_transpose_2, )
  ttnn_add_68 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_90, )
  ttnn_from_device_429 = ttnn.from_device(ttnn_add_68, )
  ttnn_to_layout_432 = ttnn.to_layout(ttnn_from_device_429, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_249 = ttnn.reshape(ttnn_to_layout_432, (1, 9, 1024), )
  ttnn_to_layout_433 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_204 = ttnn.to_device(ttnn_to_layout_433, device = device)
  ttnn_matmul_91 = ttnn.matmul(ttnn_to_device_204, ttnn_transpose_3, )
  ttnn_add_69 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_91, )
  ttnn_from_device_431 = ttnn.from_device(ttnn_add_69, )
  ttnn_to_layout_434 = ttnn.to_layout(ttnn_from_device_431, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_251 = ttnn.reshape(ttnn_to_layout_434, (1, 9, 1024), )
  ttnn_from_device_432 = ttnn.from_device(ttnn_reshape_247, )
  ttnn_to_layout_435 = ttnn.to_layout(ttnn_from_device_432, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_252 = ttnn.reshape(ttnn_to_layout_435, (1, 9, 16, 64), )
  ttnn_from_device_433 = ttnn.from_device(ttnn_reshape_252, )
  ttnn_to_layout_436 = ttnn.to_layout(ttnn_from_device_433, ttnn.TILE_LAYOUT, )
  ttnn_to_device_205 = ttnn.to_device(ttnn_to_layout_436, device = device)
  ttnn_permute_33 = ttnn.permute(ttnn_to_device_205, (0, 2, 1, 3), )
  ttnn_from_device_434 = ttnn.from_device(ttnn_reshape_249, )
  ttnn_to_layout_437 = ttnn.to_layout(ttnn_from_device_434, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_253 = ttnn.reshape(ttnn_to_layout_437, (1, 9, 16, 64), )
  ttnn_from_device_435 = ttnn.from_device(ttnn_reshape_253, )
  ttnn_to_layout_438 = ttnn.to_layout(ttnn_from_device_435, ttnn.TILE_LAYOUT, )
  ttnn_to_device_206 = ttnn.to_device(ttnn_to_layout_438, device = device)
  ttnn_permute_34 = ttnn.permute(ttnn_to_device_206, (0, 2, 1, 3), )
  ttnn_from_device_436 = ttnn.from_device(ttnn_reshape_251, )
  ttnn_to_layout_439 = ttnn.to_layout(ttnn_from_device_436, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_254 = ttnn.reshape(ttnn_to_layout_439, (1, 9, 16, 64), )
  ttnn_from_device_437 = ttnn.from_device(ttnn_reshape_254, )
  ttnn_to_layout_440 = ttnn.to_layout(ttnn_from_device_437, ttnn.TILE_LAYOUT, )
  ttnn_to_device_207 = ttnn.to_device(ttnn_to_layout_440, device = device)
  ttnn_permute_35 = ttnn.permute(ttnn_to_device_207, (0, 2, 1, 3), )
  ttnn_transpose_92 = ttnn.transpose(ttnn_permute_34, 3, 2, )
  ttnn_from_device_438 = ttnn.from_device(ttnn_permute_33, )
  ttnn_to_layout_441 = ttnn.to_layout(ttnn_from_device_438, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_255 = ttnn.reshape(ttnn_to_layout_441, (16, 9, 64), )
  ttnn_from_device_439 = ttnn.from_device(ttnn_transpose_92, )
  ttnn_to_layout_442 = ttnn.to_layout(ttnn_from_device_439, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_256 = ttnn.reshape(ttnn_to_layout_442, (16, 64, 9), )
  ttnn_from_device_440 = ttnn.from_device(ttnn_reshape_255, )
  ttnn_to_layout_443 = ttnn.to_layout(ttnn_from_device_440, ttnn.TILE_LAYOUT, )
  ttnn_to_device_208 = ttnn.to_device(ttnn_to_layout_443, device = device)
  ttnn_from_device_441 = ttnn.from_device(ttnn_reshape_256, )
  ttnn_to_layout_444 = ttnn.to_layout(ttnn_from_device_441, ttnn.TILE_LAYOUT, )
  ttnn_to_device_209 = ttnn.to_device(ttnn_to_layout_444, device = device)
  ttnn_matmul_92 = ttnn.matmul(ttnn_to_device_208, ttnn_to_device_209, )
  ttnn_from_device_442 = ttnn.from_device(ttnn_matmul_92, )
  ttnn_to_layout_445 = ttnn.to_layout(ttnn_from_device_442, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_257 = ttnn.reshape(ttnn_to_layout_445, (1, 16, 9, 9), )
  ttnn_from_device_443 = ttnn.from_device(ttnn_reshape_257, )
  ttnn_to_layout_446 = ttnn.to_layout(ttnn_from_device_443, ttnn.TILE_LAYOUT, )
  ttnn_to_device_210 = ttnn.to_device(ttnn_to_layout_446, device = device)
  ttnn_multiply_56 = ttnn.multiply(ttnn_to_device_210, 0.125, )
  ttnn_add_204 = ttnn.add(ttnn_multiply_56, ttnn_multiply, )
  ttnn_softmax_11 = ttnn.softmax(ttnn_add_204, -1, numeric_stable = True)
  test_accuracy(_softmax_11, ttnn_softmax_11)
  ttnn_prefix_clone_34 = clone_wrapper(ttnn_softmax_11, )
  ttnn_from_device_444 = ttnn.from_device(ttnn_prefix_clone_34, )
  ttnn_to_layout_447 = ttnn.to_layout(ttnn_from_device_444, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_258 = ttnn.reshape(ttnn_to_layout_447, (16, 9, 9), )
  ttnn_from_device_445 = ttnn.from_device(ttnn_permute_35, )
  ttnn_to_layout_448 = ttnn.to_layout(ttnn_from_device_445, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_259 = ttnn.reshape(ttnn_to_layout_448, (16, 9, 64), )
  ttnn_from_device_446 = ttnn.from_device(ttnn_reshape_258, )
  ttnn_to_layout_449 = ttnn.to_layout(ttnn_from_device_446, ttnn.TILE_LAYOUT, )
  ttnn_to_device_211 = ttnn.to_device(ttnn_to_layout_449, device = device)
  ttnn_from_device_447 = ttnn.from_device(ttnn_reshape_259, )
  ttnn_to_layout_450 = ttnn.to_layout(ttnn_from_device_447, ttnn.TILE_LAYOUT, )
  ttnn_to_device_212 = ttnn.to_device(ttnn_to_layout_450, device = device)
  ttnn_matmul_93 = ttnn.matmul(ttnn_to_device_211, ttnn_to_device_212, )
  ttnn_from_device_448 = ttnn.from_device(ttnn_matmul_93, )
  ttnn_to_layout_451 = ttnn.to_layout(ttnn_from_device_448, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_260 = ttnn.reshape(ttnn_to_layout_451, (1, 16, 9, 64), )
  ttnn_from_device_449 = ttnn.from_device(ttnn_reshape_260, )
  ttnn_to_layout_452 = ttnn.to_layout(ttnn_from_device_449, ttnn.TILE_LAYOUT, )
  ttnn_to_device_213 = ttnn.to_device(ttnn_to_layout_452, device = device)
  ttnn_transpose_93 = ttnn.transpose(ttnn_to_device_213, 2, 1, )
  ttnn_prefix_clone_35 = clone_wrapper(ttnn_transpose_93, )
  ttnn_from_device_450 = ttnn.from_device(ttnn_prefix_clone_35, )
  ttnn_to_layout_453 = ttnn.to_layout(ttnn_from_device_450, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_261 = ttnn.reshape(ttnn_to_layout_453, (1, 9, 1024), )
  ttnn_from_device_451 = ttnn.from_device(ttnn_reshape_261, )
  ttnn_to_layout_454 = ttnn.to_layout(ttnn_from_device_451, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_262 = ttnn.reshape(ttnn_to_layout_454, (9, 1024), )
  ttnn_from_device_452 = ttnn.from_device(ttnn_reshape_262, )
  ttnn_to_layout_455 = ttnn.to_layout(ttnn_from_device_452, ttnn.TILE_LAYOUT, )
  ttnn_to_device_214 = ttnn.to_device(ttnn_to_layout_455, device = device)
  ttnn_matmul_94 = ttnn.matmul(ttnn_to_device_214, ttnn_transpose_6, )
  ttnn_add_70 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_94, )
  ttnn_from_device_453 = ttnn.from_device(ttnn_add_70, )
  ttnn_to_layout_456 = ttnn.to_layout(ttnn_from_device_453, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_263 = ttnn.reshape(ttnn_to_layout_456, (1, 9, 1024), )
  ttnn_from_device_454 = ttnn.from_device(ttnn_reshape_263, )
  ttnn_to_layout_457 = ttnn.to_layout(ttnn_from_device_454, ttnn.TILE_LAYOUT, )
  ttnn_to_device_215 = ttnn.to_device(ttnn_to_layout_457, device = device)
  ttnn_prefix_clone_36 = clone_wrapper(ttnn_to_device_215, )
  ttnn_add_205 = ttnn.add(ttnn_layer_norm_22, ttnn_prefix_clone_36, )
  ttnn_layer_norm_23_ = ttnn.layer_norm(ttnn_add_205, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_23_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_205), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_23 = ttnn.from_torch(ttnn_layer_norm_23_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_23_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_23_), ttnn_layer_norm_23))
  ttnn_from_device_455 = ttnn.from_device(ttnn_layer_norm_23, )
  ttnn_to_layout_458 = ttnn.to_layout(ttnn_from_device_455, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_264 = ttnn.reshape(ttnn_to_layout_458, (9, 1024), )
  ttnn_from_device_456 = ttnn.from_device(ttnn_reshape_264, )
  ttnn_to_layout_459 = ttnn.to_layout(ttnn_from_device_456, ttnn.TILE_LAYOUT, )
  ttnn_to_device_216 = ttnn.to_device(ttnn_to_layout_459, device = device)
  ttnn_matmul_95 = ttnn.matmul(ttnn_to_device_216, ttnn_transpose_7, )
  ttnn_add_71 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_95, )
  ttnn_from_device_457 = ttnn.from_device(ttnn_add_71, )
  ttnn_to_layout_460 = ttnn.to_layout(ttnn_from_device_457, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_265 = ttnn.reshape(ttnn_to_layout_460, (1, 9, 4096), )
  ttnn_from_device_458 = ttnn.from_device(ttnn_reshape_265, )
  ttnn_to_layout_461 = ttnn.to_layout(ttnn_from_device_458, ttnn.TILE_LAYOUT, )
  ttnn_to_device_217 = ttnn.to_device(ttnn_to_layout_461, device = device)
  ttnn_multiply_57 = ttnn.multiply(ttnn_to_device_217, 0.5, )
  ttnn_pow_11 = ttnn.pow(ttnn_to_device_217, 3.0, )
  ttnn_multiply_58 = ttnn.multiply(ttnn_pow_11, 0.044715, )
  ttnn_add_206 = ttnn.add(ttnn_to_device_217, ttnn_multiply_58, )
  ttnn_multiply_59 = ttnn.multiply(ttnn_add_206, 0.7978845608028654, )
  ttnn_tanh_11_ = ttnn.tanh(ttnn_multiply_59, )
  ttnn_tanh_11_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_59), )
  ttnn_tanh_11 = ttnn.from_torch(ttnn_tanh_11_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_11_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_11_), ttnn_tanh_11))
  test_accuracy(tanh_11, ttnn_tanh_11)
  ttnn_add_207 = ttnn.add(ttnn_tanh_11, 1.0, )
  ttnn_multiply_60 = ttnn.multiply(ttnn_multiply_57, ttnn_add_207, )
  ttnn_from_device_459 = ttnn.from_device(ttnn_multiply_60, )
  ttnn_to_layout_462 = ttnn.to_layout(ttnn_from_device_459, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_266 = ttnn.reshape(ttnn_to_layout_462, (9, 4096), )
  ttnn_from_device_460 = ttnn.from_device(ttnn_reshape_266, )
  ttnn_to_layout_463 = ttnn.to_layout(ttnn_from_device_460, ttnn.TILE_LAYOUT, )
  ttnn_to_device_218 = ttnn.to_device(ttnn_to_layout_463, device = device)
  ttnn_matmul_96 = ttnn.matmul(ttnn_to_device_218, ttnn_transpose_8, )
  ttnn_add_72 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_96, )
  ttnn_from_device_461 = ttnn.from_device(ttnn_add_72, )
  ttnn_to_layout_464 = ttnn.to_layout(ttnn_from_device_461, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_267 = ttnn.reshape(ttnn_to_layout_464, (1, 9, 1024), )
  ttnn_from_device_462 = ttnn.from_device(ttnn_reshape_267, )
  ttnn_to_layout_465 = ttnn.to_layout(ttnn_from_device_462, ttnn.TILE_LAYOUT, )
  ttnn_to_device_219 = ttnn.to_device(ttnn_to_layout_465, device = device)
  ttnn_add_208 = ttnn.add(ttnn_to_device_219, ttnn_layer_norm_23, )
  ttnn_layer_norm_24_ = ttnn.layer_norm(ttnn_add_208, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_24_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_208), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_24 = ttnn.from_torch(ttnn_layer_norm_24_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_24_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_24_), ttnn_layer_norm_24))
  ttnn_from_device_463 = ttnn.from_device(ttnn_layer_norm_24, )
  ttnn_to_layout_466 = ttnn.to_layout(ttnn_from_device_463, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_268 = ttnn.reshape(ttnn_to_layout_466, (9, 1024), )
  ttnn_from_device_464 = ttnn.from_device(ttnn_reshape_268, )
  ttnn_to_layout_467 = ttnn.to_layout(ttnn_from_device_464, ttnn.TILE_LAYOUT, )
  ttnn_to_device_220 = ttnn.to_device(ttnn_to_layout_467, device = device)
  ttnn_matmul_97 = ttnn.matmul(ttnn_to_device_220, ttnn_transpose_1, )
  ttnn_add_73 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_97, )
  ttnn_from_device_465 = ttnn.from_device(ttnn_add_73, )
  ttnn_to_layout_468 = ttnn.to_layout(ttnn_from_device_465, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_269 = ttnn.reshape(ttnn_to_layout_468, (1, 9, 1024), )
  ttnn_to_layout_469 = ttnn.to_layout(ttnn_from_device_464, ttnn.TILE_LAYOUT, )
  ttnn_to_device_221 = ttnn.to_device(ttnn_to_layout_469, device = device)
  ttnn_matmul_98 = ttnn.matmul(ttnn_to_device_221, ttnn_transpose_2, )
  ttnn_add_74 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_98, )
  ttnn_from_device_467 = ttnn.from_device(ttnn_add_74, )
  ttnn_to_layout_470 = ttnn.to_layout(ttnn_from_device_467, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_271 = ttnn.reshape(ttnn_to_layout_470, (1, 9, 1024), )
  ttnn_to_layout_471 = ttnn.to_layout(ttnn_from_device_464, ttnn.TILE_LAYOUT, )
  ttnn_to_device_222 = ttnn.to_device(ttnn_to_layout_471, device = device)
  ttnn_matmul_99 = ttnn.matmul(ttnn_to_device_222, ttnn_transpose_3, )
  ttnn_add_75 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_99, )
  ttnn_from_device_469 = ttnn.from_device(ttnn_add_75, )
  ttnn_to_layout_472 = ttnn.to_layout(ttnn_from_device_469, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_273 = ttnn.reshape(ttnn_to_layout_472, (1, 9, 1024), )
  ttnn_from_device_470 = ttnn.from_device(ttnn_reshape_269, )
  ttnn_to_layout_473 = ttnn.to_layout(ttnn_from_device_470, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_274 = ttnn.reshape(ttnn_to_layout_473, (1, 9, 16, 64), )
  ttnn_from_device_471 = ttnn.from_device(ttnn_reshape_274, )
  ttnn_to_layout_474 = ttnn.to_layout(ttnn_from_device_471, ttnn.TILE_LAYOUT, )
  ttnn_to_device_223 = ttnn.to_device(ttnn_to_layout_474, device = device)
  ttnn_permute_36 = ttnn.permute(ttnn_to_device_223, (0, 2, 1, 3), )
  ttnn_from_device_472 = ttnn.from_device(ttnn_reshape_271, )
  ttnn_to_layout_475 = ttnn.to_layout(ttnn_from_device_472, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_275 = ttnn.reshape(ttnn_to_layout_475, (1, 9, 16, 64), )
  ttnn_from_device_473 = ttnn.from_device(ttnn_reshape_275, )
  ttnn_to_layout_476 = ttnn.to_layout(ttnn_from_device_473, ttnn.TILE_LAYOUT, )
  ttnn_to_device_224 = ttnn.to_device(ttnn_to_layout_476, device = device)
  ttnn_permute_37 = ttnn.permute(ttnn_to_device_224, (0, 2, 1, 3), )
  ttnn_from_device_474 = ttnn.from_device(ttnn_reshape_273, )
  ttnn_to_layout_477 = ttnn.to_layout(ttnn_from_device_474, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_276 = ttnn.reshape(ttnn_to_layout_477, (1, 9, 16, 64), )
  ttnn_from_device_475 = ttnn.from_device(ttnn_reshape_276, )
  ttnn_to_layout_478 = ttnn.to_layout(ttnn_from_device_475, ttnn.TILE_LAYOUT, )
  ttnn_to_device_225 = ttnn.to_device(ttnn_to_layout_478, device = device)
  ttnn_permute_38 = ttnn.permute(ttnn_to_device_225, (0, 2, 1, 3), )
  ttnn_transpose_100 = ttnn.transpose(ttnn_permute_37, 3, 2, )
  ttnn_from_device_476 = ttnn.from_device(ttnn_permute_36, )
  ttnn_to_layout_479 = ttnn.to_layout(ttnn_from_device_476, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_277 = ttnn.reshape(ttnn_to_layout_479, (16, 9, 64), )
  ttnn_from_device_477 = ttnn.from_device(ttnn_transpose_100, )
  ttnn_to_layout_480 = ttnn.to_layout(ttnn_from_device_477, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_278 = ttnn.reshape(ttnn_to_layout_480, (16, 64, 9), )
  ttnn_from_device_478 = ttnn.from_device(ttnn_reshape_277, )
  ttnn_to_layout_481 = ttnn.to_layout(ttnn_from_device_478, ttnn.TILE_LAYOUT, )
  ttnn_to_device_226 = ttnn.to_device(ttnn_to_layout_481, device = device)
  ttnn_from_device_479 = ttnn.from_device(ttnn_reshape_278, )
  ttnn_to_layout_482 = ttnn.to_layout(ttnn_from_device_479, ttnn.TILE_LAYOUT, )
  ttnn_to_device_227 = ttnn.to_device(ttnn_to_layout_482, device = device)
  ttnn_matmul_100 = ttnn.matmul(ttnn_to_device_226, ttnn_to_device_227, )
  ttnn_from_device_480 = ttnn.from_device(ttnn_matmul_100, )
  ttnn_to_layout_483 = ttnn.to_layout(ttnn_from_device_480, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_279 = ttnn.reshape(ttnn_to_layout_483, (1, 16, 9, 9), )
  ttnn_from_device_481 = ttnn.from_device(ttnn_reshape_279, )
  ttnn_to_layout_484 = ttnn.to_layout(ttnn_from_device_481, ttnn.TILE_LAYOUT, )
  ttnn_to_device_228 = ttnn.to_device(ttnn_to_layout_484, device = device)
  ttnn_multiply_61 = ttnn.multiply(ttnn_to_device_228, 0.125, )
  ttnn_add_209 = ttnn.add(ttnn_multiply_61, ttnn_multiply, )
  ttnn_softmax_12 = ttnn.softmax(ttnn_add_209, -1, numeric_stable = True)
  test_accuracy(_softmax_12, ttnn_softmax_12)
  ttnn_prefix_clone_37 = clone_wrapper(ttnn_softmax_12, )
  ttnn_from_device_482 = ttnn.from_device(ttnn_prefix_clone_37, )
  ttnn_to_layout_485 = ttnn.to_layout(ttnn_from_device_482, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_280 = ttnn.reshape(ttnn_to_layout_485, (16, 9, 9), )
  ttnn_from_device_483 = ttnn.from_device(ttnn_permute_38, )
  ttnn_to_layout_486 = ttnn.to_layout(ttnn_from_device_483, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_281 = ttnn.reshape(ttnn_to_layout_486, (16, 9, 64), )
  ttnn_from_device_484 = ttnn.from_device(ttnn_reshape_280, )
  ttnn_to_layout_487 = ttnn.to_layout(ttnn_from_device_484, ttnn.TILE_LAYOUT, )
  ttnn_to_device_229 = ttnn.to_device(ttnn_to_layout_487, device = device)
  ttnn_from_device_485 = ttnn.from_device(ttnn_reshape_281, )
  ttnn_to_layout_488 = ttnn.to_layout(ttnn_from_device_485, ttnn.TILE_LAYOUT, )
  ttnn_to_device_230 = ttnn.to_device(ttnn_to_layout_488, device = device)
  ttnn_matmul_101 = ttnn.matmul(ttnn_to_device_229, ttnn_to_device_230, )
  ttnn_from_device_486 = ttnn.from_device(ttnn_matmul_101, )
  ttnn_to_layout_489 = ttnn.to_layout(ttnn_from_device_486, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_282 = ttnn.reshape(ttnn_to_layout_489, (1, 16, 9, 64), )
  ttnn_from_device_487 = ttnn.from_device(ttnn_reshape_282, )
  ttnn_to_layout_490 = ttnn.to_layout(ttnn_from_device_487, ttnn.TILE_LAYOUT, )
  ttnn_to_device_231 = ttnn.to_device(ttnn_to_layout_490, device = device)
  ttnn_transpose_101 = ttnn.transpose(ttnn_to_device_231, 2, 1, )
  ttnn_prefix_clone_38 = clone_wrapper(ttnn_transpose_101, )
  ttnn_from_device_488 = ttnn.from_device(ttnn_prefix_clone_38, )
  ttnn_to_layout_491 = ttnn.to_layout(ttnn_from_device_488, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_283 = ttnn.reshape(ttnn_to_layout_491, (1, 9, 1024), )
  ttnn_from_device_489 = ttnn.from_device(ttnn_reshape_283, )
  ttnn_to_layout_492 = ttnn.to_layout(ttnn_from_device_489, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_284 = ttnn.reshape(ttnn_to_layout_492, (9, 1024), )
  ttnn_from_device_490 = ttnn.from_device(ttnn_reshape_284, )
  ttnn_to_layout_493 = ttnn.to_layout(ttnn_from_device_490, ttnn.TILE_LAYOUT, )
  ttnn_to_device_232 = ttnn.to_device(ttnn_to_layout_493, device = device)
  ttnn_matmul_102 = ttnn.matmul(ttnn_to_device_232, ttnn_transpose_6, )
  ttnn_add_76 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_102, )
  ttnn_from_device_491 = ttnn.from_device(ttnn_add_76, )
  ttnn_to_layout_494 = ttnn.to_layout(ttnn_from_device_491, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_285 = ttnn.reshape(ttnn_to_layout_494, (1, 9, 1024), )
  ttnn_from_device_492 = ttnn.from_device(ttnn_reshape_285, )
  ttnn_to_layout_495 = ttnn.to_layout(ttnn_from_device_492, ttnn.TILE_LAYOUT, )
  ttnn_to_device_233 = ttnn.to_device(ttnn_to_layout_495, device = device)
  ttnn_prefix_clone_39 = clone_wrapper(ttnn_to_device_233, )
  ttnn_add_210 = ttnn.add(ttnn_layer_norm_24, ttnn_prefix_clone_39, )
  ttnn_layer_norm_25_ = ttnn.layer_norm(ttnn_add_210, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_25_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_210), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_25 = ttnn.from_torch(ttnn_layer_norm_25_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_25_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_25_), ttnn_layer_norm_25))
  ttnn_from_device_493 = ttnn.from_device(ttnn_layer_norm_25, )
  ttnn_to_layout_496 = ttnn.to_layout(ttnn_from_device_493, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_286 = ttnn.reshape(ttnn_to_layout_496, (9, 1024), )
  ttnn_from_device_494 = ttnn.from_device(ttnn_reshape_286, )
  ttnn_to_layout_497 = ttnn.to_layout(ttnn_from_device_494, ttnn.TILE_LAYOUT, )
  ttnn_to_device_234 = ttnn.to_device(ttnn_to_layout_497, device = device)
  ttnn_matmul_103 = ttnn.matmul(ttnn_to_device_234, ttnn_transpose_7, )
  ttnn_add_77 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_103, )
  ttnn_from_device_495 = ttnn.from_device(ttnn_add_77, )
  ttnn_to_layout_498 = ttnn.to_layout(ttnn_from_device_495, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_287 = ttnn.reshape(ttnn_to_layout_498, (1, 9, 4096), )
  ttnn_from_device_496 = ttnn.from_device(ttnn_reshape_287, )
  ttnn_to_layout_499 = ttnn.to_layout(ttnn_from_device_496, ttnn.TILE_LAYOUT, )
  ttnn_to_device_235 = ttnn.to_device(ttnn_to_layout_499, device = device)
  ttnn_multiply_62 = ttnn.multiply(ttnn_to_device_235, 0.5, )
  ttnn_pow_12 = ttnn.pow(ttnn_to_device_235, 3.0, )
  ttnn_multiply_63 = ttnn.multiply(ttnn_pow_12, 0.044715, )
  ttnn_add_211 = ttnn.add(ttnn_to_device_235, ttnn_multiply_63, )
  ttnn_multiply_64 = ttnn.multiply(ttnn_add_211, 0.7978845608028654, )
  ttnn_tanh_12_ = ttnn.tanh(ttnn_multiply_64, )
  ttnn_tanh_12_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_64), )
  ttnn_tanh_12 = ttnn.from_torch(ttnn_tanh_12_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_12_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_12_), ttnn_tanh_12))
  test_accuracy(tanh_12, ttnn_tanh_12)
  ttnn_add_212 = ttnn.add(ttnn_tanh_12, 1.0, )
  ttnn_multiply_65 = ttnn.multiply(ttnn_multiply_62, ttnn_add_212, )
  ttnn_from_device_497 = ttnn.from_device(ttnn_multiply_65, )
  ttnn_to_layout_500 = ttnn.to_layout(ttnn_from_device_497, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_288 = ttnn.reshape(ttnn_to_layout_500, (9, 4096), )
  ttnn_from_device_498 = ttnn.from_device(ttnn_reshape_288, )
  ttnn_to_layout_501 = ttnn.to_layout(ttnn_from_device_498, ttnn.TILE_LAYOUT, )
  ttnn_to_device_236 = ttnn.to_device(ttnn_to_layout_501, device = device)
  ttnn_matmul_104 = ttnn.matmul(ttnn_to_device_236, ttnn_transpose_8, )
  ttnn_add_78 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_104, )
  ttnn_from_device_499 = ttnn.from_device(ttnn_add_78, )
  ttnn_to_layout_502 = ttnn.to_layout(ttnn_from_device_499, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_289 = ttnn.reshape(ttnn_to_layout_502, (1, 9, 1024), )
  ttnn_from_device_500 = ttnn.from_device(ttnn_reshape_289, )
  ttnn_to_layout_503 = ttnn.to_layout(ttnn_from_device_500, ttnn.TILE_LAYOUT, )
  ttnn_to_device_237 = ttnn.to_device(ttnn_to_layout_503, device = device)
  ttnn_add_213 = ttnn.add(ttnn_to_device_237, ttnn_layer_norm_25, )
  ttnn_layer_norm_26_ = ttnn.layer_norm(ttnn_add_213, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_26_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_213), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_26 = ttnn.from_torch(ttnn_layer_norm_26_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_26_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_26_), ttnn_layer_norm_26))
  ttnn_from_device_501 = ttnn.from_device(ttnn_layer_norm_26, )
  ttnn_to_layout_504 = ttnn.to_layout(ttnn_from_device_501, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_290 = ttnn.reshape(ttnn_to_layout_504, (9, 1024), )
  ttnn_from_device_502 = ttnn.from_device(ttnn_reshape_290, )
  ttnn_to_layout_505 = ttnn.to_layout(ttnn_from_device_502, ttnn.TILE_LAYOUT, )
  ttnn_to_device_238 = ttnn.to_device(ttnn_to_layout_505, device = device)
  ttnn_matmul_105 = ttnn.matmul(ttnn_to_device_238, ttnn_transpose_1, )
  ttnn_add_79 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_105, )
  ttnn_from_device_503 = ttnn.from_device(ttnn_add_79, )
  ttnn_to_layout_506 = ttnn.to_layout(ttnn_from_device_503, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_291 = ttnn.reshape(ttnn_to_layout_506, (1, 9, 1024), )
  ttnn_to_layout_507 = ttnn.to_layout(ttnn_from_device_502, ttnn.TILE_LAYOUT, )
  ttnn_to_device_239 = ttnn.to_device(ttnn_to_layout_507, device = device)
  ttnn_matmul_106 = ttnn.matmul(ttnn_to_device_239, ttnn_transpose_2, )
  ttnn_add_80 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_106, )
  ttnn_from_device_505 = ttnn.from_device(ttnn_add_80, )
  ttnn_to_layout_508 = ttnn.to_layout(ttnn_from_device_505, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_293 = ttnn.reshape(ttnn_to_layout_508, (1, 9, 1024), )
  ttnn_to_layout_509 = ttnn.to_layout(ttnn_from_device_502, ttnn.TILE_LAYOUT, )
  ttnn_to_device_240 = ttnn.to_device(ttnn_to_layout_509, device = device)
  ttnn_matmul_107 = ttnn.matmul(ttnn_to_device_240, ttnn_transpose_3, )
  ttnn_add_81 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_107, )
  ttnn_from_device_507 = ttnn.from_device(ttnn_add_81, )
  ttnn_to_layout_510 = ttnn.to_layout(ttnn_from_device_507, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_295 = ttnn.reshape(ttnn_to_layout_510, (1, 9, 1024), )
  ttnn_from_device_508 = ttnn.from_device(ttnn_reshape_291, )
  ttnn_to_layout_511 = ttnn.to_layout(ttnn_from_device_508, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_296 = ttnn.reshape(ttnn_to_layout_511, (1, 9, 16, 64), )
  ttnn_from_device_509 = ttnn.from_device(ttnn_reshape_296, )
  ttnn_to_layout_512 = ttnn.to_layout(ttnn_from_device_509, ttnn.TILE_LAYOUT, )
  ttnn_to_device_241 = ttnn.to_device(ttnn_to_layout_512, device = device)
  ttnn_permute_39 = ttnn.permute(ttnn_to_device_241, (0, 2, 1, 3), )
  ttnn_from_device_510 = ttnn.from_device(ttnn_reshape_293, )
  ttnn_to_layout_513 = ttnn.to_layout(ttnn_from_device_510, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_297 = ttnn.reshape(ttnn_to_layout_513, (1, 9, 16, 64), )
  ttnn_from_device_511 = ttnn.from_device(ttnn_reshape_297, )
  ttnn_to_layout_514 = ttnn.to_layout(ttnn_from_device_511, ttnn.TILE_LAYOUT, )
  ttnn_to_device_242 = ttnn.to_device(ttnn_to_layout_514, device = device)
  ttnn_permute_40 = ttnn.permute(ttnn_to_device_242, (0, 2, 1, 3), )
  ttnn_from_device_512 = ttnn.from_device(ttnn_reshape_295, )
  ttnn_to_layout_515 = ttnn.to_layout(ttnn_from_device_512, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_298 = ttnn.reshape(ttnn_to_layout_515, (1, 9, 16, 64), )
  ttnn_from_device_513 = ttnn.from_device(ttnn_reshape_298, )
  ttnn_to_layout_516 = ttnn.to_layout(ttnn_from_device_513, ttnn.TILE_LAYOUT, )
  ttnn_to_device_243 = ttnn.to_device(ttnn_to_layout_516, device = device)
  ttnn_permute_41 = ttnn.permute(ttnn_to_device_243, (0, 2, 1, 3), )
  ttnn_transpose_108 = ttnn.transpose(ttnn_permute_40, 3, 2, )
  ttnn_from_device_514 = ttnn.from_device(ttnn_permute_39, )
  ttnn_to_layout_517 = ttnn.to_layout(ttnn_from_device_514, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_299 = ttnn.reshape(ttnn_to_layout_517, (16, 9, 64), )
  ttnn_from_device_515 = ttnn.from_device(ttnn_transpose_108, )
  ttnn_to_layout_518 = ttnn.to_layout(ttnn_from_device_515, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_300 = ttnn.reshape(ttnn_to_layout_518, (16, 64, 9), )
  ttnn_from_device_516 = ttnn.from_device(ttnn_reshape_299, )
  ttnn_to_layout_519 = ttnn.to_layout(ttnn_from_device_516, ttnn.TILE_LAYOUT, )
  ttnn_to_device_244 = ttnn.to_device(ttnn_to_layout_519, device = device)
  ttnn_from_device_517 = ttnn.from_device(ttnn_reshape_300, )
  ttnn_to_layout_520 = ttnn.to_layout(ttnn_from_device_517, ttnn.TILE_LAYOUT, )
  ttnn_to_device_245 = ttnn.to_device(ttnn_to_layout_520, device = device)
  ttnn_matmul_108 = ttnn.matmul(ttnn_to_device_244, ttnn_to_device_245, )
  ttnn_from_device_518 = ttnn.from_device(ttnn_matmul_108, )
  ttnn_to_layout_521 = ttnn.to_layout(ttnn_from_device_518, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_301 = ttnn.reshape(ttnn_to_layout_521, (1, 16, 9, 9), )
  ttnn_from_device_519 = ttnn.from_device(ttnn_reshape_301, )
  ttnn_to_layout_522 = ttnn.to_layout(ttnn_from_device_519, ttnn.TILE_LAYOUT, )
  ttnn_to_device_246 = ttnn.to_device(ttnn_to_layout_522, device = device)
  ttnn_multiply_66 = ttnn.multiply(ttnn_to_device_246, 0.125, )
  ttnn_add_214 = ttnn.add(ttnn_multiply_66, ttnn_multiply, )
  ttnn_softmax_13 = ttnn.softmax(ttnn_add_214, -1, numeric_stable = True)
  test_accuracy(_softmax_13, ttnn_softmax_13)
  ttnn_prefix_clone_40 = clone_wrapper(ttnn_softmax_13, )
  ttnn_from_device_520 = ttnn.from_device(ttnn_prefix_clone_40, )
  ttnn_to_layout_523 = ttnn.to_layout(ttnn_from_device_520, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_302 = ttnn.reshape(ttnn_to_layout_523, (16, 9, 9), )
  ttnn_from_device_521 = ttnn.from_device(ttnn_permute_41, )
  ttnn_to_layout_524 = ttnn.to_layout(ttnn_from_device_521, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_303 = ttnn.reshape(ttnn_to_layout_524, (16, 9, 64), )
  ttnn_from_device_522 = ttnn.from_device(ttnn_reshape_302, )
  ttnn_to_layout_525 = ttnn.to_layout(ttnn_from_device_522, ttnn.TILE_LAYOUT, )
  ttnn_to_device_247 = ttnn.to_device(ttnn_to_layout_525, device = device)
  ttnn_from_device_523 = ttnn.from_device(ttnn_reshape_303, )
  ttnn_to_layout_526 = ttnn.to_layout(ttnn_from_device_523, ttnn.TILE_LAYOUT, )
  ttnn_to_device_248 = ttnn.to_device(ttnn_to_layout_526, device = device)
  ttnn_matmul_109 = ttnn.matmul(ttnn_to_device_247, ttnn_to_device_248, )
  ttnn_from_device_524 = ttnn.from_device(ttnn_matmul_109, )
  ttnn_to_layout_527 = ttnn.to_layout(ttnn_from_device_524, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_304 = ttnn.reshape(ttnn_to_layout_527, (1, 16, 9, 64), )
  ttnn_from_device_525 = ttnn.from_device(ttnn_reshape_304, )
  ttnn_to_layout_528 = ttnn.to_layout(ttnn_from_device_525, ttnn.TILE_LAYOUT, )
  ttnn_to_device_249 = ttnn.to_device(ttnn_to_layout_528, device = device)
  ttnn_transpose_109 = ttnn.transpose(ttnn_to_device_249, 2, 1, )
  ttnn_prefix_clone_41 = clone_wrapper(ttnn_transpose_109, )
  ttnn_from_device_526 = ttnn.from_device(ttnn_prefix_clone_41, )
  ttnn_to_layout_529 = ttnn.to_layout(ttnn_from_device_526, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_305 = ttnn.reshape(ttnn_to_layout_529, (1, 9, 1024), )
  ttnn_from_device_527 = ttnn.from_device(ttnn_reshape_305, )
  ttnn_to_layout_530 = ttnn.to_layout(ttnn_from_device_527, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_306 = ttnn.reshape(ttnn_to_layout_530, (9, 1024), )
  ttnn_from_device_528 = ttnn.from_device(ttnn_reshape_306, )
  ttnn_to_layout_531 = ttnn.to_layout(ttnn_from_device_528, ttnn.TILE_LAYOUT, )
  ttnn_to_device_250 = ttnn.to_device(ttnn_to_layout_531, device = device)
  ttnn_matmul_110 = ttnn.matmul(ttnn_to_device_250, ttnn_transpose_6, )
  ttnn_add_82 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_110, )
  ttnn_from_device_529 = ttnn.from_device(ttnn_add_82, )
  ttnn_to_layout_532 = ttnn.to_layout(ttnn_from_device_529, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_307 = ttnn.reshape(ttnn_to_layout_532, (1, 9, 1024), )
  ttnn_from_device_530 = ttnn.from_device(ttnn_reshape_307, )
  ttnn_to_layout_533 = ttnn.to_layout(ttnn_from_device_530, ttnn.TILE_LAYOUT, )
  ttnn_to_device_251 = ttnn.to_device(ttnn_to_layout_533, device = device)
  ttnn_prefix_clone_42 = clone_wrapper(ttnn_to_device_251, )
  ttnn_add_215 = ttnn.add(ttnn_layer_norm_26, ttnn_prefix_clone_42, )
  ttnn_layer_norm_27_ = ttnn.layer_norm(ttnn_add_215, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_27_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_215), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_27 = ttnn.from_torch(ttnn_layer_norm_27_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_27_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_27_), ttnn_layer_norm_27))
  ttnn_from_device_531 = ttnn.from_device(ttnn_layer_norm_27, )
  ttnn_to_layout_534 = ttnn.to_layout(ttnn_from_device_531, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_308 = ttnn.reshape(ttnn_to_layout_534, (9, 1024), )
  ttnn_from_device_532 = ttnn.from_device(ttnn_reshape_308, )
  ttnn_to_layout_535 = ttnn.to_layout(ttnn_from_device_532, ttnn.TILE_LAYOUT, )
  ttnn_to_device_252 = ttnn.to_device(ttnn_to_layout_535, device = device)
  ttnn_matmul_111 = ttnn.matmul(ttnn_to_device_252, ttnn_transpose_7, )
  ttnn_add_83 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_111, )
  ttnn_from_device_533 = ttnn.from_device(ttnn_add_83, )
  ttnn_to_layout_536 = ttnn.to_layout(ttnn_from_device_533, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_309 = ttnn.reshape(ttnn_to_layout_536, (1, 9, 4096), )
  ttnn_from_device_534 = ttnn.from_device(ttnn_reshape_309, )
  ttnn_to_layout_537 = ttnn.to_layout(ttnn_from_device_534, ttnn.TILE_LAYOUT, )
  ttnn_to_device_253 = ttnn.to_device(ttnn_to_layout_537, device = device)
  ttnn_multiply_67 = ttnn.multiply(ttnn_to_device_253, 0.5, )
  ttnn_pow_13 = ttnn.pow(ttnn_to_device_253, 3.0, )
  ttnn_multiply_68 = ttnn.multiply(ttnn_pow_13, 0.044715, )
  ttnn_add_216 = ttnn.add(ttnn_to_device_253, ttnn_multiply_68, )
  ttnn_multiply_69 = ttnn.multiply(ttnn_add_216, 0.7978845608028654, )
  ttnn_tanh_13_ = ttnn.tanh(ttnn_multiply_69, )
  ttnn_tanh_13_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_69), )
  ttnn_tanh_13 = ttnn.from_torch(ttnn_tanh_13_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_13_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_13_), ttnn_tanh_13))
  test_accuracy(tanh_13, ttnn_tanh_13)
  ttnn_add_217 = ttnn.add(ttnn_tanh_13, 1.0, )
  ttnn_multiply_70 = ttnn.multiply(ttnn_multiply_67, ttnn_add_217, )
  ttnn_from_device_535 = ttnn.from_device(ttnn_multiply_70, )
  ttnn_to_layout_538 = ttnn.to_layout(ttnn_from_device_535, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_310 = ttnn.reshape(ttnn_to_layout_538, (9, 4096), )
  ttnn_from_device_536 = ttnn.from_device(ttnn_reshape_310, )
  ttnn_to_layout_539 = ttnn.to_layout(ttnn_from_device_536, ttnn.TILE_LAYOUT, )
  ttnn_to_device_254 = ttnn.to_device(ttnn_to_layout_539, device = device)
  ttnn_matmul_112 = ttnn.matmul(ttnn_to_device_254, ttnn_transpose_8, )
  ttnn_add_84 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_112, )
  ttnn_from_device_537 = ttnn.from_device(ttnn_add_84, )
  ttnn_to_layout_540 = ttnn.to_layout(ttnn_from_device_537, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_311 = ttnn.reshape(ttnn_to_layout_540, (1, 9, 1024), )
  ttnn_from_device_538 = ttnn.from_device(ttnn_reshape_311, )
  ttnn_to_layout_541 = ttnn.to_layout(ttnn_from_device_538, ttnn.TILE_LAYOUT, )
  ttnn_to_device_255 = ttnn.to_device(ttnn_to_layout_541, device = device)
  ttnn_add_218 = ttnn.add(ttnn_to_device_255, ttnn_layer_norm_27, )
  ttnn_layer_norm_28_ = ttnn.layer_norm(ttnn_add_218, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_28_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_218), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_28 = ttnn.from_torch(ttnn_layer_norm_28_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_28_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_28_), ttnn_layer_norm_28))
  ttnn_from_device_539 = ttnn.from_device(ttnn_layer_norm_28, )
  ttnn_to_layout_542 = ttnn.to_layout(ttnn_from_device_539, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_312 = ttnn.reshape(ttnn_to_layout_542, (9, 1024), )
  ttnn_from_device_540 = ttnn.from_device(ttnn_reshape_312, )
  ttnn_to_layout_543 = ttnn.to_layout(ttnn_from_device_540, ttnn.TILE_LAYOUT, )
  ttnn_to_device_256 = ttnn.to_device(ttnn_to_layout_543, device = device)
  ttnn_matmul_113 = ttnn.matmul(ttnn_to_device_256, ttnn_transpose_1, )
  ttnn_add_85 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_113, )
  ttnn_from_device_541 = ttnn.from_device(ttnn_add_85, )
  ttnn_to_layout_544 = ttnn.to_layout(ttnn_from_device_541, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_313 = ttnn.reshape(ttnn_to_layout_544, (1, 9, 1024), )
  ttnn_to_layout_545 = ttnn.to_layout(ttnn_from_device_540, ttnn.TILE_LAYOUT, )
  ttnn_to_device_257 = ttnn.to_device(ttnn_to_layout_545, device = device)
  ttnn_matmul_114 = ttnn.matmul(ttnn_to_device_257, ttnn_transpose_2, )
  ttnn_add_86 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_114, )
  ttnn_from_device_543 = ttnn.from_device(ttnn_add_86, )
  ttnn_to_layout_546 = ttnn.to_layout(ttnn_from_device_543, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_315 = ttnn.reshape(ttnn_to_layout_546, (1, 9, 1024), )
  ttnn_to_layout_547 = ttnn.to_layout(ttnn_from_device_540, ttnn.TILE_LAYOUT, )
  ttnn_to_device_258 = ttnn.to_device(ttnn_to_layout_547, device = device)
  ttnn_matmul_115 = ttnn.matmul(ttnn_to_device_258, ttnn_transpose_3, )
  ttnn_add_87 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_115, )
  ttnn_from_device_545 = ttnn.from_device(ttnn_add_87, )
  ttnn_to_layout_548 = ttnn.to_layout(ttnn_from_device_545, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_317 = ttnn.reshape(ttnn_to_layout_548, (1, 9, 1024), )
  ttnn_from_device_546 = ttnn.from_device(ttnn_reshape_313, )
  ttnn_to_layout_549 = ttnn.to_layout(ttnn_from_device_546, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_318 = ttnn.reshape(ttnn_to_layout_549, (1, 9, 16, 64), )
  ttnn_from_device_547 = ttnn.from_device(ttnn_reshape_318, )
  ttnn_to_layout_550 = ttnn.to_layout(ttnn_from_device_547, ttnn.TILE_LAYOUT, )
  ttnn_to_device_259 = ttnn.to_device(ttnn_to_layout_550, device = device)
  ttnn_permute_42 = ttnn.permute(ttnn_to_device_259, (0, 2, 1, 3), )
  ttnn_from_device_548 = ttnn.from_device(ttnn_reshape_315, )
  ttnn_to_layout_551 = ttnn.to_layout(ttnn_from_device_548, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_319 = ttnn.reshape(ttnn_to_layout_551, (1, 9, 16, 64), )
  ttnn_from_device_549 = ttnn.from_device(ttnn_reshape_319, )
  ttnn_to_layout_552 = ttnn.to_layout(ttnn_from_device_549, ttnn.TILE_LAYOUT, )
  ttnn_to_device_260 = ttnn.to_device(ttnn_to_layout_552, device = device)
  ttnn_permute_43 = ttnn.permute(ttnn_to_device_260, (0, 2, 1, 3), )
  ttnn_from_device_550 = ttnn.from_device(ttnn_reshape_317, )
  ttnn_to_layout_553 = ttnn.to_layout(ttnn_from_device_550, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_320 = ttnn.reshape(ttnn_to_layout_553, (1, 9, 16, 64), )
  ttnn_from_device_551 = ttnn.from_device(ttnn_reshape_320, )
  ttnn_to_layout_554 = ttnn.to_layout(ttnn_from_device_551, ttnn.TILE_LAYOUT, )
  ttnn_to_device_261 = ttnn.to_device(ttnn_to_layout_554, device = device)
  ttnn_permute_44 = ttnn.permute(ttnn_to_device_261, (0, 2, 1, 3), )
  ttnn_transpose_116 = ttnn.transpose(ttnn_permute_43, 3, 2, )
  ttnn_from_device_552 = ttnn.from_device(ttnn_permute_42, )
  ttnn_to_layout_555 = ttnn.to_layout(ttnn_from_device_552, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_321 = ttnn.reshape(ttnn_to_layout_555, (16, 9, 64), )
  ttnn_from_device_553 = ttnn.from_device(ttnn_transpose_116, )
  ttnn_to_layout_556 = ttnn.to_layout(ttnn_from_device_553, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_322 = ttnn.reshape(ttnn_to_layout_556, (16, 64, 9), )
  ttnn_from_device_554 = ttnn.from_device(ttnn_reshape_321, )
  ttnn_to_layout_557 = ttnn.to_layout(ttnn_from_device_554, ttnn.TILE_LAYOUT, )
  ttnn_to_device_262 = ttnn.to_device(ttnn_to_layout_557, device = device)
  ttnn_from_device_555 = ttnn.from_device(ttnn_reshape_322, )
  ttnn_to_layout_558 = ttnn.to_layout(ttnn_from_device_555, ttnn.TILE_LAYOUT, )
  ttnn_to_device_263 = ttnn.to_device(ttnn_to_layout_558, device = device)
  ttnn_matmul_116 = ttnn.matmul(ttnn_to_device_262, ttnn_to_device_263, )
  ttnn_from_device_556 = ttnn.from_device(ttnn_matmul_116, )
  ttnn_to_layout_559 = ttnn.to_layout(ttnn_from_device_556, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_323 = ttnn.reshape(ttnn_to_layout_559, (1, 16, 9, 9), )
  ttnn_from_device_557 = ttnn.from_device(ttnn_reshape_323, )
  ttnn_to_layout_560 = ttnn.to_layout(ttnn_from_device_557, ttnn.TILE_LAYOUT, )
  ttnn_to_device_264 = ttnn.to_device(ttnn_to_layout_560, device = device)
  ttnn_multiply_71 = ttnn.multiply(ttnn_to_device_264, 0.125, )
  ttnn_add_219 = ttnn.add(ttnn_multiply_71, ttnn_multiply, )
  ttnn_softmax_14 = ttnn.softmax(ttnn_add_219, -1, numeric_stable = True)
  test_accuracy(_softmax_14, ttnn_softmax_14)
  ttnn_prefix_clone_43 = clone_wrapper(ttnn_softmax_14, )
  ttnn_from_device_558 = ttnn.from_device(ttnn_prefix_clone_43, )
  ttnn_to_layout_561 = ttnn.to_layout(ttnn_from_device_558, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_324 = ttnn.reshape(ttnn_to_layout_561, (16, 9, 9), )
  ttnn_from_device_559 = ttnn.from_device(ttnn_permute_44, )
  ttnn_to_layout_562 = ttnn.to_layout(ttnn_from_device_559, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_325 = ttnn.reshape(ttnn_to_layout_562, (16, 9, 64), )
  ttnn_from_device_560 = ttnn.from_device(ttnn_reshape_324, )
  ttnn_to_layout_563 = ttnn.to_layout(ttnn_from_device_560, ttnn.TILE_LAYOUT, )
  ttnn_to_device_265 = ttnn.to_device(ttnn_to_layout_563, device = device)
  ttnn_from_device_561 = ttnn.from_device(ttnn_reshape_325, )
  ttnn_to_layout_564 = ttnn.to_layout(ttnn_from_device_561, ttnn.TILE_LAYOUT, )
  ttnn_to_device_266 = ttnn.to_device(ttnn_to_layout_564, device = device)
  ttnn_matmul_117 = ttnn.matmul(ttnn_to_device_265, ttnn_to_device_266, )
  ttnn_from_device_562 = ttnn.from_device(ttnn_matmul_117, )
  ttnn_to_layout_565 = ttnn.to_layout(ttnn_from_device_562, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_326 = ttnn.reshape(ttnn_to_layout_565, (1, 16, 9, 64), )
  ttnn_from_device_563 = ttnn.from_device(ttnn_reshape_326, )
  ttnn_to_layout_566 = ttnn.to_layout(ttnn_from_device_563, ttnn.TILE_LAYOUT, )
  ttnn_to_device_267 = ttnn.to_device(ttnn_to_layout_566, device = device)
  ttnn_transpose_117 = ttnn.transpose(ttnn_to_device_267, 2, 1, )
  ttnn_prefix_clone_44 = clone_wrapper(ttnn_transpose_117, )
  ttnn_from_device_564 = ttnn.from_device(ttnn_prefix_clone_44, )
  ttnn_to_layout_567 = ttnn.to_layout(ttnn_from_device_564, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_327 = ttnn.reshape(ttnn_to_layout_567, (1, 9, 1024), )
  ttnn_from_device_565 = ttnn.from_device(ttnn_reshape_327, )
  ttnn_to_layout_568 = ttnn.to_layout(ttnn_from_device_565, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_328 = ttnn.reshape(ttnn_to_layout_568, (9, 1024), )
  ttnn_from_device_566 = ttnn.from_device(ttnn_reshape_328, )
  ttnn_to_layout_569 = ttnn.to_layout(ttnn_from_device_566, ttnn.TILE_LAYOUT, )
  ttnn_to_device_268 = ttnn.to_device(ttnn_to_layout_569, device = device)
  ttnn_matmul_118 = ttnn.matmul(ttnn_to_device_268, ttnn_transpose_6, )
  ttnn_add_88 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_118, )
  ttnn_from_device_567 = ttnn.from_device(ttnn_add_88, )
  ttnn_to_layout_570 = ttnn.to_layout(ttnn_from_device_567, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_329 = ttnn.reshape(ttnn_to_layout_570, (1, 9, 1024), )
  ttnn_from_device_568 = ttnn.from_device(ttnn_reshape_329, )
  ttnn_to_layout_571 = ttnn.to_layout(ttnn_from_device_568, ttnn.TILE_LAYOUT, )
  ttnn_to_device_269 = ttnn.to_device(ttnn_to_layout_571, device = device)
  ttnn_prefix_clone_45 = clone_wrapper(ttnn_to_device_269, )
  ttnn_add_220 = ttnn.add(ttnn_layer_norm_28, ttnn_prefix_clone_45, )
  ttnn_layer_norm_29_ = ttnn.layer_norm(ttnn_add_220, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_29_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_220), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_29 = ttnn.from_torch(ttnn_layer_norm_29_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_29_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_29_), ttnn_layer_norm_29))
  ttnn_from_device_569 = ttnn.from_device(ttnn_layer_norm_29, )
  ttnn_to_layout_572 = ttnn.to_layout(ttnn_from_device_569, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_330 = ttnn.reshape(ttnn_to_layout_572, (9, 1024), )
  ttnn_from_device_570 = ttnn.from_device(ttnn_reshape_330, )
  ttnn_to_layout_573 = ttnn.to_layout(ttnn_from_device_570, ttnn.TILE_LAYOUT, )
  ttnn_to_device_270 = ttnn.to_device(ttnn_to_layout_573, device = device)
  ttnn_matmul_119 = ttnn.matmul(ttnn_to_device_270, ttnn_transpose_7, )
  ttnn_add_89 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_119, )
  ttnn_from_device_571 = ttnn.from_device(ttnn_add_89, )
  ttnn_to_layout_574 = ttnn.to_layout(ttnn_from_device_571, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_331 = ttnn.reshape(ttnn_to_layout_574, (1, 9, 4096), )
  ttnn_from_device_572 = ttnn.from_device(ttnn_reshape_331, )
  ttnn_to_layout_575 = ttnn.to_layout(ttnn_from_device_572, ttnn.TILE_LAYOUT, )
  ttnn_to_device_271 = ttnn.to_device(ttnn_to_layout_575, device = device)
  ttnn_multiply_72 = ttnn.multiply(ttnn_to_device_271, 0.5, )
  ttnn_pow_14 = ttnn.pow(ttnn_to_device_271, 3.0, )
  ttnn_multiply_73 = ttnn.multiply(ttnn_pow_14, 0.044715, )
  ttnn_add_221 = ttnn.add(ttnn_to_device_271, ttnn_multiply_73, )
  ttnn_multiply_74 = ttnn.multiply(ttnn_add_221, 0.7978845608028654, )
  ttnn_tanh_14_ = ttnn.tanh(ttnn_multiply_74, )
  ttnn_tanh_14_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_74), )
  ttnn_tanh_14 = ttnn.from_torch(ttnn_tanh_14_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_14_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_14_), ttnn_tanh_14))
  test_accuracy(tanh_14, ttnn_tanh_14)
  ttnn_add_222 = ttnn.add(ttnn_tanh_14, 1.0, )
  ttnn_multiply_75 = ttnn.multiply(ttnn_multiply_72, ttnn_add_222, )
  ttnn_from_device_573 = ttnn.from_device(ttnn_multiply_75, )
  ttnn_to_layout_576 = ttnn.to_layout(ttnn_from_device_573, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_332 = ttnn.reshape(ttnn_to_layout_576, (9, 4096), )
  ttnn_from_device_574 = ttnn.from_device(ttnn_reshape_332, )
  ttnn_to_layout_577 = ttnn.to_layout(ttnn_from_device_574, ttnn.TILE_LAYOUT, )
  ttnn_to_device_272 = ttnn.to_device(ttnn_to_layout_577, device = device)
  ttnn_matmul_120 = ttnn.matmul(ttnn_to_device_272, ttnn_transpose_8, )
  ttnn_add_90 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_120, )
  ttnn_from_device_575 = ttnn.from_device(ttnn_add_90, )
  ttnn_to_layout_578 = ttnn.to_layout(ttnn_from_device_575, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_333 = ttnn.reshape(ttnn_to_layout_578, (1, 9, 1024), )
  ttnn_from_device_576 = ttnn.from_device(ttnn_reshape_333, )
  ttnn_to_layout_579 = ttnn.to_layout(ttnn_from_device_576, ttnn.TILE_LAYOUT, )
  ttnn_to_device_273 = ttnn.to_device(ttnn_to_layout_579, device = device)
  ttnn_add_223 = ttnn.add(ttnn_to_device_273, ttnn_layer_norm_29, )
  ttnn_layer_norm_30_ = ttnn.layer_norm(ttnn_add_223, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_30_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_223), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_30 = ttnn.from_torch(ttnn_layer_norm_30_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_30_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_30_), ttnn_layer_norm_30))
  ttnn_from_device_577 = ttnn.from_device(ttnn_layer_norm_30, )
  ttnn_to_layout_580 = ttnn.to_layout(ttnn_from_device_577, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_334 = ttnn.reshape(ttnn_to_layout_580, (9, 1024), )
  ttnn_from_device_578 = ttnn.from_device(ttnn_reshape_334, )
  ttnn_to_layout_581 = ttnn.to_layout(ttnn_from_device_578, ttnn.TILE_LAYOUT, )
  ttnn_to_device_274 = ttnn.to_device(ttnn_to_layout_581, device = device)
  ttnn_matmul_121 = ttnn.matmul(ttnn_to_device_274, ttnn_transpose_1, )
  ttnn_add_91 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_121, )
  ttnn_from_device_579 = ttnn.from_device(ttnn_add_91, )
  ttnn_to_layout_582 = ttnn.to_layout(ttnn_from_device_579, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_335 = ttnn.reshape(ttnn_to_layout_582, (1, 9, 1024), )
  ttnn_to_layout_583 = ttnn.to_layout(ttnn_from_device_578, ttnn.TILE_LAYOUT, )
  ttnn_to_device_275 = ttnn.to_device(ttnn_to_layout_583, device = device)
  ttnn_matmul_122 = ttnn.matmul(ttnn_to_device_275, ttnn_transpose_2, )
  ttnn_add_92 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_122, )
  ttnn_from_device_581 = ttnn.from_device(ttnn_add_92, )
  ttnn_to_layout_584 = ttnn.to_layout(ttnn_from_device_581, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_337 = ttnn.reshape(ttnn_to_layout_584, (1, 9, 1024), )
  ttnn_to_layout_585 = ttnn.to_layout(ttnn_from_device_578, ttnn.TILE_LAYOUT, )
  ttnn_to_device_276 = ttnn.to_device(ttnn_to_layout_585, device = device)
  ttnn_matmul_123 = ttnn.matmul(ttnn_to_device_276, ttnn_transpose_3, )
  ttnn_add_93 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_123, )
  ttnn_from_device_583 = ttnn.from_device(ttnn_add_93, )
  ttnn_to_layout_586 = ttnn.to_layout(ttnn_from_device_583, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_339 = ttnn.reshape(ttnn_to_layout_586, (1, 9, 1024), )
  ttnn_from_device_584 = ttnn.from_device(ttnn_reshape_335, )
  ttnn_to_layout_587 = ttnn.to_layout(ttnn_from_device_584, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_340 = ttnn.reshape(ttnn_to_layout_587, (1, 9, 16, 64), )
  ttnn_from_device_585 = ttnn.from_device(ttnn_reshape_340, )
  ttnn_to_layout_588 = ttnn.to_layout(ttnn_from_device_585, ttnn.TILE_LAYOUT, )
  ttnn_to_device_277 = ttnn.to_device(ttnn_to_layout_588, device = device)
  ttnn_permute_45 = ttnn.permute(ttnn_to_device_277, (0, 2, 1, 3), )
  ttnn_from_device_586 = ttnn.from_device(ttnn_reshape_337, )
  ttnn_to_layout_589 = ttnn.to_layout(ttnn_from_device_586, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_341 = ttnn.reshape(ttnn_to_layout_589, (1, 9, 16, 64), )
  ttnn_from_device_587 = ttnn.from_device(ttnn_reshape_341, )
  ttnn_to_layout_590 = ttnn.to_layout(ttnn_from_device_587, ttnn.TILE_LAYOUT, )
  ttnn_to_device_278 = ttnn.to_device(ttnn_to_layout_590, device = device)
  ttnn_permute_46 = ttnn.permute(ttnn_to_device_278, (0, 2, 1, 3), )
  ttnn_from_device_588 = ttnn.from_device(ttnn_reshape_339, )
  ttnn_to_layout_591 = ttnn.to_layout(ttnn_from_device_588, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_342 = ttnn.reshape(ttnn_to_layout_591, (1, 9, 16, 64), )
  ttnn_from_device_589 = ttnn.from_device(ttnn_reshape_342, )
  ttnn_to_layout_592 = ttnn.to_layout(ttnn_from_device_589, ttnn.TILE_LAYOUT, )
  ttnn_to_device_279 = ttnn.to_device(ttnn_to_layout_592, device = device)
  ttnn_permute_47 = ttnn.permute(ttnn_to_device_279, (0, 2, 1, 3), )
  ttnn_transpose_124 = ttnn.transpose(ttnn_permute_46, 3, 2, )
  ttnn_from_device_590 = ttnn.from_device(ttnn_permute_45, )
  ttnn_to_layout_593 = ttnn.to_layout(ttnn_from_device_590, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_343 = ttnn.reshape(ttnn_to_layout_593, (16, 9, 64), )
  ttnn_from_device_591 = ttnn.from_device(ttnn_transpose_124, )
  ttnn_to_layout_594 = ttnn.to_layout(ttnn_from_device_591, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_344 = ttnn.reshape(ttnn_to_layout_594, (16, 64, 9), )
  ttnn_from_device_592 = ttnn.from_device(ttnn_reshape_343, )
  ttnn_to_layout_595 = ttnn.to_layout(ttnn_from_device_592, ttnn.TILE_LAYOUT, )
  ttnn_to_device_280 = ttnn.to_device(ttnn_to_layout_595, device = device)
  ttnn_from_device_593 = ttnn.from_device(ttnn_reshape_344, )
  ttnn_to_layout_596 = ttnn.to_layout(ttnn_from_device_593, ttnn.TILE_LAYOUT, )
  ttnn_to_device_281 = ttnn.to_device(ttnn_to_layout_596, device = device)
  ttnn_matmul_124 = ttnn.matmul(ttnn_to_device_280, ttnn_to_device_281, )
  ttnn_from_device_594 = ttnn.from_device(ttnn_matmul_124, )
  ttnn_to_layout_597 = ttnn.to_layout(ttnn_from_device_594, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_345 = ttnn.reshape(ttnn_to_layout_597, (1, 16, 9, 9), )
  ttnn_from_device_595 = ttnn.from_device(ttnn_reshape_345, )
  ttnn_to_layout_598 = ttnn.to_layout(ttnn_from_device_595, ttnn.TILE_LAYOUT, )
  ttnn_to_device_282 = ttnn.to_device(ttnn_to_layout_598, device = device)
  ttnn_multiply_76 = ttnn.multiply(ttnn_to_device_282, 0.125, )
  ttnn_add_224 = ttnn.add(ttnn_multiply_76, ttnn_multiply, )
  ttnn_softmax_15 = ttnn.softmax(ttnn_add_224, -1, numeric_stable = True)
  test_accuracy(_softmax_15, ttnn_softmax_15)
  ttnn_prefix_clone_46 = clone_wrapper(ttnn_softmax_15, )
  ttnn_from_device_596 = ttnn.from_device(ttnn_prefix_clone_46, )
  ttnn_to_layout_599 = ttnn.to_layout(ttnn_from_device_596, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_346 = ttnn.reshape(ttnn_to_layout_599, (16, 9, 9), )
  ttnn_from_device_597 = ttnn.from_device(ttnn_permute_47, )
  ttnn_to_layout_600 = ttnn.to_layout(ttnn_from_device_597, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_347 = ttnn.reshape(ttnn_to_layout_600, (16, 9, 64), )
  ttnn_from_device_598 = ttnn.from_device(ttnn_reshape_346, )
  ttnn_to_layout_601 = ttnn.to_layout(ttnn_from_device_598, ttnn.TILE_LAYOUT, )
  ttnn_to_device_283 = ttnn.to_device(ttnn_to_layout_601, device = device)
  ttnn_from_device_599 = ttnn.from_device(ttnn_reshape_347, )
  ttnn_to_layout_602 = ttnn.to_layout(ttnn_from_device_599, ttnn.TILE_LAYOUT, )
  ttnn_to_device_284 = ttnn.to_device(ttnn_to_layout_602, device = device)
  ttnn_matmul_125 = ttnn.matmul(ttnn_to_device_283, ttnn_to_device_284, )
  ttnn_from_device_600 = ttnn.from_device(ttnn_matmul_125, )
  ttnn_to_layout_603 = ttnn.to_layout(ttnn_from_device_600, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_348 = ttnn.reshape(ttnn_to_layout_603, (1, 16, 9, 64), )
  ttnn_from_device_601 = ttnn.from_device(ttnn_reshape_348, )
  ttnn_to_layout_604 = ttnn.to_layout(ttnn_from_device_601, ttnn.TILE_LAYOUT, )
  ttnn_to_device_285 = ttnn.to_device(ttnn_to_layout_604, device = device)
  ttnn_transpose_125 = ttnn.transpose(ttnn_to_device_285, 2, 1, )
  ttnn_prefix_clone_47 = clone_wrapper(ttnn_transpose_125, )
  ttnn_from_device_602 = ttnn.from_device(ttnn_prefix_clone_47, )
  ttnn_to_layout_605 = ttnn.to_layout(ttnn_from_device_602, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_349 = ttnn.reshape(ttnn_to_layout_605, (1, 9, 1024), )
  ttnn_from_device_603 = ttnn.from_device(ttnn_reshape_349, )
  ttnn_to_layout_606 = ttnn.to_layout(ttnn_from_device_603, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_350 = ttnn.reshape(ttnn_to_layout_606, (9, 1024), )
  ttnn_from_device_604 = ttnn.from_device(ttnn_reshape_350, )
  ttnn_to_layout_607 = ttnn.to_layout(ttnn_from_device_604, ttnn.TILE_LAYOUT, )
  ttnn_to_device_286 = ttnn.to_device(ttnn_to_layout_607, device = device)
  ttnn_matmul_126 = ttnn.matmul(ttnn_to_device_286, ttnn_transpose_6, )
  ttnn_add_94 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_126, )
  ttnn_from_device_605 = ttnn.from_device(ttnn_add_94, )
  ttnn_to_layout_608 = ttnn.to_layout(ttnn_from_device_605, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_351 = ttnn.reshape(ttnn_to_layout_608, (1, 9, 1024), )
  ttnn_from_device_606 = ttnn.from_device(ttnn_reshape_351, )
  ttnn_to_layout_609 = ttnn.to_layout(ttnn_from_device_606, ttnn.TILE_LAYOUT, )
  ttnn_to_device_287 = ttnn.to_device(ttnn_to_layout_609, device = device)
  ttnn_prefix_clone_48 = clone_wrapper(ttnn_to_device_287, )
  ttnn_add_225 = ttnn.add(ttnn_layer_norm_30, ttnn_prefix_clone_48, )
  ttnn_layer_norm_31_ = ttnn.layer_norm(ttnn_add_225, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_31_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_225), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_31 = ttnn.from_torch(ttnn_layer_norm_31_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_31_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_31_), ttnn_layer_norm_31))
  ttnn_from_device_607 = ttnn.from_device(ttnn_layer_norm_31, )
  ttnn_to_layout_610 = ttnn.to_layout(ttnn_from_device_607, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_352 = ttnn.reshape(ttnn_to_layout_610, (9, 1024), )
  ttnn_from_device_608 = ttnn.from_device(ttnn_reshape_352, )
  ttnn_to_layout_611 = ttnn.to_layout(ttnn_from_device_608, ttnn.TILE_LAYOUT, )
  ttnn_to_device_288 = ttnn.to_device(ttnn_to_layout_611, device = device)
  ttnn_matmul_127 = ttnn.matmul(ttnn_to_device_288, ttnn_transpose_7, )
  ttnn_add_95 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_127, )
  ttnn_from_device_609 = ttnn.from_device(ttnn_add_95, )
  ttnn_to_layout_612 = ttnn.to_layout(ttnn_from_device_609, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_353 = ttnn.reshape(ttnn_to_layout_612, (1, 9, 4096), )
  ttnn_from_device_610 = ttnn.from_device(ttnn_reshape_353, )
  ttnn_to_layout_613 = ttnn.to_layout(ttnn_from_device_610, ttnn.TILE_LAYOUT, )
  ttnn_to_device_289 = ttnn.to_device(ttnn_to_layout_613, device = device)
  ttnn_multiply_77 = ttnn.multiply(ttnn_to_device_289, 0.5, )
  ttnn_pow_15 = ttnn.pow(ttnn_to_device_289, 3.0, )
  ttnn_multiply_78 = ttnn.multiply(ttnn_pow_15, 0.044715, )
  ttnn_add_226 = ttnn.add(ttnn_to_device_289, ttnn_multiply_78, )
  ttnn_multiply_79 = ttnn.multiply(ttnn_add_226, 0.7978845608028654, )
  ttnn_tanh_15_ = ttnn.tanh(ttnn_multiply_79, )
  ttnn_tanh_15_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_79), )
  ttnn_tanh_15 = ttnn.from_torch(ttnn_tanh_15_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_15_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_15_), ttnn_tanh_15))
  test_accuracy(tanh_15, ttnn_tanh_15)
  ttnn_add_227 = ttnn.add(ttnn_tanh_15, 1.0, )
  ttnn_multiply_80 = ttnn.multiply(ttnn_multiply_77, ttnn_add_227, )
  ttnn_from_device_611 = ttnn.from_device(ttnn_multiply_80, )
  ttnn_to_layout_614 = ttnn.to_layout(ttnn_from_device_611, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_354 = ttnn.reshape(ttnn_to_layout_614, (9, 4096), )
  ttnn_from_device_612 = ttnn.from_device(ttnn_reshape_354, )
  ttnn_to_layout_615 = ttnn.to_layout(ttnn_from_device_612, ttnn.TILE_LAYOUT, )
  ttnn_to_device_290 = ttnn.to_device(ttnn_to_layout_615, device = device)
  ttnn_matmul_128 = ttnn.matmul(ttnn_to_device_290, ttnn_transpose_8, )
  ttnn_add_96 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_128, )
  ttnn_from_device_613 = ttnn.from_device(ttnn_add_96, )
  ttnn_to_layout_616 = ttnn.to_layout(ttnn_from_device_613, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_355 = ttnn.reshape(ttnn_to_layout_616, (1, 9, 1024), )
  ttnn_from_device_614 = ttnn.from_device(ttnn_reshape_355, )
  ttnn_to_layout_617 = ttnn.to_layout(ttnn_from_device_614, ttnn.TILE_LAYOUT, )
  ttnn_to_device_291 = ttnn.to_device(ttnn_to_layout_617, device = device)
  ttnn_add_228 = ttnn.add(ttnn_to_device_291, ttnn_layer_norm_31, )
  ttnn_layer_norm_32_ = ttnn.layer_norm(ttnn_add_228, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_32_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_228), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_32 = ttnn.from_torch(ttnn_layer_norm_32_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_32_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_32_), ttnn_layer_norm_32))
  ttnn_from_device_615 = ttnn.from_device(ttnn_layer_norm_32, )
  ttnn_to_layout_618 = ttnn.to_layout(ttnn_from_device_615, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_356 = ttnn.reshape(ttnn_to_layout_618, (9, 1024), )
  ttnn_from_device_616 = ttnn.from_device(ttnn_reshape_356, )
  ttnn_to_layout_619 = ttnn.to_layout(ttnn_from_device_616, ttnn.TILE_LAYOUT, )
  ttnn_to_device_292 = ttnn.to_device(ttnn_to_layout_619, device = device)
  ttnn_matmul_129 = ttnn.matmul(ttnn_to_device_292, ttnn_transpose_1, )
  ttnn_add_97 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_129, )
  ttnn_from_device_617 = ttnn.from_device(ttnn_add_97, )
  ttnn_to_layout_620 = ttnn.to_layout(ttnn_from_device_617, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_357 = ttnn.reshape(ttnn_to_layout_620, (1, 9, 1024), )
  ttnn_to_layout_621 = ttnn.to_layout(ttnn_from_device_616, ttnn.TILE_LAYOUT, )
  ttnn_to_device_293 = ttnn.to_device(ttnn_to_layout_621, device = device)
  ttnn_matmul_130 = ttnn.matmul(ttnn_to_device_293, ttnn_transpose_2, )
  ttnn_add_98 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_130, )
  ttnn_from_device_619 = ttnn.from_device(ttnn_add_98, )
  ttnn_to_layout_622 = ttnn.to_layout(ttnn_from_device_619, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_359 = ttnn.reshape(ttnn_to_layout_622, (1, 9, 1024), )
  ttnn_to_layout_623 = ttnn.to_layout(ttnn_from_device_616, ttnn.TILE_LAYOUT, )
  ttnn_to_device_294 = ttnn.to_device(ttnn_to_layout_623, device = device)
  ttnn_matmul_131 = ttnn.matmul(ttnn_to_device_294, ttnn_transpose_3, )
  ttnn_add_99 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_131, )
  ttnn_from_device_621 = ttnn.from_device(ttnn_add_99, )
  ttnn_to_layout_624 = ttnn.to_layout(ttnn_from_device_621, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_361 = ttnn.reshape(ttnn_to_layout_624, (1, 9, 1024), )
  ttnn_from_device_622 = ttnn.from_device(ttnn_reshape_357, )
  ttnn_to_layout_625 = ttnn.to_layout(ttnn_from_device_622, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_362 = ttnn.reshape(ttnn_to_layout_625, (1, 9, 16, 64), )
  ttnn_from_device_623 = ttnn.from_device(ttnn_reshape_362, )
  ttnn_to_layout_626 = ttnn.to_layout(ttnn_from_device_623, ttnn.TILE_LAYOUT, )
  ttnn_to_device_295 = ttnn.to_device(ttnn_to_layout_626, device = device)
  ttnn_permute_48 = ttnn.permute(ttnn_to_device_295, (0, 2, 1, 3), )
  ttnn_from_device_624 = ttnn.from_device(ttnn_reshape_359, )
  ttnn_to_layout_627 = ttnn.to_layout(ttnn_from_device_624, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_363 = ttnn.reshape(ttnn_to_layout_627, (1, 9, 16, 64), )
  ttnn_from_device_625 = ttnn.from_device(ttnn_reshape_363, )
  ttnn_to_layout_628 = ttnn.to_layout(ttnn_from_device_625, ttnn.TILE_LAYOUT, )
  ttnn_to_device_296 = ttnn.to_device(ttnn_to_layout_628, device = device)
  ttnn_permute_49 = ttnn.permute(ttnn_to_device_296, (0, 2, 1, 3), )
  ttnn_from_device_626 = ttnn.from_device(ttnn_reshape_361, )
  ttnn_to_layout_629 = ttnn.to_layout(ttnn_from_device_626, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_364 = ttnn.reshape(ttnn_to_layout_629, (1, 9, 16, 64), )
  ttnn_from_device_627 = ttnn.from_device(ttnn_reshape_364, )
  ttnn_to_layout_630 = ttnn.to_layout(ttnn_from_device_627, ttnn.TILE_LAYOUT, )
  ttnn_to_device_297 = ttnn.to_device(ttnn_to_layout_630, device = device)
  ttnn_permute_50 = ttnn.permute(ttnn_to_device_297, (0, 2, 1, 3), )
  ttnn_transpose_132 = ttnn.transpose(ttnn_permute_49, 3, 2, )
  ttnn_from_device_628 = ttnn.from_device(ttnn_permute_48, )
  ttnn_to_layout_631 = ttnn.to_layout(ttnn_from_device_628, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_365 = ttnn.reshape(ttnn_to_layout_631, (16, 9, 64), )
  ttnn_from_device_629 = ttnn.from_device(ttnn_transpose_132, )
  ttnn_to_layout_632 = ttnn.to_layout(ttnn_from_device_629, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_366 = ttnn.reshape(ttnn_to_layout_632, (16, 64, 9), )
  ttnn_from_device_630 = ttnn.from_device(ttnn_reshape_365, )
  ttnn_to_layout_633 = ttnn.to_layout(ttnn_from_device_630, ttnn.TILE_LAYOUT, )
  ttnn_to_device_298 = ttnn.to_device(ttnn_to_layout_633, device = device)
  ttnn_from_device_631 = ttnn.from_device(ttnn_reshape_366, )
  ttnn_to_layout_634 = ttnn.to_layout(ttnn_from_device_631, ttnn.TILE_LAYOUT, )
  ttnn_to_device_299 = ttnn.to_device(ttnn_to_layout_634, device = device)
  ttnn_matmul_132 = ttnn.matmul(ttnn_to_device_298, ttnn_to_device_299, )
  ttnn_from_device_632 = ttnn.from_device(ttnn_matmul_132, )
  ttnn_to_layout_635 = ttnn.to_layout(ttnn_from_device_632, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_367 = ttnn.reshape(ttnn_to_layout_635, (1, 16, 9, 9), )
  ttnn_from_device_633 = ttnn.from_device(ttnn_reshape_367, )
  ttnn_to_layout_636 = ttnn.to_layout(ttnn_from_device_633, ttnn.TILE_LAYOUT, )
  ttnn_to_device_300 = ttnn.to_device(ttnn_to_layout_636, device = device)
  ttnn_multiply_81 = ttnn.multiply(ttnn_to_device_300, 0.125, )
  ttnn_add_229 = ttnn.add(ttnn_multiply_81, ttnn_multiply, )
  ttnn_softmax_16 = ttnn.softmax(ttnn_add_229, -1, numeric_stable = True)
  test_accuracy(_softmax_16, ttnn_softmax_16)
  ttnn_prefix_clone_49 = clone_wrapper(ttnn_softmax_16, )
  ttnn_from_device_634 = ttnn.from_device(ttnn_prefix_clone_49, )
  ttnn_to_layout_637 = ttnn.to_layout(ttnn_from_device_634, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_368 = ttnn.reshape(ttnn_to_layout_637, (16, 9, 9), )
  ttnn_from_device_635 = ttnn.from_device(ttnn_permute_50, )
  ttnn_to_layout_638 = ttnn.to_layout(ttnn_from_device_635, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_369 = ttnn.reshape(ttnn_to_layout_638, (16, 9, 64), )
  ttnn_from_device_636 = ttnn.from_device(ttnn_reshape_368, )
  ttnn_to_layout_639 = ttnn.to_layout(ttnn_from_device_636, ttnn.TILE_LAYOUT, )
  ttnn_to_device_301 = ttnn.to_device(ttnn_to_layout_639, device = device)
  ttnn_from_device_637 = ttnn.from_device(ttnn_reshape_369, )
  ttnn_to_layout_640 = ttnn.to_layout(ttnn_from_device_637, ttnn.TILE_LAYOUT, )
  ttnn_to_device_302 = ttnn.to_device(ttnn_to_layout_640, device = device)
  ttnn_matmul_133 = ttnn.matmul(ttnn_to_device_301, ttnn_to_device_302, )
  ttnn_from_device_638 = ttnn.from_device(ttnn_matmul_133, )
  ttnn_to_layout_641 = ttnn.to_layout(ttnn_from_device_638, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_370 = ttnn.reshape(ttnn_to_layout_641, (1, 16, 9, 64), )
  ttnn_from_device_639 = ttnn.from_device(ttnn_reshape_370, )
  ttnn_to_layout_642 = ttnn.to_layout(ttnn_from_device_639, ttnn.TILE_LAYOUT, )
  ttnn_to_device_303 = ttnn.to_device(ttnn_to_layout_642, device = device)
  ttnn_transpose_133 = ttnn.transpose(ttnn_to_device_303, 2, 1, )
  ttnn_prefix_clone_50 = clone_wrapper(ttnn_transpose_133, )
  ttnn_from_device_640 = ttnn.from_device(ttnn_prefix_clone_50, )
  ttnn_to_layout_643 = ttnn.to_layout(ttnn_from_device_640, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_371 = ttnn.reshape(ttnn_to_layout_643, (1, 9, 1024), )
  ttnn_from_device_641 = ttnn.from_device(ttnn_reshape_371, )
  ttnn_to_layout_644 = ttnn.to_layout(ttnn_from_device_641, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_372 = ttnn.reshape(ttnn_to_layout_644, (9, 1024), )
  ttnn_from_device_642 = ttnn.from_device(ttnn_reshape_372, )
  ttnn_to_layout_645 = ttnn.to_layout(ttnn_from_device_642, ttnn.TILE_LAYOUT, )
  ttnn_to_device_304 = ttnn.to_device(ttnn_to_layout_645, device = device)
  ttnn_matmul_134 = ttnn.matmul(ttnn_to_device_304, ttnn_transpose_6, )
  ttnn_add_100 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_134, )
  ttnn_from_device_643 = ttnn.from_device(ttnn_add_100, )
  ttnn_to_layout_646 = ttnn.to_layout(ttnn_from_device_643, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_373 = ttnn.reshape(ttnn_to_layout_646, (1, 9, 1024), )
  ttnn_from_device_644 = ttnn.from_device(ttnn_reshape_373, )
  ttnn_to_layout_647 = ttnn.to_layout(ttnn_from_device_644, ttnn.TILE_LAYOUT, )
  ttnn_to_device_305 = ttnn.to_device(ttnn_to_layout_647, device = device)
  ttnn_prefix_clone_51 = clone_wrapper(ttnn_to_device_305, )
  ttnn_add_230 = ttnn.add(ttnn_layer_norm_32, ttnn_prefix_clone_51, )
  ttnn_layer_norm_33_ = ttnn.layer_norm(ttnn_add_230, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_33_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_230), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_33 = ttnn.from_torch(ttnn_layer_norm_33_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_33_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_33_), ttnn_layer_norm_33))
  ttnn_from_device_645 = ttnn.from_device(ttnn_layer_norm_33, )
  ttnn_to_layout_648 = ttnn.to_layout(ttnn_from_device_645, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_374 = ttnn.reshape(ttnn_to_layout_648, (9, 1024), )
  ttnn_from_device_646 = ttnn.from_device(ttnn_reshape_374, )
  ttnn_to_layout_649 = ttnn.to_layout(ttnn_from_device_646, ttnn.TILE_LAYOUT, )
  ttnn_to_device_306 = ttnn.to_device(ttnn_to_layout_649, device = device)
  ttnn_matmul_135 = ttnn.matmul(ttnn_to_device_306, ttnn_transpose_7, )
  ttnn_add_101 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_135, )
  ttnn_from_device_647 = ttnn.from_device(ttnn_add_101, )
  ttnn_to_layout_650 = ttnn.to_layout(ttnn_from_device_647, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_375 = ttnn.reshape(ttnn_to_layout_650, (1, 9, 4096), )
  ttnn_from_device_648 = ttnn.from_device(ttnn_reshape_375, )
  ttnn_to_layout_651 = ttnn.to_layout(ttnn_from_device_648, ttnn.TILE_LAYOUT, )
  ttnn_to_device_307 = ttnn.to_device(ttnn_to_layout_651, device = device)
  ttnn_multiply_82 = ttnn.multiply(ttnn_to_device_307, 0.5, )
  ttnn_pow_16 = ttnn.pow(ttnn_to_device_307, 3.0, )
  ttnn_multiply_83 = ttnn.multiply(ttnn_pow_16, 0.044715, )
  ttnn_add_231 = ttnn.add(ttnn_to_device_307, ttnn_multiply_83, )
  ttnn_multiply_84 = ttnn.multiply(ttnn_add_231, 0.7978845608028654, )
  ttnn_tanh_16_ = ttnn.tanh(ttnn_multiply_84, )
  ttnn_tanh_16_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_84), )
  ttnn_tanh_16 = ttnn.from_torch(ttnn_tanh_16_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_16_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_16_), ttnn_tanh_16))
  test_accuracy(tanh_16, ttnn_tanh_16)
  ttnn_add_232 = ttnn.add(ttnn_tanh_16, 1.0, )
  ttnn_multiply_85 = ttnn.multiply(ttnn_multiply_82, ttnn_add_232, )
  ttnn_from_device_649 = ttnn.from_device(ttnn_multiply_85, )
  ttnn_to_layout_652 = ttnn.to_layout(ttnn_from_device_649, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_376 = ttnn.reshape(ttnn_to_layout_652, (9, 4096), )
  ttnn_from_device_650 = ttnn.from_device(ttnn_reshape_376, )
  ttnn_to_layout_653 = ttnn.to_layout(ttnn_from_device_650, ttnn.TILE_LAYOUT, )
  ttnn_to_device_308 = ttnn.to_device(ttnn_to_layout_653, device = device)
  ttnn_matmul_136 = ttnn.matmul(ttnn_to_device_308, ttnn_transpose_8, )
  ttnn_add_102 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_136, )
  ttnn_from_device_651 = ttnn.from_device(ttnn_add_102, )
  ttnn_to_layout_654 = ttnn.to_layout(ttnn_from_device_651, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_377 = ttnn.reshape(ttnn_to_layout_654, (1, 9, 1024), )
  ttnn_from_device_652 = ttnn.from_device(ttnn_reshape_377, )
  ttnn_to_layout_655 = ttnn.to_layout(ttnn_from_device_652, ttnn.TILE_LAYOUT, )
  ttnn_to_device_309 = ttnn.to_device(ttnn_to_layout_655, device = device)
  ttnn_add_233 = ttnn.add(ttnn_to_device_309, ttnn_layer_norm_33, )
  ttnn_layer_norm_34_ = ttnn.layer_norm(ttnn_add_233, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_34_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_233), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_34 = ttnn.from_torch(ttnn_layer_norm_34_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_34_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_34_), ttnn_layer_norm_34))
  ttnn_from_device_653 = ttnn.from_device(ttnn_layer_norm_34, )
  ttnn_to_layout_656 = ttnn.to_layout(ttnn_from_device_653, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_378 = ttnn.reshape(ttnn_to_layout_656, (9, 1024), )
  ttnn_from_device_654 = ttnn.from_device(ttnn_reshape_378, )
  ttnn_to_layout_657 = ttnn.to_layout(ttnn_from_device_654, ttnn.TILE_LAYOUT, )
  ttnn_to_device_310 = ttnn.to_device(ttnn_to_layout_657, device = device)
  ttnn_matmul_137 = ttnn.matmul(ttnn_to_device_310, ttnn_transpose_1, )
  ttnn_add_103 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_137, )
  ttnn_from_device_655 = ttnn.from_device(ttnn_add_103, )
  ttnn_to_layout_658 = ttnn.to_layout(ttnn_from_device_655, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_379 = ttnn.reshape(ttnn_to_layout_658, (1, 9, 1024), )
  ttnn_to_layout_659 = ttnn.to_layout(ttnn_from_device_654, ttnn.TILE_LAYOUT, )
  ttnn_to_device_311 = ttnn.to_device(ttnn_to_layout_659, device = device)
  ttnn_matmul_138 = ttnn.matmul(ttnn_to_device_311, ttnn_transpose_2, )
  ttnn_add_104 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_138, )
  ttnn_from_device_657 = ttnn.from_device(ttnn_add_104, )
  ttnn_to_layout_660 = ttnn.to_layout(ttnn_from_device_657, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_381 = ttnn.reshape(ttnn_to_layout_660, (1, 9, 1024), )
  ttnn_to_layout_661 = ttnn.to_layout(ttnn_from_device_654, ttnn.TILE_LAYOUT, )
  ttnn_to_device_312 = ttnn.to_device(ttnn_to_layout_661, device = device)
  ttnn_matmul_139 = ttnn.matmul(ttnn_to_device_312, ttnn_transpose_3, )
  ttnn_add_105 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_139, )
  ttnn_from_device_659 = ttnn.from_device(ttnn_add_105, )
  ttnn_to_layout_662 = ttnn.to_layout(ttnn_from_device_659, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_383 = ttnn.reshape(ttnn_to_layout_662, (1, 9, 1024), )
  ttnn_from_device_660 = ttnn.from_device(ttnn_reshape_379, )
  ttnn_to_layout_663 = ttnn.to_layout(ttnn_from_device_660, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_384 = ttnn.reshape(ttnn_to_layout_663, (1, 9, 16, 64), )
  ttnn_from_device_661 = ttnn.from_device(ttnn_reshape_384, )
  ttnn_to_layout_664 = ttnn.to_layout(ttnn_from_device_661, ttnn.TILE_LAYOUT, )
  ttnn_to_device_313 = ttnn.to_device(ttnn_to_layout_664, device = device)
  ttnn_permute_51 = ttnn.permute(ttnn_to_device_313, (0, 2, 1, 3), )
  ttnn_from_device_662 = ttnn.from_device(ttnn_reshape_381, )
  ttnn_to_layout_665 = ttnn.to_layout(ttnn_from_device_662, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_385 = ttnn.reshape(ttnn_to_layout_665, (1, 9, 16, 64), )
  ttnn_from_device_663 = ttnn.from_device(ttnn_reshape_385, )
  ttnn_to_layout_666 = ttnn.to_layout(ttnn_from_device_663, ttnn.TILE_LAYOUT, )
  ttnn_to_device_314 = ttnn.to_device(ttnn_to_layout_666, device = device)
  ttnn_permute_52 = ttnn.permute(ttnn_to_device_314, (0, 2, 1, 3), )
  ttnn_from_device_664 = ttnn.from_device(ttnn_reshape_383, )
  ttnn_to_layout_667 = ttnn.to_layout(ttnn_from_device_664, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_386 = ttnn.reshape(ttnn_to_layout_667, (1, 9, 16, 64), )
  ttnn_from_device_665 = ttnn.from_device(ttnn_reshape_386, )
  ttnn_to_layout_668 = ttnn.to_layout(ttnn_from_device_665, ttnn.TILE_LAYOUT, )
  ttnn_to_device_315 = ttnn.to_device(ttnn_to_layout_668, device = device)
  ttnn_permute_53 = ttnn.permute(ttnn_to_device_315, (0, 2, 1, 3), )
  ttnn_transpose_140 = ttnn.transpose(ttnn_permute_52, 3, 2, )
  ttnn_from_device_666 = ttnn.from_device(ttnn_permute_51, )
  ttnn_to_layout_669 = ttnn.to_layout(ttnn_from_device_666, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_387 = ttnn.reshape(ttnn_to_layout_669, (16, 9, 64), )
  ttnn_from_device_667 = ttnn.from_device(ttnn_transpose_140, )
  ttnn_to_layout_670 = ttnn.to_layout(ttnn_from_device_667, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_388 = ttnn.reshape(ttnn_to_layout_670, (16, 64, 9), )
  ttnn_from_device_668 = ttnn.from_device(ttnn_reshape_387, )
  ttnn_to_layout_671 = ttnn.to_layout(ttnn_from_device_668, ttnn.TILE_LAYOUT, )
  ttnn_to_device_316 = ttnn.to_device(ttnn_to_layout_671, device = device)
  ttnn_from_device_669 = ttnn.from_device(ttnn_reshape_388, )
  ttnn_to_layout_672 = ttnn.to_layout(ttnn_from_device_669, ttnn.TILE_LAYOUT, )
  ttnn_to_device_317 = ttnn.to_device(ttnn_to_layout_672, device = device)
  ttnn_matmul_140 = ttnn.matmul(ttnn_to_device_316, ttnn_to_device_317, )
  ttnn_from_device_670 = ttnn.from_device(ttnn_matmul_140, )
  ttnn_to_layout_673 = ttnn.to_layout(ttnn_from_device_670, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_389 = ttnn.reshape(ttnn_to_layout_673, (1, 16, 9, 9), )
  ttnn_from_device_671 = ttnn.from_device(ttnn_reshape_389, )
  ttnn_to_layout_674 = ttnn.to_layout(ttnn_from_device_671, ttnn.TILE_LAYOUT, )
  ttnn_to_device_318 = ttnn.to_device(ttnn_to_layout_674, device = device)
  ttnn_multiply_86 = ttnn.multiply(ttnn_to_device_318, 0.125, )
  ttnn_add_234 = ttnn.add(ttnn_multiply_86, ttnn_multiply, )
  ttnn_softmax_17 = ttnn.softmax(ttnn_add_234, -1, numeric_stable = True)
  test_accuracy(_softmax_17, ttnn_softmax_17)
  ttnn_prefix_clone_52 = clone_wrapper(ttnn_softmax_17, )
  ttnn_from_device_672 = ttnn.from_device(ttnn_prefix_clone_52, )
  ttnn_to_layout_675 = ttnn.to_layout(ttnn_from_device_672, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_390 = ttnn.reshape(ttnn_to_layout_675, (16, 9, 9), )
  ttnn_from_device_673 = ttnn.from_device(ttnn_permute_53, )
  ttnn_to_layout_676 = ttnn.to_layout(ttnn_from_device_673, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_391 = ttnn.reshape(ttnn_to_layout_676, (16, 9, 64), )
  ttnn_from_device_674 = ttnn.from_device(ttnn_reshape_390, )
  ttnn_to_layout_677 = ttnn.to_layout(ttnn_from_device_674, ttnn.TILE_LAYOUT, )
  ttnn_to_device_319 = ttnn.to_device(ttnn_to_layout_677, device = device)
  ttnn_from_device_675 = ttnn.from_device(ttnn_reshape_391, )
  ttnn_to_layout_678 = ttnn.to_layout(ttnn_from_device_675, ttnn.TILE_LAYOUT, )
  ttnn_to_device_320 = ttnn.to_device(ttnn_to_layout_678, device = device)
  ttnn_matmul_141 = ttnn.matmul(ttnn_to_device_319, ttnn_to_device_320, )
  ttnn_from_device_676 = ttnn.from_device(ttnn_matmul_141, )
  ttnn_to_layout_679 = ttnn.to_layout(ttnn_from_device_676, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_392 = ttnn.reshape(ttnn_to_layout_679, (1, 16, 9, 64), )
  ttnn_from_device_677 = ttnn.from_device(ttnn_reshape_392, )
  ttnn_to_layout_680 = ttnn.to_layout(ttnn_from_device_677, ttnn.TILE_LAYOUT, )
  ttnn_to_device_321 = ttnn.to_device(ttnn_to_layout_680, device = device)
  ttnn_transpose_141 = ttnn.transpose(ttnn_to_device_321, 2, 1, )
  ttnn_prefix_clone_53 = clone_wrapper(ttnn_transpose_141, )
  ttnn_from_device_678 = ttnn.from_device(ttnn_prefix_clone_53, )
  ttnn_to_layout_681 = ttnn.to_layout(ttnn_from_device_678, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_393 = ttnn.reshape(ttnn_to_layout_681, (1, 9, 1024), )
  ttnn_from_device_679 = ttnn.from_device(ttnn_reshape_393, )
  ttnn_to_layout_682 = ttnn.to_layout(ttnn_from_device_679, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_394 = ttnn.reshape(ttnn_to_layout_682, (9, 1024), )
  ttnn_from_device_680 = ttnn.from_device(ttnn_reshape_394, )
  ttnn_to_layout_683 = ttnn.to_layout(ttnn_from_device_680, ttnn.TILE_LAYOUT, )
  ttnn_to_device_322 = ttnn.to_device(ttnn_to_layout_683, device = device)
  ttnn_matmul_142 = ttnn.matmul(ttnn_to_device_322, ttnn_transpose_6, )
  ttnn_add_106 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_142, )
  ttnn_from_device_681 = ttnn.from_device(ttnn_add_106, )
  ttnn_to_layout_684 = ttnn.to_layout(ttnn_from_device_681, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_395 = ttnn.reshape(ttnn_to_layout_684, (1, 9, 1024), )
  ttnn_from_device_682 = ttnn.from_device(ttnn_reshape_395, )
  ttnn_to_layout_685 = ttnn.to_layout(ttnn_from_device_682, ttnn.TILE_LAYOUT, )
  ttnn_to_device_323 = ttnn.to_device(ttnn_to_layout_685, device = device)
  ttnn_prefix_clone_54 = clone_wrapper(ttnn_to_device_323, )
  ttnn_add_235 = ttnn.add(ttnn_layer_norm_34, ttnn_prefix_clone_54, )
  ttnn_layer_norm_35_ = ttnn.layer_norm(ttnn_add_235, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_35_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_235), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_35 = ttnn.from_torch(ttnn_layer_norm_35_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_35_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_35_), ttnn_layer_norm_35))
  ttnn_from_device_683 = ttnn.from_device(ttnn_layer_norm_35, )
  ttnn_to_layout_686 = ttnn.to_layout(ttnn_from_device_683, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_396 = ttnn.reshape(ttnn_to_layout_686, (9, 1024), )
  ttnn_from_device_684 = ttnn.from_device(ttnn_reshape_396, )
  ttnn_to_layout_687 = ttnn.to_layout(ttnn_from_device_684, ttnn.TILE_LAYOUT, )
  ttnn_to_device_324 = ttnn.to_device(ttnn_to_layout_687, device = device)
  ttnn_matmul_143 = ttnn.matmul(ttnn_to_device_324, ttnn_transpose_7, )
  ttnn_add_107 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_143, )
  ttnn_from_device_685 = ttnn.from_device(ttnn_add_107, )
  ttnn_to_layout_688 = ttnn.to_layout(ttnn_from_device_685, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_397 = ttnn.reshape(ttnn_to_layout_688, (1, 9, 4096), )
  ttnn_from_device_686 = ttnn.from_device(ttnn_reshape_397, )
  ttnn_to_layout_689 = ttnn.to_layout(ttnn_from_device_686, ttnn.TILE_LAYOUT, )
  ttnn_to_device_325 = ttnn.to_device(ttnn_to_layout_689, device = device)
  ttnn_multiply_87 = ttnn.multiply(ttnn_to_device_325, 0.5, )
  ttnn_pow_17 = ttnn.pow(ttnn_to_device_325, 3.0, )
  ttnn_multiply_88 = ttnn.multiply(ttnn_pow_17, 0.044715, )
  ttnn_add_236 = ttnn.add(ttnn_to_device_325, ttnn_multiply_88, )
  ttnn_multiply_89 = ttnn.multiply(ttnn_add_236, 0.7978845608028654, )
  ttnn_tanh_17_ = ttnn.tanh(ttnn_multiply_89, )
  ttnn_tanh_17_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_89), )
  ttnn_tanh_17 = ttnn.from_torch(ttnn_tanh_17_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_17_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_17_), ttnn_tanh_17))
  test_accuracy(tanh_17, ttnn_tanh_17)
  ttnn_add_237 = ttnn.add(ttnn_tanh_17, 1.0, )
  ttnn_multiply_90 = ttnn.multiply(ttnn_multiply_87, ttnn_add_237, )
  ttnn_from_device_687 = ttnn.from_device(ttnn_multiply_90, )
  ttnn_to_layout_690 = ttnn.to_layout(ttnn_from_device_687, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_398 = ttnn.reshape(ttnn_to_layout_690, (9, 4096), )
  ttnn_from_device_688 = ttnn.from_device(ttnn_reshape_398, )
  ttnn_to_layout_691 = ttnn.to_layout(ttnn_from_device_688, ttnn.TILE_LAYOUT, )
  ttnn_to_device_326 = ttnn.to_device(ttnn_to_layout_691, device = device)
  ttnn_matmul_144 = ttnn.matmul(ttnn_to_device_326, ttnn_transpose_8, )
  ttnn_add_108 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_144, )
  ttnn_from_device_689 = ttnn.from_device(ttnn_add_108, )
  ttnn_to_layout_692 = ttnn.to_layout(ttnn_from_device_689, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_399 = ttnn.reshape(ttnn_to_layout_692, (1, 9, 1024), )
  ttnn_from_device_690 = ttnn.from_device(ttnn_reshape_399, )
  ttnn_to_layout_693 = ttnn.to_layout(ttnn_from_device_690, ttnn.TILE_LAYOUT, )
  ttnn_to_device_327 = ttnn.to_device(ttnn_to_layout_693, device = device)
  ttnn_add_238 = ttnn.add(ttnn_to_device_327, ttnn_layer_norm_35, )
  ttnn_layer_norm_36_ = ttnn.layer_norm(ttnn_add_238, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_36_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_238), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_36 = ttnn.from_torch(ttnn_layer_norm_36_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_36_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_36_), ttnn_layer_norm_36))
  ttnn_from_device_691 = ttnn.from_device(ttnn_layer_norm_36, )
  ttnn_to_layout_694 = ttnn.to_layout(ttnn_from_device_691, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_400 = ttnn.reshape(ttnn_to_layout_694, (9, 1024), )
  ttnn_from_device_692 = ttnn.from_device(ttnn_reshape_400, )
  ttnn_to_layout_695 = ttnn.to_layout(ttnn_from_device_692, ttnn.TILE_LAYOUT, )
  ttnn_to_device_328 = ttnn.to_device(ttnn_to_layout_695, device = device)
  ttnn_matmul_145 = ttnn.matmul(ttnn_to_device_328, ttnn_transpose_1, )
  ttnn_add_109 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_145, )
  ttnn_from_device_693 = ttnn.from_device(ttnn_add_109, )
  ttnn_to_layout_696 = ttnn.to_layout(ttnn_from_device_693, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_401 = ttnn.reshape(ttnn_to_layout_696, (1, 9, 1024), )
  ttnn_to_layout_697 = ttnn.to_layout(ttnn_from_device_692, ttnn.TILE_LAYOUT, )
  ttnn_to_device_329 = ttnn.to_device(ttnn_to_layout_697, device = device)
  ttnn_matmul_146 = ttnn.matmul(ttnn_to_device_329, ttnn_transpose_2, )
  ttnn_add_110 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_146, )
  ttnn_from_device_695 = ttnn.from_device(ttnn_add_110, )
  ttnn_to_layout_698 = ttnn.to_layout(ttnn_from_device_695, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_403 = ttnn.reshape(ttnn_to_layout_698, (1, 9, 1024), )
  ttnn_to_layout_699 = ttnn.to_layout(ttnn_from_device_692, ttnn.TILE_LAYOUT, )
  ttnn_to_device_330 = ttnn.to_device(ttnn_to_layout_699, device = device)
  ttnn_matmul_147 = ttnn.matmul(ttnn_to_device_330, ttnn_transpose_3, )
  ttnn_add_111 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_147, )
  ttnn_from_device_697 = ttnn.from_device(ttnn_add_111, )
  ttnn_to_layout_700 = ttnn.to_layout(ttnn_from_device_697, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_405 = ttnn.reshape(ttnn_to_layout_700, (1, 9, 1024), )
  ttnn_from_device_698 = ttnn.from_device(ttnn_reshape_401, )
  ttnn_to_layout_701 = ttnn.to_layout(ttnn_from_device_698, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_406 = ttnn.reshape(ttnn_to_layout_701, (1, 9, 16, 64), )
  ttnn_from_device_699 = ttnn.from_device(ttnn_reshape_406, )
  ttnn_to_layout_702 = ttnn.to_layout(ttnn_from_device_699, ttnn.TILE_LAYOUT, )
  ttnn_to_device_331 = ttnn.to_device(ttnn_to_layout_702, device = device)
  ttnn_permute_54 = ttnn.permute(ttnn_to_device_331, (0, 2, 1, 3), )
  ttnn_from_device_700 = ttnn.from_device(ttnn_reshape_403, )
  ttnn_to_layout_703 = ttnn.to_layout(ttnn_from_device_700, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_407 = ttnn.reshape(ttnn_to_layout_703, (1, 9, 16, 64), )
  ttnn_from_device_701 = ttnn.from_device(ttnn_reshape_407, )
  ttnn_to_layout_704 = ttnn.to_layout(ttnn_from_device_701, ttnn.TILE_LAYOUT, )
  ttnn_to_device_332 = ttnn.to_device(ttnn_to_layout_704, device = device)
  ttnn_permute_55 = ttnn.permute(ttnn_to_device_332, (0, 2, 1, 3), )
  ttnn_from_device_702 = ttnn.from_device(ttnn_reshape_405, )
  ttnn_to_layout_705 = ttnn.to_layout(ttnn_from_device_702, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_408 = ttnn.reshape(ttnn_to_layout_705, (1, 9, 16, 64), )
  ttnn_from_device_703 = ttnn.from_device(ttnn_reshape_408, )
  ttnn_to_layout_706 = ttnn.to_layout(ttnn_from_device_703, ttnn.TILE_LAYOUT, )
  ttnn_to_device_333 = ttnn.to_device(ttnn_to_layout_706, device = device)
  ttnn_permute_56 = ttnn.permute(ttnn_to_device_333, (0, 2, 1, 3), )
  ttnn_transpose_148 = ttnn.transpose(ttnn_permute_55, 3, 2, )
  ttnn_from_device_704 = ttnn.from_device(ttnn_permute_54, )
  ttnn_to_layout_707 = ttnn.to_layout(ttnn_from_device_704, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_409 = ttnn.reshape(ttnn_to_layout_707, (16, 9, 64), )
  ttnn_from_device_705 = ttnn.from_device(ttnn_transpose_148, )
  ttnn_to_layout_708 = ttnn.to_layout(ttnn_from_device_705, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_410 = ttnn.reshape(ttnn_to_layout_708, (16, 64, 9), )
  ttnn_from_device_706 = ttnn.from_device(ttnn_reshape_409, )
  ttnn_to_layout_709 = ttnn.to_layout(ttnn_from_device_706, ttnn.TILE_LAYOUT, )
  ttnn_to_device_334 = ttnn.to_device(ttnn_to_layout_709, device = device)
  ttnn_from_device_707 = ttnn.from_device(ttnn_reshape_410, )
  ttnn_to_layout_710 = ttnn.to_layout(ttnn_from_device_707, ttnn.TILE_LAYOUT, )
  ttnn_to_device_335 = ttnn.to_device(ttnn_to_layout_710, device = device)
  ttnn_matmul_148 = ttnn.matmul(ttnn_to_device_334, ttnn_to_device_335, )
  ttnn_from_device_708 = ttnn.from_device(ttnn_matmul_148, )
  ttnn_to_layout_711 = ttnn.to_layout(ttnn_from_device_708, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_411 = ttnn.reshape(ttnn_to_layout_711, (1, 16, 9, 9), )
  ttnn_from_device_709 = ttnn.from_device(ttnn_reshape_411, )
  ttnn_to_layout_712 = ttnn.to_layout(ttnn_from_device_709, ttnn.TILE_LAYOUT, )
  ttnn_to_device_336 = ttnn.to_device(ttnn_to_layout_712, device = device)
  ttnn_multiply_91 = ttnn.multiply(ttnn_to_device_336, 0.125, )
  ttnn_add_239 = ttnn.add(ttnn_multiply_91, ttnn_multiply, )
  ttnn_softmax_18 = ttnn.softmax(ttnn_add_239, -1, numeric_stable = True)
  test_accuracy(_softmax_18, ttnn_softmax_18)
  ttnn_prefix_clone_55 = clone_wrapper(ttnn_softmax_18, )
  ttnn_from_device_710 = ttnn.from_device(ttnn_prefix_clone_55, )
  ttnn_to_layout_713 = ttnn.to_layout(ttnn_from_device_710, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_412 = ttnn.reshape(ttnn_to_layout_713, (16, 9, 9), )
  ttnn_from_device_711 = ttnn.from_device(ttnn_permute_56, )
  ttnn_to_layout_714 = ttnn.to_layout(ttnn_from_device_711, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_413 = ttnn.reshape(ttnn_to_layout_714, (16, 9, 64), )
  ttnn_from_device_712 = ttnn.from_device(ttnn_reshape_412, )
  ttnn_to_layout_715 = ttnn.to_layout(ttnn_from_device_712, ttnn.TILE_LAYOUT, )
  ttnn_to_device_337 = ttnn.to_device(ttnn_to_layout_715, device = device)
  ttnn_from_device_713 = ttnn.from_device(ttnn_reshape_413, )
  ttnn_to_layout_716 = ttnn.to_layout(ttnn_from_device_713, ttnn.TILE_LAYOUT, )
  ttnn_to_device_338 = ttnn.to_device(ttnn_to_layout_716, device = device)
  ttnn_matmul_149 = ttnn.matmul(ttnn_to_device_337, ttnn_to_device_338, )
  ttnn_from_device_714 = ttnn.from_device(ttnn_matmul_149, )
  ttnn_to_layout_717 = ttnn.to_layout(ttnn_from_device_714, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_414 = ttnn.reshape(ttnn_to_layout_717, (1, 16, 9, 64), )
  ttnn_from_device_715 = ttnn.from_device(ttnn_reshape_414, )
  ttnn_to_layout_718 = ttnn.to_layout(ttnn_from_device_715, ttnn.TILE_LAYOUT, )
  ttnn_to_device_339 = ttnn.to_device(ttnn_to_layout_718, device = device)
  ttnn_transpose_149 = ttnn.transpose(ttnn_to_device_339, 2, 1, )
  ttnn_prefix_clone_56 = clone_wrapper(ttnn_transpose_149, )
  ttnn_from_device_716 = ttnn.from_device(ttnn_prefix_clone_56, )
  ttnn_to_layout_719 = ttnn.to_layout(ttnn_from_device_716, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_415 = ttnn.reshape(ttnn_to_layout_719, (1, 9, 1024), )
  ttnn_from_device_717 = ttnn.from_device(ttnn_reshape_415, )
  ttnn_to_layout_720 = ttnn.to_layout(ttnn_from_device_717, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_416 = ttnn.reshape(ttnn_to_layout_720, (9, 1024), )
  ttnn_from_device_718 = ttnn.from_device(ttnn_reshape_416, )
  ttnn_to_layout_721 = ttnn.to_layout(ttnn_from_device_718, ttnn.TILE_LAYOUT, )
  ttnn_to_device_340 = ttnn.to_device(ttnn_to_layout_721, device = device)
  ttnn_matmul_150 = ttnn.matmul(ttnn_to_device_340, ttnn_transpose_6, )
  ttnn_add_112 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_150, )
  ttnn_from_device_719 = ttnn.from_device(ttnn_add_112, )
  ttnn_to_layout_722 = ttnn.to_layout(ttnn_from_device_719, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_417 = ttnn.reshape(ttnn_to_layout_722, (1, 9, 1024), )
  ttnn_from_device_720 = ttnn.from_device(ttnn_reshape_417, )
  ttnn_to_layout_723 = ttnn.to_layout(ttnn_from_device_720, ttnn.TILE_LAYOUT, )
  ttnn_to_device_341 = ttnn.to_device(ttnn_to_layout_723, device = device)
  ttnn_prefix_clone_57 = clone_wrapper(ttnn_to_device_341, )
  ttnn_add_240 = ttnn.add(ttnn_layer_norm_36, ttnn_prefix_clone_57, )
  ttnn_layer_norm_37_ = ttnn.layer_norm(ttnn_add_240, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_37_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_240), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_37 = ttnn.from_torch(ttnn_layer_norm_37_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_37_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_37_), ttnn_layer_norm_37))
  ttnn_from_device_721 = ttnn.from_device(ttnn_layer_norm_37, )
  ttnn_to_layout_724 = ttnn.to_layout(ttnn_from_device_721, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_418 = ttnn.reshape(ttnn_to_layout_724, (9, 1024), )
  ttnn_from_device_722 = ttnn.from_device(ttnn_reshape_418, )
  ttnn_to_layout_725 = ttnn.to_layout(ttnn_from_device_722, ttnn.TILE_LAYOUT, )
  ttnn_to_device_342 = ttnn.to_device(ttnn_to_layout_725, device = device)
  ttnn_matmul_151 = ttnn.matmul(ttnn_to_device_342, ttnn_transpose_7, )
  ttnn_add_113 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_151, )
  ttnn_from_device_723 = ttnn.from_device(ttnn_add_113, )
  ttnn_to_layout_726 = ttnn.to_layout(ttnn_from_device_723, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_419 = ttnn.reshape(ttnn_to_layout_726, (1, 9, 4096), )
  ttnn_from_device_724 = ttnn.from_device(ttnn_reshape_419, )
  ttnn_to_layout_727 = ttnn.to_layout(ttnn_from_device_724, ttnn.TILE_LAYOUT, )
  ttnn_to_device_343 = ttnn.to_device(ttnn_to_layout_727, device = device)
  ttnn_multiply_92 = ttnn.multiply(ttnn_to_device_343, 0.5, )
  ttnn_pow_18 = ttnn.pow(ttnn_to_device_343, 3.0, )
  ttnn_multiply_93 = ttnn.multiply(ttnn_pow_18, 0.044715, )
  ttnn_add_241 = ttnn.add(ttnn_to_device_343, ttnn_multiply_93, )
  ttnn_multiply_94 = ttnn.multiply(ttnn_add_241, 0.7978845608028654, )
  ttnn_tanh_18_ = ttnn.tanh(ttnn_multiply_94, )
  ttnn_tanh_18_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_94), )
  ttnn_tanh_18 = ttnn.from_torch(ttnn_tanh_18_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_18_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_18_), ttnn_tanh_18))
  test_accuracy(tanh_18, ttnn_tanh_18)
  ttnn_add_242 = ttnn.add(ttnn_tanh_18, 1.0, )
  ttnn_multiply_95 = ttnn.multiply(ttnn_multiply_92, ttnn_add_242, )
  ttnn_from_device_725 = ttnn.from_device(ttnn_multiply_95, )
  ttnn_to_layout_728 = ttnn.to_layout(ttnn_from_device_725, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_420 = ttnn.reshape(ttnn_to_layout_728, (9, 4096), )
  ttnn_from_device_726 = ttnn.from_device(ttnn_reshape_420, )
  ttnn_to_layout_729 = ttnn.to_layout(ttnn_from_device_726, ttnn.TILE_LAYOUT, )
  ttnn_to_device_344 = ttnn.to_device(ttnn_to_layout_729, device = device)
  ttnn_matmul_152 = ttnn.matmul(ttnn_to_device_344, ttnn_transpose_8, )
  ttnn_add_114 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_152, )
  ttnn_from_device_727 = ttnn.from_device(ttnn_add_114, )
  ttnn_to_layout_730 = ttnn.to_layout(ttnn_from_device_727, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_421 = ttnn.reshape(ttnn_to_layout_730, (1, 9, 1024), )
  ttnn_from_device_728 = ttnn.from_device(ttnn_reshape_421, )
  ttnn_to_layout_731 = ttnn.to_layout(ttnn_from_device_728, ttnn.TILE_LAYOUT, )
  ttnn_to_device_345 = ttnn.to_device(ttnn_to_layout_731, device = device)
  ttnn_add_243 = ttnn.add(ttnn_to_device_345, ttnn_layer_norm_37, )
  ttnn_layer_norm_38_ = ttnn.layer_norm(ttnn_add_243, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_38_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_243), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_38 = ttnn.from_torch(ttnn_layer_norm_38_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_38_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_38_), ttnn_layer_norm_38))
  ttnn_from_device_729 = ttnn.from_device(ttnn_layer_norm_38, )
  ttnn_to_layout_732 = ttnn.to_layout(ttnn_from_device_729, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_422 = ttnn.reshape(ttnn_to_layout_732, (9, 1024), )
  ttnn_from_device_730 = ttnn.from_device(ttnn_reshape_422, )
  ttnn_to_layout_733 = ttnn.to_layout(ttnn_from_device_730, ttnn.TILE_LAYOUT, )
  ttnn_to_device_346 = ttnn.to_device(ttnn_to_layout_733, device = device)
  ttnn_matmul_153 = ttnn.matmul(ttnn_to_device_346, ttnn_transpose_1, )
  ttnn_add_115 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_153, )
  ttnn_from_device_731 = ttnn.from_device(ttnn_add_115, )
  ttnn_to_layout_734 = ttnn.to_layout(ttnn_from_device_731, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_423 = ttnn.reshape(ttnn_to_layout_734, (1, 9, 1024), )
  ttnn_to_layout_735 = ttnn.to_layout(ttnn_from_device_730, ttnn.TILE_LAYOUT, )
  ttnn_to_device_347 = ttnn.to_device(ttnn_to_layout_735, device = device)
  ttnn_matmul_154 = ttnn.matmul(ttnn_to_device_347, ttnn_transpose_2, )
  ttnn_add_116 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_154, )
  ttnn_from_device_733 = ttnn.from_device(ttnn_add_116, )
  ttnn_to_layout_736 = ttnn.to_layout(ttnn_from_device_733, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_425 = ttnn.reshape(ttnn_to_layout_736, (1, 9, 1024), )
  ttnn_to_layout_737 = ttnn.to_layout(ttnn_from_device_730, ttnn.TILE_LAYOUT, )
  ttnn_to_device_348 = ttnn.to_device(ttnn_to_layout_737, device = device)
  ttnn_matmul_155 = ttnn.matmul(ttnn_to_device_348, ttnn_transpose_3, )
  ttnn_add_117 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_155, )
  ttnn_from_device_735 = ttnn.from_device(ttnn_add_117, )
  ttnn_to_layout_738 = ttnn.to_layout(ttnn_from_device_735, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_427 = ttnn.reshape(ttnn_to_layout_738, (1, 9, 1024), )
  ttnn_from_device_736 = ttnn.from_device(ttnn_reshape_423, )
  ttnn_to_layout_739 = ttnn.to_layout(ttnn_from_device_736, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_428 = ttnn.reshape(ttnn_to_layout_739, (1, 9, 16, 64), )
  ttnn_from_device_737 = ttnn.from_device(ttnn_reshape_428, )
  ttnn_to_layout_740 = ttnn.to_layout(ttnn_from_device_737, ttnn.TILE_LAYOUT, )
  ttnn_to_device_349 = ttnn.to_device(ttnn_to_layout_740, device = device)
  ttnn_permute_57 = ttnn.permute(ttnn_to_device_349, (0, 2, 1, 3), )
  ttnn_from_device_738 = ttnn.from_device(ttnn_reshape_425, )
  ttnn_to_layout_741 = ttnn.to_layout(ttnn_from_device_738, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_429 = ttnn.reshape(ttnn_to_layout_741, (1, 9, 16, 64), )
  ttnn_from_device_739 = ttnn.from_device(ttnn_reshape_429, )
  ttnn_to_layout_742 = ttnn.to_layout(ttnn_from_device_739, ttnn.TILE_LAYOUT, )
  ttnn_to_device_350 = ttnn.to_device(ttnn_to_layout_742, device = device)
  ttnn_permute_58 = ttnn.permute(ttnn_to_device_350, (0, 2, 1, 3), )
  ttnn_from_device_740 = ttnn.from_device(ttnn_reshape_427, )
  ttnn_to_layout_743 = ttnn.to_layout(ttnn_from_device_740, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_430 = ttnn.reshape(ttnn_to_layout_743, (1, 9, 16, 64), )
  ttnn_from_device_741 = ttnn.from_device(ttnn_reshape_430, )
  ttnn_to_layout_744 = ttnn.to_layout(ttnn_from_device_741, ttnn.TILE_LAYOUT, )
  ttnn_to_device_351 = ttnn.to_device(ttnn_to_layout_744, device = device)
  ttnn_permute_59 = ttnn.permute(ttnn_to_device_351, (0, 2, 1, 3), )
  ttnn_transpose_156 = ttnn.transpose(ttnn_permute_58, 3, 2, )
  ttnn_from_device_742 = ttnn.from_device(ttnn_permute_57, )
  ttnn_to_layout_745 = ttnn.to_layout(ttnn_from_device_742, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_431 = ttnn.reshape(ttnn_to_layout_745, (16, 9, 64), )
  ttnn_from_device_743 = ttnn.from_device(ttnn_transpose_156, )
  ttnn_to_layout_746 = ttnn.to_layout(ttnn_from_device_743, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_432 = ttnn.reshape(ttnn_to_layout_746, (16, 64, 9), )
  ttnn_from_device_744 = ttnn.from_device(ttnn_reshape_431, )
  ttnn_to_layout_747 = ttnn.to_layout(ttnn_from_device_744, ttnn.TILE_LAYOUT, )
  ttnn_to_device_352 = ttnn.to_device(ttnn_to_layout_747, device = device)
  ttnn_from_device_745 = ttnn.from_device(ttnn_reshape_432, )
  ttnn_to_layout_748 = ttnn.to_layout(ttnn_from_device_745, ttnn.TILE_LAYOUT, )
  ttnn_to_device_353 = ttnn.to_device(ttnn_to_layout_748, device = device)
  ttnn_matmul_156 = ttnn.matmul(ttnn_to_device_352, ttnn_to_device_353, )
  ttnn_from_device_746 = ttnn.from_device(ttnn_matmul_156, )
  ttnn_to_layout_749 = ttnn.to_layout(ttnn_from_device_746, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_433 = ttnn.reshape(ttnn_to_layout_749, (1, 16, 9, 9), )
  ttnn_from_device_747 = ttnn.from_device(ttnn_reshape_433, )
  ttnn_to_layout_750 = ttnn.to_layout(ttnn_from_device_747, ttnn.TILE_LAYOUT, )
  ttnn_to_device_354 = ttnn.to_device(ttnn_to_layout_750, device = device)
  ttnn_multiply_96 = ttnn.multiply(ttnn_to_device_354, 0.125, )
  ttnn_add_244 = ttnn.add(ttnn_multiply_96, ttnn_multiply, )
  ttnn_softmax_19 = ttnn.softmax(ttnn_add_244, -1, numeric_stable = True)
  test_accuracy(_softmax_19, ttnn_softmax_19)
  ttnn_prefix_clone_58 = clone_wrapper(ttnn_softmax_19, )
  ttnn_from_device_748 = ttnn.from_device(ttnn_prefix_clone_58, )
  ttnn_to_layout_751 = ttnn.to_layout(ttnn_from_device_748, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_434 = ttnn.reshape(ttnn_to_layout_751, (16, 9, 9), )
  ttnn_from_device_749 = ttnn.from_device(ttnn_permute_59, )
  ttnn_to_layout_752 = ttnn.to_layout(ttnn_from_device_749, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_435 = ttnn.reshape(ttnn_to_layout_752, (16, 9, 64), )
  ttnn_from_device_750 = ttnn.from_device(ttnn_reshape_434, )
  ttnn_to_layout_753 = ttnn.to_layout(ttnn_from_device_750, ttnn.TILE_LAYOUT, )
  ttnn_to_device_355 = ttnn.to_device(ttnn_to_layout_753, device = device)
  ttnn_from_device_751 = ttnn.from_device(ttnn_reshape_435, )
  ttnn_to_layout_754 = ttnn.to_layout(ttnn_from_device_751, ttnn.TILE_LAYOUT, )
  ttnn_to_device_356 = ttnn.to_device(ttnn_to_layout_754, device = device)
  ttnn_matmul_157 = ttnn.matmul(ttnn_to_device_355, ttnn_to_device_356, )
  ttnn_from_device_752 = ttnn.from_device(ttnn_matmul_157, )
  ttnn_to_layout_755 = ttnn.to_layout(ttnn_from_device_752, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_436 = ttnn.reshape(ttnn_to_layout_755, (1, 16, 9, 64), )
  ttnn_from_device_753 = ttnn.from_device(ttnn_reshape_436, )
  ttnn_to_layout_756 = ttnn.to_layout(ttnn_from_device_753, ttnn.TILE_LAYOUT, )
  ttnn_to_device_357 = ttnn.to_device(ttnn_to_layout_756, device = device)
  ttnn_transpose_157 = ttnn.transpose(ttnn_to_device_357, 2, 1, )
  ttnn_prefix_clone_59 = clone_wrapper(ttnn_transpose_157, )
  ttnn_from_device_754 = ttnn.from_device(ttnn_prefix_clone_59, )
  ttnn_to_layout_757 = ttnn.to_layout(ttnn_from_device_754, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_437 = ttnn.reshape(ttnn_to_layout_757, (1, 9, 1024), )
  ttnn_from_device_755 = ttnn.from_device(ttnn_reshape_437, )
  ttnn_to_layout_758 = ttnn.to_layout(ttnn_from_device_755, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_438 = ttnn.reshape(ttnn_to_layout_758, (9, 1024), )
  ttnn_from_device_756 = ttnn.from_device(ttnn_reshape_438, )
  ttnn_to_layout_759 = ttnn.to_layout(ttnn_from_device_756, ttnn.TILE_LAYOUT, )
  ttnn_to_device_358 = ttnn.to_device(ttnn_to_layout_759, device = device)
  ttnn_matmul_158 = ttnn.matmul(ttnn_to_device_358, ttnn_transpose_6, )
  ttnn_add_118 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_158, )
  ttnn_from_device_757 = ttnn.from_device(ttnn_add_118, )
  ttnn_to_layout_760 = ttnn.to_layout(ttnn_from_device_757, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_439 = ttnn.reshape(ttnn_to_layout_760, (1, 9, 1024), )
  ttnn_from_device_758 = ttnn.from_device(ttnn_reshape_439, )
  ttnn_to_layout_761 = ttnn.to_layout(ttnn_from_device_758, ttnn.TILE_LAYOUT, )
  ttnn_to_device_359 = ttnn.to_device(ttnn_to_layout_761, device = device)
  ttnn_prefix_clone_60 = clone_wrapper(ttnn_to_device_359, )
  ttnn_add_245 = ttnn.add(ttnn_layer_norm_38, ttnn_prefix_clone_60, )
  ttnn_layer_norm_39_ = ttnn.layer_norm(ttnn_add_245, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_39_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_245), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_39 = ttnn.from_torch(ttnn_layer_norm_39_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_39_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_39_), ttnn_layer_norm_39))
  ttnn_from_device_759 = ttnn.from_device(ttnn_layer_norm_39, )
  ttnn_to_layout_762 = ttnn.to_layout(ttnn_from_device_759, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_440 = ttnn.reshape(ttnn_to_layout_762, (9, 1024), )
  ttnn_from_device_760 = ttnn.from_device(ttnn_reshape_440, )
  ttnn_to_layout_763 = ttnn.to_layout(ttnn_from_device_760, ttnn.TILE_LAYOUT, )
  ttnn_to_device_360 = ttnn.to_device(ttnn_to_layout_763, device = device)
  ttnn_matmul_159 = ttnn.matmul(ttnn_to_device_360, ttnn_transpose_7, )
  ttnn_add_119 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_159, )
  ttnn_from_device_761 = ttnn.from_device(ttnn_add_119, )
  ttnn_to_layout_764 = ttnn.to_layout(ttnn_from_device_761, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_441 = ttnn.reshape(ttnn_to_layout_764, (1, 9, 4096), )
  ttnn_from_device_762 = ttnn.from_device(ttnn_reshape_441, )
  ttnn_to_layout_765 = ttnn.to_layout(ttnn_from_device_762, ttnn.TILE_LAYOUT, )
  ttnn_to_device_361 = ttnn.to_device(ttnn_to_layout_765, device = device)
  ttnn_multiply_97 = ttnn.multiply(ttnn_to_device_361, 0.5, )
  ttnn_pow_19 = ttnn.pow(ttnn_to_device_361, 3.0, )
  ttnn_multiply_98 = ttnn.multiply(ttnn_pow_19, 0.044715, )
  ttnn_add_246 = ttnn.add(ttnn_to_device_361, ttnn_multiply_98, )
  ttnn_multiply_99 = ttnn.multiply(ttnn_add_246, 0.7978845608028654, )
  ttnn_tanh_19_ = ttnn.tanh(ttnn_multiply_99, )
  ttnn_tanh_19_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_99), )
  ttnn_tanh_19 = ttnn.from_torch(ttnn_tanh_19_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_19_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_19_), ttnn_tanh_19))
  test_accuracy(tanh_19, ttnn_tanh_19)
  ttnn_add_247 = ttnn.add(ttnn_tanh_19, 1.0, )
  ttnn_multiply_100 = ttnn.multiply(ttnn_multiply_97, ttnn_add_247, )
  ttnn_from_device_763 = ttnn.from_device(ttnn_multiply_100, )
  ttnn_to_layout_766 = ttnn.to_layout(ttnn_from_device_763, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_442 = ttnn.reshape(ttnn_to_layout_766, (9, 4096), )
  ttnn_from_device_764 = ttnn.from_device(ttnn_reshape_442, )
  ttnn_to_layout_767 = ttnn.to_layout(ttnn_from_device_764, ttnn.TILE_LAYOUT, )
  ttnn_to_device_362 = ttnn.to_device(ttnn_to_layout_767, device = device)
  ttnn_matmul_160 = ttnn.matmul(ttnn_to_device_362, ttnn_transpose_8, )
  ttnn_add_120 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_160, )
  ttnn_from_device_765 = ttnn.from_device(ttnn_add_120, )
  ttnn_to_layout_768 = ttnn.to_layout(ttnn_from_device_765, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_443 = ttnn.reshape(ttnn_to_layout_768, (1, 9, 1024), )
  ttnn_from_device_766 = ttnn.from_device(ttnn_reshape_443, )
  ttnn_to_layout_769 = ttnn.to_layout(ttnn_from_device_766, ttnn.TILE_LAYOUT, )
  ttnn_to_device_363 = ttnn.to_device(ttnn_to_layout_769, device = device)
  ttnn_add_248 = ttnn.add(ttnn_to_device_363, ttnn_layer_norm_39, )
  ttnn_layer_norm_40_ = ttnn.layer_norm(ttnn_add_248, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_40_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_248), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_40 = ttnn.from_torch(ttnn_layer_norm_40_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_40_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_40_), ttnn_layer_norm_40))
  ttnn_from_device_767 = ttnn.from_device(ttnn_layer_norm_40, )
  ttnn_to_layout_770 = ttnn.to_layout(ttnn_from_device_767, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_444 = ttnn.reshape(ttnn_to_layout_770, (9, 1024), )
  ttnn_from_device_768 = ttnn.from_device(ttnn_reshape_444, )
  ttnn_to_layout_771 = ttnn.to_layout(ttnn_from_device_768, ttnn.TILE_LAYOUT, )
  ttnn_to_device_364 = ttnn.to_device(ttnn_to_layout_771, device = device)
  ttnn_matmul_161 = ttnn.matmul(ttnn_to_device_364, ttnn_transpose_1, )
  ttnn_add_121 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_161, )
  ttnn_from_device_769 = ttnn.from_device(ttnn_add_121, )
  ttnn_to_layout_772 = ttnn.to_layout(ttnn_from_device_769, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_445 = ttnn.reshape(ttnn_to_layout_772, (1, 9, 1024), )
  ttnn_to_layout_773 = ttnn.to_layout(ttnn_from_device_768, ttnn.TILE_LAYOUT, )
  ttnn_to_device_365 = ttnn.to_device(ttnn_to_layout_773, device = device)
  ttnn_matmul_162 = ttnn.matmul(ttnn_to_device_365, ttnn_transpose_2, )
  ttnn_add_122 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_162, )
  ttnn_from_device_771 = ttnn.from_device(ttnn_add_122, )
  ttnn_to_layout_774 = ttnn.to_layout(ttnn_from_device_771, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_447 = ttnn.reshape(ttnn_to_layout_774, (1, 9, 1024), )
  ttnn_to_layout_775 = ttnn.to_layout(ttnn_from_device_768, ttnn.TILE_LAYOUT, )
  ttnn_to_device_366 = ttnn.to_device(ttnn_to_layout_775, device = device)
  ttnn_matmul_163 = ttnn.matmul(ttnn_to_device_366, ttnn_transpose_3, )
  ttnn_add_123 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_163, )
  ttnn_from_device_773 = ttnn.from_device(ttnn_add_123, )
  ttnn_to_layout_776 = ttnn.to_layout(ttnn_from_device_773, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_449 = ttnn.reshape(ttnn_to_layout_776, (1, 9, 1024), )
  ttnn_from_device_774 = ttnn.from_device(ttnn_reshape_445, )
  ttnn_to_layout_777 = ttnn.to_layout(ttnn_from_device_774, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_450 = ttnn.reshape(ttnn_to_layout_777, (1, 9, 16, 64), )
  ttnn_from_device_775 = ttnn.from_device(ttnn_reshape_450, )
  ttnn_to_layout_778 = ttnn.to_layout(ttnn_from_device_775, ttnn.TILE_LAYOUT, )
  ttnn_to_device_367 = ttnn.to_device(ttnn_to_layout_778, device = device)
  ttnn_permute_60 = ttnn.permute(ttnn_to_device_367, (0, 2, 1, 3), )
  ttnn_from_device_776 = ttnn.from_device(ttnn_reshape_447, )
  ttnn_to_layout_779 = ttnn.to_layout(ttnn_from_device_776, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_451 = ttnn.reshape(ttnn_to_layout_779, (1, 9, 16, 64), )
  ttnn_from_device_777 = ttnn.from_device(ttnn_reshape_451, )
  ttnn_to_layout_780 = ttnn.to_layout(ttnn_from_device_777, ttnn.TILE_LAYOUT, )
  ttnn_to_device_368 = ttnn.to_device(ttnn_to_layout_780, device = device)
  ttnn_permute_61 = ttnn.permute(ttnn_to_device_368, (0, 2, 1, 3), )
  ttnn_from_device_778 = ttnn.from_device(ttnn_reshape_449, )
  ttnn_to_layout_781 = ttnn.to_layout(ttnn_from_device_778, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_452 = ttnn.reshape(ttnn_to_layout_781, (1, 9, 16, 64), )
  ttnn_from_device_779 = ttnn.from_device(ttnn_reshape_452, )
  ttnn_to_layout_782 = ttnn.to_layout(ttnn_from_device_779, ttnn.TILE_LAYOUT, )
  ttnn_to_device_369 = ttnn.to_device(ttnn_to_layout_782, device = device)
  ttnn_permute_62 = ttnn.permute(ttnn_to_device_369, (0, 2, 1, 3), )
  ttnn_transpose_164 = ttnn.transpose(ttnn_permute_61, 3, 2, )
  ttnn_from_device_780 = ttnn.from_device(ttnn_permute_60, )
  ttnn_to_layout_783 = ttnn.to_layout(ttnn_from_device_780, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_453 = ttnn.reshape(ttnn_to_layout_783, (16, 9, 64), )
  ttnn_from_device_781 = ttnn.from_device(ttnn_transpose_164, )
  ttnn_to_layout_784 = ttnn.to_layout(ttnn_from_device_781, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_454 = ttnn.reshape(ttnn_to_layout_784, (16, 64, 9), )
  ttnn_from_device_782 = ttnn.from_device(ttnn_reshape_453, )
  ttnn_to_layout_785 = ttnn.to_layout(ttnn_from_device_782, ttnn.TILE_LAYOUT, )
  ttnn_to_device_370 = ttnn.to_device(ttnn_to_layout_785, device = device)
  ttnn_from_device_783 = ttnn.from_device(ttnn_reshape_454, )
  ttnn_to_layout_786 = ttnn.to_layout(ttnn_from_device_783, ttnn.TILE_LAYOUT, )
  ttnn_to_device_371 = ttnn.to_device(ttnn_to_layout_786, device = device)
  ttnn_matmul_164 = ttnn.matmul(ttnn_to_device_370, ttnn_to_device_371, )
  ttnn_from_device_784 = ttnn.from_device(ttnn_matmul_164, )
  ttnn_to_layout_787 = ttnn.to_layout(ttnn_from_device_784, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_455 = ttnn.reshape(ttnn_to_layout_787, (1, 16, 9, 9), )
  ttnn_from_device_785 = ttnn.from_device(ttnn_reshape_455, )
  ttnn_to_layout_788 = ttnn.to_layout(ttnn_from_device_785, ttnn.TILE_LAYOUT, )
  ttnn_to_device_372 = ttnn.to_device(ttnn_to_layout_788, device = device)
  ttnn_multiply_101 = ttnn.multiply(ttnn_to_device_372, 0.125, )
  ttnn_add_249 = ttnn.add(ttnn_multiply_101, ttnn_multiply, )
  ttnn_softmax_20 = ttnn.softmax(ttnn_add_249, -1, numeric_stable = True)
  test_accuracy(_softmax_20, ttnn_softmax_20)
  ttnn_prefix_clone_61 = clone_wrapper(ttnn_softmax_20, )
  ttnn_from_device_786 = ttnn.from_device(ttnn_prefix_clone_61, )
  ttnn_to_layout_789 = ttnn.to_layout(ttnn_from_device_786, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_456 = ttnn.reshape(ttnn_to_layout_789, (16, 9, 9), )
  ttnn_from_device_787 = ttnn.from_device(ttnn_permute_62, )
  ttnn_to_layout_790 = ttnn.to_layout(ttnn_from_device_787, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_457 = ttnn.reshape(ttnn_to_layout_790, (16, 9, 64), )
  ttnn_from_device_788 = ttnn.from_device(ttnn_reshape_456, )
  ttnn_to_layout_791 = ttnn.to_layout(ttnn_from_device_788, ttnn.TILE_LAYOUT, )
  ttnn_to_device_373 = ttnn.to_device(ttnn_to_layout_791, device = device)
  ttnn_from_device_789 = ttnn.from_device(ttnn_reshape_457, )
  ttnn_to_layout_792 = ttnn.to_layout(ttnn_from_device_789, ttnn.TILE_LAYOUT, )
  ttnn_to_device_374 = ttnn.to_device(ttnn_to_layout_792, device = device)
  ttnn_matmul_165 = ttnn.matmul(ttnn_to_device_373, ttnn_to_device_374, )
  ttnn_from_device_790 = ttnn.from_device(ttnn_matmul_165, )
  ttnn_to_layout_793 = ttnn.to_layout(ttnn_from_device_790, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_458 = ttnn.reshape(ttnn_to_layout_793, (1, 16, 9, 64), )
  ttnn_from_device_791 = ttnn.from_device(ttnn_reshape_458, )
  ttnn_to_layout_794 = ttnn.to_layout(ttnn_from_device_791, ttnn.TILE_LAYOUT, )
  ttnn_to_device_375 = ttnn.to_device(ttnn_to_layout_794, device = device)
  ttnn_transpose_165 = ttnn.transpose(ttnn_to_device_375, 2, 1, )
  ttnn_prefix_clone_62 = clone_wrapper(ttnn_transpose_165, )
  ttnn_from_device_792 = ttnn.from_device(ttnn_prefix_clone_62, )
  ttnn_to_layout_795 = ttnn.to_layout(ttnn_from_device_792, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_459 = ttnn.reshape(ttnn_to_layout_795, (1, 9, 1024), )
  ttnn_from_device_793 = ttnn.from_device(ttnn_reshape_459, )
  ttnn_to_layout_796 = ttnn.to_layout(ttnn_from_device_793, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_460 = ttnn.reshape(ttnn_to_layout_796, (9, 1024), )
  ttnn_from_device_794 = ttnn.from_device(ttnn_reshape_460, )
  ttnn_to_layout_797 = ttnn.to_layout(ttnn_from_device_794, ttnn.TILE_LAYOUT, )
  ttnn_to_device_376 = ttnn.to_device(ttnn_to_layout_797, device = device)
  ttnn_matmul_166 = ttnn.matmul(ttnn_to_device_376, ttnn_transpose_6, )
  ttnn_add_124 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_166, )
  ttnn_from_device_795 = ttnn.from_device(ttnn_add_124, )
  ttnn_to_layout_798 = ttnn.to_layout(ttnn_from_device_795, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_461 = ttnn.reshape(ttnn_to_layout_798, (1, 9, 1024), )
  ttnn_from_device_796 = ttnn.from_device(ttnn_reshape_461, )
  ttnn_to_layout_799 = ttnn.to_layout(ttnn_from_device_796, ttnn.TILE_LAYOUT, )
  ttnn_to_device_377 = ttnn.to_device(ttnn_to_layout_799, device = device)
  ttnn_prefix_clone_63 = clone_wrapper(ttnn_to_device_377, )
  ttnn_add_250 = ttnn.add(ttnn_layer_norm_40, ttnn_prefix_clone_63, )
  ttnn_layer_norm_41_ = ttnn.layer_norm(ttnn_add_250, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_41_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_250), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_41 = ttnn.from_torch(ttnn_layer_norm_41_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_41_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_41_), ttnn_layer_norm_41))
  ttnn_from_device_797 = ttnn.from_device(ttnn_layer_norm_41, )
  ttnn_to_layout_800 = ttnn.to_layout(ttnn_from_device_797, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_462 = ttnn.reshape(ttnn_to_layout_800, (9, 1024), )
  ttnn_from_device_798 = ttnn.from_device(ttnn_reshape_462, )
  ttnn_to_layout_801 = ttnn.to_layout(ttnn_from_device_798, ttnn.TILE_LAYOUT, )
  ttnn_to_device_378 = ttnn.to_device(ttnn_to_layout_801, device = device)
  ttnn_matmul_167 = ttnn.matmul(ttnn_to_device_378, ttnn_transpose_7, )
  ttnn_add_125 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_167, )
  ttnn_from_device_799 = ttnn.from_device(ttnn_add_125, )
  ttnn_to_layout_802 = ttnn.to_layout(ttnn_from_device_799, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_463 = ttnn.reshape(ttnn_to_layout_802, (1, 9, 4096), )
  ttnn_from_device_800 = ttnn.from_device(ttnn_reshape_463, )
  ttnn_to_layout_803 = ttnn.to_layout(ttnn_from_device_800, ttnn.TILE_LAYOUT, )
  ttnn_to_device_379 = ttnn.to_device(ttnn_to_layout_803, device = device)
  ttnn_multiply_102 = ttnn.multiply(ttnn_to_device_379, 0.5, )
  ttnn_pow_20 = ttnn.pow(ttnn_to_device_379, 3.0, )
  ttnn_multiply_103 = ttnn.multiply(ttnn_pow_20, 0.044715, )
  ttnn_add_251 = ttnn.add(ttnn_to_device_379, ttnn_multiply_103, )
  ttnn_multiply_104 = ttnn.multiply(ttnn_add_251, 0.7978845608028654, )
  ttnn_tanh_20_ = ttnn.tanh(ttnn_multiply_104, )
  ttnn_tanh_20_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_104), )
  ttnn_tanh_20 = ttnn.from_torch(ttnn_tanh_20_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_20_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_20_), ttnn_tanh_20))
  test_accuracy(tanh_20, ttnn_tanh_20)
  ttnn_add_252 = ttnn.add(ttnn_tanh_20, 1.0, )
  ttnn_multiply_105 = ttnn.multiply(ttnn_multiply_102, ttnn_add_252, )
  ttnn_from_device_801 = ttnn.from_device(ttnn_multiply_105, )
  ttnn_to_layout_804 = ttnn.to_layout(ttnn_from_device_801, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_464 = ttnn.reshape(ttnn_to_layout_804, (9, 4096), )
  ttnn_from_device_802 = ttnn.from_device(ttnn_reshape_464, )
  ttnn_to_layout_805 = ttnn.to_layout(ttnn_from_device_802, ttnn.TILE_LAYOUT, )
  ttnn_to_device_380 = ttnn.to_device(ttnn_to_layout_805, device = device)
  ttnn_matmul_168 = ttnn.matmul(ttnn_to_device_380, ttnn_transpose_8, )
  ttnn_add_126 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_168, )
  ttnn_from_device_803 = ttnn.from_device(ttnn_add_126, )
  ttnn_to_layout_806 = ttnn.to_layout(ttnn_from_device_803, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_465 = ttnn.reshape(ttnn_to_layout_806, (1, 9, 1024), )
  ttnn_from_device_804 = ttnn.from_device(ttnn_reshape_465, )
  ttnn_to_layout_807 = ttnn.to_layout(ttnn_from_device_804, ttnn.TILE_LAYOUT, )
  ttnn_to_device_381 = ttnn.to_device(ttnn_to_layout_807, device = device)
  ttnn_add_253 = ttnn.add(ttnn_to_device_381, ttnn_layer_norm_41, )
  ttnn_layer_norm_42_ = ttnn.layer_norm(ttnn_add_253, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_42_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_253), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_42 = ttnn.from_torch(ttnn_layer_norm_42_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_42_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_42_), ttnn_layer_norm_42))
  ttnn_from_device_805 = ttnn.from_device(ttnn_layer_norm_42, )
  ttnn_to_layout_808 = ttnn.to_layout(ttnn_from_device_805, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_466 = ttnn.reshape(ttnn_to_layout_808, (9, 1024), )
  ttnn_from_device_806 = ttnn.from_device(ttnn_reshape_466, )
  ttnn_to_layout_809 = ttnn.to_layout(ttnn_from_device_806, ttnn.TILE_LAYOUT, )
  ttnn_to_device_382 = ttnn.to_device(ttnn_to_layout_809, device = device)
  ttnn_matmul_169 = ttnn.matmul(ttnn_to_device_382, ttnn_transpose_1, )
  ttnn_add_127 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_169, )
  ttnn_from_device_807 = ttnn.from_device(ttnn_add_127, )
  ttnn_to_layout_810 = ttnn.to_layout(ttnn_from_device_807, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_467 = ttnn.reshape(ttnn_to_layout_810, (1, 9, 1024), )
  ttnn_to_layout_811 = ttnn.to_layout(ttnn_from_device_806, ttnn.TILE_LAYOUT, )
  ttnn_to_device_383 = ttnn.to_device(ttnn_to_layout_811, device = device)
  ttnn_matmul_170 = ttnn.matmul(ttnn_to_device_383, ttnn_transpose_2, )
  ttnn_add_128 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_170, )
  ttnn_from_device_809 = ttnn.from_device(ttnn_add_128, )
  ttnn_to_layout_812 = ttnn.to_layout(ttnn_from_device_809, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_469 = ttnn.reshape(ttnn_to_layout_812, (1, 9, 1024), )
  ttnn_to_layout_813 = ttnn.to_layout(ttnn_from_device_806, ttnn.TILE_LAYOUT, )
  ttnn_to_device_384 = ttnn.to_device(ttnn_to_layout_813, device = device)
  ttnn_matmul_171 = ttnn.matmul(ttnn_to_device_384, ttnn_transpose_3, )
  ttnn_add_129 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_171, )
  ttnn_from_device_811 = ttnn.from_device(ttnn_add_129, )
  ttnn_to_layout_814 = ttnn.to_layout(ttnn_from_device_811, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_471 = ttnn.reshape(ttnn_to_layout_814, (1, 9, 1024), )
  ttnn_from_device_812 = ttnn.from_device(ttnn_reshape_467, )
  ttnn_to_layout_815 = ttnn.to_layout(ttnn_from_device_812, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_472 = ttnn.reshape(ttnn_to_layout_815, (1, 9, 16, 64), )
  ttnn_from_device_813 = ttnn.from_device(ttnn_reshape_472, )
  ttnn_to_layout_816 = ttnn.to_layout(ttnn_from_device_813, ttnn.TILE_LAYOUT, )
  ttnn_to_device_385 = ttnn.to_device(ttnn_to_layout_816, device = device)
  ttnn_permute_63 = ttnn.permute(ttnn_to_device_385, (0, 2, 1, 3), )
  ttnn_from_device_814 = ttnn.from_device(ttnn_reshape_469, )
  ttnn_to_layout_817 = ttnn.to_layout(ttnn_from_device_814, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_473 = ttnn.reshape(ttnn_to_layout_817, (1, 9, 16, 64), )
  ttnn_from_device_815 = ttnn.from_device(ttnn_reshape_473, )
  ttnn_to_layout_818 = ttnn.to_layout(ttnn_from_device_815, ttnn.TILE_LAYOUT, )
  ttnn_to_device_386 = ttnn.to_device(ttnn_to_layout_818, device = device)
  ttnn_permute_64 = ttnn.permute(ttnn_to_device_386, (0, 2, 1, 3), )
  ttnn_from_device_816 = ttnn.from_device(ttnn_reshape_471, )
  ttnn_to_layout_819 = ttnn.to_layout(ttnn_from_device_816, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_474 = ttnn.reshape(ttnn_to_layout_819, (1, 9, 16, 64), )
  ttnn_from_device_817 = ttnn.from_device(ttnn_reshape_474, )
  ttnn_to_layout_820 = ttnn.to_layout(ttnn_from_device_817, ttnn.TILE_LAYOUT, )
  ttnn_to_device_387 = ttnn.to_device(ttnn_to_layout_820, device = device)
  ttnn_permute_65 = ttnn.permute(ttnn_to_device_387, (0, 2, 1, 3), )
  ttnn_transpose_172 = ttnn.transpose(ttnn_permute_64, 3, 2, )
  ttnn_from_device_818 = ttnn.from_device(ttnn_permute_63, )
  ttnn_to_layout_821 = ttnn.to_layout(ttnn_from_device_818, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_475 = ttnn.reshape(ttnn_to_layout_821, (16, 9, 64), )
  ttnn_from_device_819 = ttnn.from_device(ttnn_transpose_172, )
  ttnn_to_layout_822 = ttnn.to_layout(ttnn_from_device_819, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_476 = ttnn.reshape(ttnn_to_layout_822, (16, 64, 9), )
  ttnn_from_device_820 = ttnn.from_device(ttnn_reshape_475, )
  ttnn_to_layout_823 = ttnn.to_layout(ttnn_from_device_820, ttnn.TILE_LAYOUT, )
  ttnn_to_device_388 = ttnn.to_device(ttnn_to_layout_823, device = device)
  ttnn_from_device_821 = ttnn.from_device(ttnn_reshape_476, )
  ttnn_to_layout_824 = ttnn.to_layout(ttnn_from_device_821, ttnn.TILE_LAYOUT, )
  ttnn_to_device_389 = ttnn.to_device(ttnn_to_layout_824, device = device)
  ttnn_matmul_172 = ttnn.matmul(ttnn_to_device_388, ttnn_to_device_389, )
  ttnn_from_device_822 = ttnn.from_device(ttnn_matmul_172, )
  ttnn_to_layout_825 = ttnn.to_layout(ttnn_from_device_822, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_477 = ttnn.reshape(ttnn_to_layout_825, (1, 16, 9, 9), )
  ttnn_from_device_823 = ttnn.from_device(ttnn_reshape_477, )
  ttnn_to_layout_826 = ttnn.to_layout(ttnn_from_device_823, ttnn.TILE_LAYOUT, )
  ttnn_to_device_390 = ttnn.to_device(ttnn_to_layout_826, device = device)
  ttnn_multiply_106 = ttnn.multiply(ttnn_to_device_390, 0.125, )
  ttnn_add_254 = ttnn.add(ttnn_multiply_106, ttnn_multiply, )
  ttnn_softmax_21 = ttnn.softmax(ttnn_add_254, -1, numeric_stable = True)
  test_accuracy(_softmax_21, ttnn_softmax_21)
  ttnn_prefix_clone_64 = clone_wrapper(ttnn_softmax_21, )
  ttnn_from_device_824 = ttnn.from_device(ttnn_prefix_clone_64, )
  ttnn_to_layout_827 = ttnn.to_layout(ttnn_from_device_824, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_478 = ttnn.reshape(ttnn_to_layout_827, (16, 9, 9), )
  ttnn_from_device_825 = ttnn.from_device(ttnn_permute_65, )
  ttnn_to_layout_828 = ttnn.to_layout(ttnn_from_device_825, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_479 = ttnn.reshape(ttnn_to_layout_828, (16, 9, 64), )
  ttnn_from_device_826 = ttnn.from_device(ttnn_reshape_478, )
  ttnn_to_layout_829 = ttnn.to_layout(ttnn_from_device_826, ttnn.TILE_LAYOUT, )
  ttnn_to_device_391 = ttnn.to_device(ttnn_to_layout_829, device = device)
  ttnn_from_device_827 = ttnn.from_device(ttnn_reshape_479, )
  ttnn_to_layout_830 = ttnn.to_layout(ttnn_from_device_827, ttnn.TILE_LAYOUT, )
  ttnn_to_device_392 = ttnn.to_device(ttnn_to_layout_830, device = device)
  ttnn_matmul_173 = ttnn.matmul(ttnn_to_device_391, ttnn_to_device_392, )
  ttnn_from_device_828 = ttnn.from_device(ttnn_matmul_173, )
  ttnn_to_layout_831 = ttnn.to_layout(ttnn_from_device_828, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_480 = ttnn.reshape(ttnn_to_layout_831, (1, 16, 9, 64), )
  ttnn_from_device_829 = ttnn.from_device(ttnn_reshape_480, )
  ttnn_to_layout_832 = ttnn.to_layout(ttnn_from_device_829, ttnn.TILE_LAYOUT, )
  ttnn_to_device_393 = ttnn.to_device(ttnn_to_layout_832, device = device)
  ttnn_transpose_173 = ttnn.transpose(ttnn_to_device_393, 2, 1, )
  ttnn_prefix_clone_65 = clone_wrapper(ttnn_transpose_173, )
  ttnn_from_device_830 = ttnn.from_device(ttnn_prefix_clone_65, )
  ttnn_to_layout_833 = ttnn.to_layout(ttnn_from_device_830, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_481 = ttnn.reshape(ttnn_to_layout_833, (1, 9, 1024), )
  ttnn_from_device_831 = ttnn.from_device(ttnn_reshape_481, )
  ttnn_to_layout_834 = ttnn.to_layout(ttnn_from_device_831, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_482 = ttnn.reshape(ttnn_to_layout_834, (9, 1024), )
  ttnn_from_device_832 = ttnn.from_device(ttnn_reshape_482, )
  ttnn_to_layout_835 = ttnn.to_layout(ttnn_from_device_832, ttnn.TILE_LAYOUT, )
  ttnn_to_device_394 = ttnn.to_device(ttnn_to_layout_835, device = device)
  ttnn_matmul_174 = ttnn.matmul(ttnn_to_device_394, ttnn_transpose_6, )
  ttnn_add_130 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_174, )
  ttnn_from_device_833 = ttnn.from_device(ttnn_add_130, )
  ttnn_to_layout_836 = ttnn.to_layout(ttnn_from_device_833, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_483 = ttnn.reshape(ttnn_to_layout_836, (1, 9, 1024), )
  ttnn_from_device_834 = ttnn.from_device(ttnn_reshape_483, )
  ttnn_to_layout_837 = ttnn.to_layout(ttnn_from_device_834, ttnn.TILE_LAYOUT, )
  ttnn_to_device_395 = ttnn.to_device(ttnn_to_layout_837, device = device)
  ttnn_prefix_clone_66 = clone_wrapper(ttnn_to_device_395, )
  ttnn_add_255 = ttnn.add(ttnn_layer_norm_42, ttnn_prefix_clone_66, )
  ttnn_layer_norm_43_ = ttnn.layer_norm(ttnn_add_255, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_43_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_255), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_43 = ttnn.from_torch(ttnn_layer_norm_43_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_43_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_43_), ttnn_layer_norm_43))
  ttnn_from_device_835 = ttnn.from_device(ttnn_layer_norm_43, )
  ttnn_to_layout_838 = ttnn.to_layout(ttnn_from_device_835, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_484 = ttnn.reshape(ttnn_to_layout_838, (9, 1024), )
  ttnn_from_device_836 = ttnn.from_device(ttnn_reshape_484, )
  ttnn_to_layout_839 = ttnn.to_layout(ttnn_from_device_836, ttnn.TILE_LAYOUT, )
  ttnn_to_device_396 = ttnn.to_device(ttnn_to_layout_839, device = device)
  ttnn_matmul_175 = ttnn.matmul(ttnn_to_device_396, ttnn_transpose_7, )
  ttnn_add_131 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_175, )
  ttnn_from_device_837 = ttnn.from_device(ttnn_add_131, )
  ttnn_to_layout_840 = ttnn.to_layout(ttnn_from_device_837, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_485 = ttnn.reshape(ttnn_to_layout_840, (1, 9, 4096), )
  ttnn_from_device_838 = ttnn.from_device(ttnn_reshape_485, )
  ttnn_to_layout_841 = ttnn.to_layout(ttnn_from_device_838, ttnn.TILE_LAYOUT, )
  ttnn_to_device_397 = ttnn.to_device(ttnn_to_layout_841, device = device)
  ttnn_multiply_107 = ttnn.multiply(ttnn_to_device_397, 0.5, )
  ttnn_pow_21 = ttnn.pow(ttnn_to_device_397, 3.0, )
  ttnn_multiply_108 = ttnn.multiply(ttnn_pow_21, 0.044715, )
  ttnn_add_256 = ttnn.add(ttnn_to_device_397, ttnn_multiply_108, )
  ttnn_multiply_109 = ttnn.multiply(ttnn_add_256, 0.7978845608028654, )
  ttnn_tanh_21_ = ttnn.tanh(ttnn_multiply_109, )
  ttnn_tanh_21_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_109), )
  ttnn_tanh_21 = ttnn.from_torch(ttnn_tanh_21_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_21_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_21_), ttnn_tanh_21))
  test_accuracy(tanh_21, ttnn_tanh_21)
  ttnn_add_257 = ttnn.add(ttnn_tanh_21, 1.0, )
  ttnn_multiply_110 = ttnn.multiply(ttnn_multiply_107, ttnn_add_257, )
  ttnn_from_device_839 = ttnn.from_device(ttnn_multiply_110, )
  ttnn_to_layout_842 = ttnn.to_layout(ttnn_from_device_839, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_486 = ttnn.reshape(ttnn_to_layout_842, (9, 4096), )
  ttnn_from_device_840 = ttnn.from_device(ttnn_reshape_486, )
  ttnn_to_layout_843 = ttnn.to_layout(ttnn_from_device_840, ttnn.TILE_LAYOUT, )
  ttnn_to_device_398 = ttnn.to_device(ttnn_to_layout_843, device = device)
  ttnn_matmul_176 = ttnn.matmul(ttnn_to_device_398, ttnn_transpose_8, )
  ttnn_add_132 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_176, )
  ttnn_from_device_841 = ttnn.from_device(ttnn_add_132, )
  ttnn_to_layout_844 = ttnn.to_layout(ttnn_from_device_841, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_487 = ttnn.reshape(ttnn_to_layout_844, (1, 9, 1024), )
  ttnn_from_device_842 = ttnn.from_device(ttnn_reshape_487, )
  ttnn_to_layout_845 = ttnn.to_layout(ttnn_from_device_842, ttnn.TILE_LAYOUT, )
  ttnn_to_device_399 = ttnn.to_device(ttnn_to_layout_845, device = device)
  ttnn_add_258 = ttnn.add(ttnn_to_device_399, ttnn_layer_norm_43, )
  ttnn_layer_norm_44_ = ttnn.layer_norm(ttnn_add_258, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_44_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_258), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_44 = ttnn.from_torch(ttnn_layer_norm_44_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_44_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_44_), ttnn_layer_norm_44))
  ttnn_from_device_843 = ttnn.from_device(ttnn_layer_norm_44, )
  ttnn_to_layout_846 = ttnn.to_layout(ttnn_from_device_843, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_488 = ttnn.reshape(ttnn_to_layout_846, (9, 1024), )
  ttnn_from_device_844 = ttnn.from_device(ttnn_reshape_488, )
  ttnn_to_layout_847 = ttnn.to_layout(ttnn_from_device_844, ttnn.TILE_LAYOUT, )
  ttnn_to_device_400 = ttnn.to_device(ttnn_to_layout_847, device = device)
  ttnn_matmul_177 = ttnn.matmul(ttnn_to_device_400, ttnn_transpose_1, )
  ttnn_add_133 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_177, )
  ttnn_from_device_845 = ttnn.from_device(ttnn_add_133, )
  ttnn_to_layout_848 = ttnn.to_layout(ttnn_from_device_845, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_489 = ttnn.reshape(ttnn_to_layout_848, (1, 9, 1024), )
  ttnn_to_layout_849 = ttnn.to_layout(ttnn_from_device_844, ttnn.TILE_LAYOUT, )
  ttnn_to_device_401 = ttnn.to_device(ttnn_to_layout_849, device = device)
  ttnn_matmul_178 = ttnn.matmul(ttnn_to_device_401, ttnn_transpose_2, )
  ttnn_add_134 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_178, )
  ttnn_from_device_847 = ttnn.from_device(ttnn_add_134, )
  ttnn_to_layout_850 = ttnn.to_layout(ttnn_from_device_847, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_491 = ttnn.reshape(ttnn_to_layout_850, (1, 9, 1024), )
  ttnn_to_layout_851 = ttnn.to_layout(ttnn_from_device_844, ttnn.TILE_LAYOUT, )
  ttnn_to_device_402 = ttnn.to_device(ttnn_to_layout_851, device = device)
  ttnn_matmul_179 = ttnn.matmul(ttnn_to_device_402, ttnn_transpose_3, )
  ttnn_add_135 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_179, )
  ttnn_from_device_849 = ttnn.from_device(ttnn_add_135, )
  ttnn_to_layout_852 = ttnn.to_layout(ttnn_from_device_849, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_493 = ttnn.reshape(ttnn_to_layout_852, (1, 9, 1024), )
  ttnn_from_device_850 = ttnn.from_device(ttnn_reshape_489, )
  ttnn_to_layout_853 = ttnn.to_layout(ttnn_from_device_850, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_494 = ttnn.reshape(ttnn_to_layout_853, (1, 9, 16, 64), )
  ttnn_from_device_851 = ttnn.from_device(ttnn_reshape_494, )
  ttnn_to_layout_854 = ttnn.to_layout(ttnn_from_device_851, ttnn.TILE_LAYOUT, )
  ttnn_to_device_403 = ttnn.to_device(ttnn_to_layout_854, device = device)
  ttnn_permute_66 = ttnn.permute(ttnn_to_device_403, (0, 2, 1, 3), )
  ttnn_from_device_852 = ttnn.from_device(ttnn_reshape_491, )
  ttnn_to_layout_855 = ttnn.to_layout(ttnn_from_device_852, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_495 = ttnn.reshape(ttnn_to_layout_855, (1, 9, 16, 64), )
  ttnn_from_device_853 = ttnn.from_device(ttnn_reshape_495, )
  ttnn_to_layout_856 = ttnn.to_layout(ttnn_from_device_853, ttnn.TILE_LAYOUT, )
  ttnn_to_device_404 = ttnn.to_device(ttnn_to_layout_856, device = device)
  ttnn_permute_67 = ttnn.permute(ttnn_to_device_404, (0, 2, 1, 3), )
  ttnn_from_device_854 = ttnn.from_device(ttnn_reshape_493, )
  ttnn_to_layout_857 = ttnn.to_layout(ttnn_from_device_854, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_496 = ttnn.reshape(ttnn_to_layout_857, (1, 9, 16, 64), )
  ttnn_from_device_855 = ttnn.from_device(ttnn_reshape_496, )
  ttnn_to_layout_858 = ttnn.to_layout(ttnn_from_device_855, ttnn.TILE_LAYOUT, )
  ttnn_to_device_405 = ttnn.to_device(ttnn_to_layout_858, device = device)
  ttnn_permute_68 = ttnn.permute(ttnn_to_device_405, (0, 2, 1, 3), )
  ttnn_transpose_180 = ttnn.transpose(ttnn_permute_67, 3, 2, )
  ttnn_from_device_856 = ttnn.from_device(ttnn_permute_66, )
  ttnn_to_layout_859 = ttnn.to_layout(ttnn_from_device_856, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_497 = ttnn.reshape(ttnn_to_layout_859, (16, 9, 64), )
  ttnn_from_device_857 = ttnn.from_device(ttnn_transpose_180, )
  ttnn_to_layout_860 = ttnn.to_layout(ttnn_from_device_857, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_498 = ttnn.reshape(ttnn_to_layout_860, (16, 64, 9), )
  ttnn_from_device_858 = ttnn.from_device(ttnn_reshape_497, )
  ttnn_to_layout_861 = ttnn.to_layout(ttnn_from_device_858, ttnn.TILE_LAYOUT, )
  ttnn_to_device_406 = ttnn.to_device(ttnn_to_layout_861, device = device)
  ttnn_from_device_859 = ttnn.from_device(ttnn_reshape_498, )
  ttnn_to_layout_862 = ttnn.to_layout(ttnn_from_device_859, ttnn.TILE_LAYOUT, )
  ttnn_to_device_407 = ttnn.to_device(ttnn_to_layout_862, device = device)
  ttnn_matmul_180 = ttnn.matmul(ttnn_to_device_406, ttnn_to_device_407, )
  ttnn_from_device_860 = ttnn.from_device(ttnn_matmul_180, )
  ttnn_to_layout_863 = ttnn.to_layout(ttnn_from_device_860, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_499 = ttnn.reshape(ttnn_to_layout_863, (1, 16, 9, 9), )
  ttnn_from_device_861 = ttnn.from_device(ttnn_reshape_499, )
  ttnn_to_layout_864 = ttnn.to_layout(ttnn_from_device_861, ttnn.TILE_LAYOUT, )
  ttnn_to_device_408 = ttnn.to_device(ttnn_to_layout_864, device = device)
  ttnn_multiply_111 = ttnn.multiply(ttnn_to_device_408, 0.125, )
  ttnn_add_259 = ttnn.add(ttnn_multiply_111, ttnn_multiply, )
  ttnn_softmax_22 = ttnn.softmax(ttnn_add_259, -1, numeric_stable = True)
  test_accuracy(_softmax_22, ttnn_softmax_22)
  ttnn_prefix_clone_67 = clone_wrapper(ttnn_softmax_22, )
  ttnn_from_device_862 = ttnn.from_device(ttnn_prefix_clone_67, )
  ttnn_to_layout_865 = ttnn.to_layout(ttnn_from_device_862, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_500 = ttnn.reshape(ttnn_to_layout_865, (16, 9, 9), )
  ttnn_from_device_863 = ttnn.from_device(ttnn_permute_68, )
  ttnn_to_layout_866 = ttnn.to_layout(ttnn_from_device_863, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_501 = ttnn.reshape(ttnn_to_layout_866, (16, 9, 64), )
  ttnn_from_device_864 = ttnn.from_device(ttnn_reshape_500, )
  ttnn_to_layout_867 = ttnn.to_layout(ttnn_from_device_864, ttnn.TILE_LAYOUT, )
  ttnn_to_device_409 = ttnn.to_device(ttnn_to_layout_867, device = device)
  ttnn_from_device_865 = ttnn.from_device(ttnn_reshape_501, )
  ttnn_to_layout_868 = ttnn.to_layout(ttnn_from_device_865, ttnn.TILE_LAYOUT, )
  ttnn_to_device_410 = ttnn.to_device(ttnn_to_layout_868, device = device)
  ttnn_matmul_181 = ttnn.matmul(ttnn_to_device_409, ttnn_to_device_410, )
  ttnn_from_device_866 = ttnn.from_device(ttnn_matmul_181, )
  ttnn_to_layout_869 = ttnn.to_layout(ttnn_from_device_866, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_502 = ttnn.reshape(ttnn_to_layout_869, (1, 16, 9, 64), )
  ttnn_from_device_867 = ttnn.from_device(ttnn_reshape_502, )
  ttnn_to_layout_870 = ttnn.to_layout(ttnn_from_device_867, ttnn.TILE_LAYOUT, )
  ttnn_to_device_411 = ttnn.to_device(ttnn_to_layout_870, device = device)
  ttnn_transpose_181 = ttnn.transpose(ttnn_to_device_411, 2, 1, )
  ttnn_prefix_clone_68 = clone_wrapper(ttnn_transpose_181, )
  ttnn_from_device_868 = ttnn.from_device(ttnn_prefix_clone_68, )
  ttnn_to_layout_871 = ttnn.to_layout(ttnn_from_device_868, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_503 = ttnn.reshape(ttnn_to_layout_871, (1, 9, 1024), )
  ttnn_from_device_869 = ttnn.from_device(ttnn_reshape_503, )
  ttnn_to_layout_872 = ttnn.to_layout(ttnn_from_device_869, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_504 = ttnn.reshape(ttnn_to_layout_872, (9, 1024), )
  ttnn_from_device_870 = ttnn.from_device(ttnn_reshape_504, )
  ttnn_to_layout_873 = ttnn.to_layout(ttnn_from_device_870, ttnn.TILE_LAYOUT, )
  ttnn_to_device_412 = ttnn.to_device(ttnn_to_layout_873, device = device)
  ttnn_matmul_182 = ttnn.matmul(ttnn_to_device_412, ttnn_transpose_6, )
  ttnn_add_136 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_182, )
  ttnn_from_device_871 = ttnn.from_device(ttnn_add_136, )
  ttnn_to_layout_874 = ttnn.to_layout(ttnn_from_device_871, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_505 = ttnn.reshape(ttnn_to_layout_874, (1, 9, 1024), )
  ttnn_from_device_872 = ttnn.from_device(ttnn_reshape_505, )
  ttnn_to_layout_875 = ttnn.to_layout(ttnn_from_device_872, ttnn.TILE_LAYOUT, )
  ttnn_to_device_413 = ttnn.to_device(ttnn_to_layout_875, device = device)
  ttnn_prefix_clone_69 = clone_wrapper(ttnn_to_device_413, )
  ttnn_add_260 = ttnn.add(ttnn_layer_norm_44, ttnn_prefix_clone_69, )
  ttnn_layer_norm_45_ = ttnn.layer_norm(ttnn_add_260, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_45_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_260), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_45 = ttnn.from_torch(ttnn_layer_norm_45_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_45_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_45_), ttnn_layer_norm_45))
  ttnn_from_device_873 = ttnn.from_device(ttnn_layer_norm_45, )
  ttnn_to_layout_876 = ttnn.to_layout(ttnn_from_device_873, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_506 = ttnn.reshape(ttnn_to_layout_876, (9, 1024), )
  ttnn_from_device_874 = ttnn.from_device(ttnn_reshape_506, )
  ttnn_to_layout_877 = ttnn.to_layout(ttnn_from_device_874, ttnn.TILE_LAYOUT, )
  ttnn_to_device_414 = ttnn.to_device(ttnn_to_layout_877, device = device)
  ttnn_matmul_183 = ttnn.matmul(ttnn_to_device_414, ttnn_transpose_7, )
  ttnn_add_137 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_183, )
  ttnn_from_device_875 = ttnn.from_device(ttnn_add_137, )
  ttnn_to_layout_878 = ttnn.to_layout(ttnn_from_device_875, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_507 = ttnn.reshape(ttnn_to_layout_878, (1, 9, 4096), )
  ttnn_from_device_876 = ttnn.from_device(ttnn_reshape_507, )
  ttnn_to_layout_879 = ttnn.to_layout(ttnn_from_device_876, ttnn.TILE_LAYOUT, )
  ttnn_to_device_415 = ttnn.to_device(ttnn_to_layout_879, device = device)
  ttnn_multiply_112 = ttnn.multiply(ttnn_to_device_415, 0.5, )
  ttnn_pow_22 = ttnn.pow(ttnn_to_device_415, 3.0, )
  ttnn_multiply_113 = ttnn.multiply(ttnn_pow_22, 0.044715, )
  ttnn_add_261 = ttnn.add(ttnn_to_device_415, ttnn_multiply_113, )
  ttnn_multiply_114 = ttnn.multiply(ttnn_add_261, 0.7978845608028654, )
  ttnn_tanh_22_ = ttnn.tanh(ttnn_multiply_114, )
  ttnn_tanh_22_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_114), )
  ttnn_tanh_22 = ttnn.from_torch(ttnn_tanh_22_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_22_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_22_), ttnn_tanh_22))
  test_accuracy(tanh_22, ttnn_tanh_22)
  ttnn_add_262 = ttnn.add(ttnn_tanh_22, 1.0, )
  ttnn_multiply_115 = ttnn.multiply(ttnn_multiply_112, ttnn_add_262, )
  ttnn_from_device_877 = ttnn.from_device(ttnn_multiply_115, )
  ttnn_to_layout_880 = ttnn.to_layout(ttnn_from_device_877, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_508 = ttnn.reshape(ttnn_to_layout_880, (9, 4096), )
  ttnn_from_device_878 = ttnn.from_device(ttnn_reshape_508, )
  ttnn_to_layout_881 = ttnn.to_layout(ttnn_from_device_878, ttnn.TILE_LAYOUT, )
  ttnn_to_device_416 = ttnn.to_device(ttnn_to_layout_881, device = device)
  ttnn_matmul_184 = ttnn.matmul(ttnn_to_device_416, ttnn_transpose_8, )
  ttnn_add_138 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_184, )
  ttnn_from_device_879 = ttnn.from_device(ttnn_add_138, )
  ttnn_to_layout_882 = ttnn.to_layout(ttnn_from_device_879, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_509 = ttnn.reshape(ttnn_to_layout_882, (1, 9, 1024), )
  ttnn_from_device_880 = ttnn.from_device(ttnn_reshape_509, )
  ttnn_to_layout_883 = ttnn.to_layout(ttnn_from_device_880, ttnn.TILE_LAYOUT, )
  ttnn_to_device_417 = ttnn.to_device(ttnn_to_layout_883, device = device)
  ttnn_add_263 = ttnn.add(ttnn_to_device_417, ttnn_layer_norm_45, )
  ttnn_layer_norm_46_ = ttnn.layer_norm(ttnn_add_263, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_46_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_263), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_46 = ttnn.from_torch(ttnn_layer_norm_46_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_46_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_46_), ttnn_layer_norm_46))
  ttnn_from_device_881 = ttnn.from_device(ttnn_layer_norm_46, )
  ttnn_to_layout_884 = ttnn.to_layout(ttnn_from_device_881, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_510 = ttnn.reshape(ttnn_to_layout_884, (9, 1024), )
  ttnn_from_device_882 = ttnn.from_device(ttnn_reshape_510, )
  ttnn_to_layout_885 = ttnn.to_layout(ttnn_from_device_882, ttnn.TILE_LAYOUT, )
  ttnn_to_device_418 = ttnn.to_device(ttnn_to_layout_885, device = device)
  ttnn_matmul_185 = ttnn.matmul(ttnn_to_device_418, ttnn_transpose_1, )
  ttnn_add_139 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_185, )
  ttnn_from_device_883 = ttnn.from_device(ttnn_add_139, )
  ttnn_to_layout_886 = ttnn.to_layout(ttnn_from_device_883, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_511 = ttnn.reshape(ttnn_to_layout_886, (1, 9, 1024), )
  ttnn_to_layout_887 = ttnn.to_layout(ttnn_from_device_882, ttnn.TILE_LAYOUT, )
  ttnn_to_device_419 = ttnn.to_device(ttnn_to_layout_887, device = device)
  ttnn_matmul_186 = ttnn.matmul(ttnn_to_device_419, ttnn_transpose_2, )
  ttnn_add_140 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_186, )
  ttnn_from_device_885 = ttnn.from_device(ttnn_add_140, )
  ttnn_to_layout_888 = ttnn.to_layout(ttnn_from_device_885, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_513 = ttnn.reshape(ttnn_to_layout_888, (1, 9, 1024), )
  ttnn_to_layout_889 = ttnn.to_layout(ttnn_from_device_882, ttnn.TILE_LAYOUT, )
  ttnn_to_device_420 = ttnn.to_device(ttnn_to_layout_889, device = device)
  ttnn_matmul_187 = ttnn.matmul(ttnn_to_device_420, ttnn_transpose_3, )
  ttnn_add_141 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_187, )
  ttnn_from_device_887 = ttnn.from_device(ttnn_add_141, )
  ttnn_to_layout_890 = ttnn.to_layout(ttnn_from_device_887, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_515 = ttnn.reshape(ttnn_to_layout_890, (1, 9, 1024), )
  ttnn_from_device_888 = ttnn.from_device(ttnn_reshape_511, )
  ttnn_to_layout_891 = ttnn.to_layout(ttnn_from_device_888, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_516 = ttnn.reshape(ttnn_to_layout_891, (1, 9, 16, 64), )
  ttnn_from_device_889 = ttnn.from_device(ttnn_reshape_516, )
  ttnn_to_layout_892 = ttnn.to_layout(ttnn_from_device_889, ttnn.TILE_LAYOUT, )
  ttnn_to_device_421 = ttnn.to_device(ttnn_to_layout_892, device = device)
  ttnn_permute_69 = ttnn.permute(ttnn_to_device_421, (0, 2, 1, 3), )
  ttnn_from_device_890 = ttnn.from_device(ttnn_reshape_513, )
  ttnn_to_layout_893 = ttnn.to_layout(ttnn_from_device_890, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_517 = ttnn.reshape(ttnn_to_layout_893, (1, 9, 16, 64), )
  ttnn_from_device_891 = ttnn.from_device(ttnn_reshape_517, )
  ttnn_to_layout_894 = ttnn.to_layout(ttnn_from_device_891, ttnn.TILE_LAYOUT, )
  ttnn_to_device_422 = ttnn.to_device(ttnn_to_layout_894, device = device)
  ttnn_permute_70 = ttnn.permute(ttnn_to_device_422, (0, 2, 1, 3), )
  ttnn_from_device_892 = ttnn.from_device(ttnn_reshape_515, )
  ttnn_to_layout_895 = ttnn.to_layout(ttnn_from_device_892, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_518 = ttnn.reshape(ttnn_to_layout_895, (1, 9, 16, 64), )
  test_accuracy(view_493, ttnn_reshape_518)
  ttnn_from_device_893 = ttnn.from_device(ttnn_reshape_518, )
  ttnn_to_layout_896 = ttnn.to_layout(ttnn_from_device_893, ttnn.TILE_LAYOUT, )
  ttnn_to_device_423 = ttnn.to_device(ttnn_to_layout_896, device = device)
  ttnn_permute_71 = ttnn.permute(ttnn_to_device_423, (0, 2, 1, 3), )
  test_accuracy(permute_71, ttnn_permute_71)
  ttnn_transpose_188 = ttnn.transpose(ttnn_permute_70, 3, 2, )
  test_accuracy(transpose_46, ttnn_transpose_188)
  ttnn_from_device_894 = ttnn.from_device(ttnn_permute_69, )
  ttnn_to_layout_897 = ttnn.to_layout(ttnn_from_device_894, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_519 = ttnn.reshape(ttnn_to_layout_897, (16, 9, 64), )
  ttnn_from_device_895 = ttnn.from_device(ttnn_transpose_188, )
  ttnn_to_layout_898 = ttnn.to_layout(ttnn_from_device_895, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_520 = ttnn.reshape(ttnn_to_layout_898, (16, 64, 9), )
  test_accuracy(view_495, ttnn_reshape_520)
  ttnn_from_device_896 = ttnn.from_device(ttnn_reshape_519, )
  ttnn_to_layout_899 = ttnn.to_layout(ttnn_from_device_896, ttnn.TILE_LAYOUT, )
  ttnn_to_device_424 = ttnn.to_device(ttnn_to_layout_899, device = device)
  ttnn_from_device_897 = ttnn.from_device(ttnn_reshape_520, )
  ttnn_to_layout_900 = ttnn.to_layout(ttnn_from_device_897, ttnn.TILE_LAYOUT, )
  ttnn_to_device_425 = ttnn.to_device(ttnn_to_layout_900, device = device)
  ttnn_matmul_188 = ttnn.matmul(ttnn_to_device_424, ttnn_to_device_425, )
  test_accuracy(bmm_46, ttnn_matmul_188)
  ttnn_from_device_898 = ttnn.from_device(ttnn_matmul_188, )
  ttnn_to_layout_901 = ttnn.to_layout(ttnn_from_device_898, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_521 = ttnn.reshape(ttnn_to_layout_901, (1, 16, 9, 9), )
  test_accuracy(view_496, ttnn_reshape_521)
  ttnn_from_device_899 = ttnn.from_device(ttnn_reshape_521, )
  ttnn_to_layout_902 = ttnn.to_layout(ttnn_from_device_899, ttnn.TILE_LAYOUT, )
  ttnn_to_device_426 = ttnn.to_device(ttnn_to_layout_902, device = device)
  ttnn_multiply_116 = ttnn.multiply(ttnn_to_device_426, 0.125, )
  test_accuracy(div_23, ttnn_multiply_116)
  ttnn_add_264 = ttnn.add(ttnn_multiply_116, ttnn_multiply, )
  test_accuracy(add_117, ttnn_add_264)
  ttnn_softmax_23 = ttnn.softmax(ttnn_add_264, -1, numeric_stable = True)
  test_accuracy(_softmax_23, ttnn_softmax_23)
  test_accuracy(_softmax_23, ttnn_softmax_23)
  ttnn_prefix_clone_70 = clone_wrapper(ttnn_softmax_23, )
  test_accuracy(clone_73, ttnn_prefix_clone_70)
  ttnn_from_device_900 = ttnn.from_device(ttnn_prefix_clone_70, )
  ttnn_to_layout_903 = ttnn.to_layout(ttnn_from_device_900, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_522 = ttnn.reshape(ttnn_to_layout_903, (16, 9, 9), )
  test_accuracy(view_497, ttnn_reshape_522)
  ttnn_from_device_901 = ttnn.from_device(ttnn_permute_71, )
  ttnn_to_layout_904 = ttnn.to_layout(ttnn_from_device_901, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_523 = ttnn.reshape(ttnn_to_layout_904, (16, 9, 64), )
  test_accuracy(view_498, ttnn_reshape_523)
  ttnn_from_device_902 = ttnn.from_device(ttnn_reshape_522, )
  ttnn_to_layout_905 = ttnn.to_layout(ttnn_from_device_902, ttnn.TILE_LAYOUT, )
  ttnn_to_device_427 = ttnn.to_device(ttnn_to_layout_905, device = device)
  ttnn_from_device_903 = ttnn.from_device(ttnn_reshape_523, )
  ttnn_to_layout_906 = ttnn.to_layout(ttnn_from_device_903, ttnn.TILE_LAYOUT, )
  ttnn_to_device_428 = ttnn.to_device(ttnn_to_layout_906, device = device)
  ttnn_matmul_189 = ttnn.matmul(ttnn_to_device_427, ttnn_to_device_428, )
  test_accuracy(bmm_47, ttnn_matmul_189)
  ttnn_from_device_904 = ttnn.from_device(ttnn_matmul_189, )
  ttnn_to_layout_907 = ttnn.to_layout(ttnn_from_device_904, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_524 = ttnn.reshape(ttnn_to_layout_907, (1, 16, 9, 64), )
  test_accuracy(view_499, ttnn_reshape_524)
  ttnn_from_device_905 = ttnn.from_device(ttnn_reshape_524, )
  ttnn_to_layout_908 = ttnn.to_layout(ttnn_from_device_905, ttnn.TILE_LAYOUT, )
  ttnn_to_device_429 = ttnn.to_device(ttnn_to_layout_908, device = device)
  ttnn_transpose_189 = ttnn.transpose(ttnn_to_device_429, 2, 1, )
  test_accuracy(transpose_47, ttnn_transpose_189)
  ttnn_prefix_clone_71 = clone_wrapper(ttnn_transpose_189, )
  test_accuracy(clone_74, ttnn_prefix_clone_71)
  ttnn_from_device_906 = ttnn.from_device(ttnn_prefix_clone_71, )
  ttnn_to_layout_909 = ttnn.to_layout(ttnn_from_device_906, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_525 = ttnn.reshape(ttnn_to_layout_909, (1, 9, 1024), )
  test_accuracy(_unsafe_view_23, ttnn_reshape_525)
  ttnn_from_device_907 = ttnn.from_device(ttnn_reshape_525, )
  ttnn_to_layout_910 = ttnn.to_layout(ttnn_from_device_907, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_526 = ttnn.reshape(ttnn_to_layout_910, (9, 1024), )
  ttnn_from_device_908 = ttnn.from_device(ttnn_reshape_526, )
  ttnn_to_layout_911 = ttnn.to_layout(ttnn_from_device_908, ttnn.TILE_LAYOUT, )
  ttnn_to_device_430 = ttnn.to_device(ttnn_to_layout_911, device = device)
  ttnn_matmul_190 = ttnn.matmul(ttnn_to_device_430, ttnn_transpose_6, )
  ttnn_add_142 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_190, )
  ttnn_from_device_909 = ttnn.from_device(ttnn_add_142, )
  ttnn_to_layout_912 = ttnn.to_layout(ttnn_from_device_909, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_527 = ttnn.reshape(ttnn_to_layout_912, (1, 9, 1024), )
  ttnn_from_device_910 = ttnn.from_device(ttnn_reshape_527, )
  ttnn_to_layout_913 = ttnn.to_layout(ttnn_from_device_910, ttnn.TILE_LAYOUT, )
  ttnn_to_device_431 = ttnn.to_device(ttnn_to_layout_913, device = device)
  ttnn_prefix_clone_72 = clone_wrapper(ttnn_to_device_431, )
  test_accuracy(clone_75, ttnn_prefix_clone_72)
  ttnn_add_265 = ttnn.add(ttnn_layer_norm_46, ttnn_prefix_clone_72, )
  ttnn_layer_norm_47_ = ttnn.layer_norm(ttnn_add_265, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_47_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_265), [1024], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_47 = ttnn.from_torch(ttnn_layer_norm_47_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_47_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_47_), ttnn_layer_norm_47))
  ttnn_from_device_911 = ttnn.from_device(ttnn_layer_norm_47, )
  ttnn_to_layout_914 = ttnn.to_layout(ttnn_from_device_911, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_528 = ttnn.reshape(ttnn_to_layout_914, (9, 1024), )
  ttnn_from_device_912 = ttnn.from_device(ttnn_reshape_528, )
  ttnn_to_layout_915 = ttnn.to_layout(ttnn_from_device_912, ttnn.TILE_LAYOUT, )
  ttnn_to_device_432 = ttnn.to_device(ttnn_to_layout_915, device = device)
  ttnn_matmul_191 = ttnn.matmul(ttnn_to_device_432, ttnn_transpose_7, )
  ttnn_add_143 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_191, )
  test_accuracy(addmm_143, ttnn_add_143)
  ttnn_from_device_913 = ttnn.from_device(ttnn_add_143, )
  ttnn_to_layout_916 = ttnn.to_layout(ttnn_from_device_913, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_529 = ttnn.reshape(ttnn_to_layout_916, (1, 9, 4096), )
  test_accuracy(view_503, ttnn_reshape_529)
  ttnn_from_device_914 = ttnn.from_device(ttnn_reshape_529, )
  ttnn_to_layout_917 = ttnn.to_layout(ttnn_from_device_914, ttnn.TILE_LAYOUT, )
  ttnn_to_device_433 = ttnn.to_device(ttnn_to_layout_917, device = device)
  ttnn_multiply_117 = ttnn.multiply(ttnn_to_device_433, 0.5, )
  ttnn_pow_23 = ttnn.pow(ttnn_to_device_433, 3.0, )
  test_accuracy(pow_24, ttnn_pow_23)
  ttnn_multiply_118 = ttnn.multiply(ttnn_pow_23, 0.044715, )
  ttnn_add_266 = ttnn.add(ttnn_to_device_433, ttnn_multiply_118, )
  ttnn_multiply_119 = ttnn.multiply(ttnn_add_266, 0.7978845608028654, )
  ttnn_tanh_23_ = ttnn.tanh(ttnn_multiply_119, )
  ttnn_tanh_23_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_119), )
  ttnn_tanh_23 = ttnn.from_torch(ttnn_tanh_23_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_23_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_23_), ttnn_tanh_23))
  test_accuracy(tanh_23, ttnn_tanh_23)
  test_accuracy(tanh_23, ttnn_tanh_23)
  ttnn_add_267 = ttnn.add(ttnn_tanh_23, 1.0, )
  test_accuracy(add_120, ttnn_add_267)
  ttnn_multiply_120 = ttnn.multiply(ttnn_multiply_117, ttnn_add_267, )
  test_accuracy(mul_96, ttnn_multiply_120)
  ttnn_from_device_915 = ttnn.from_device(ttnn_multiply_120, )
  ttnn_to_layout_918 = ttnn.to_layout(ttnn_from_device_915, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_530 = ttnn.reshape(ttnn_to_layout_918, (9, 4096), )
  test_accuracy(view_504, ttnn_reshape_530)
  ttnn_from_device_916 = ttnn.from_device(ttnn_reshape_530, )
  ttnn_to_layout_919 = ttnn.to_layout(ttnn_from_device_916, ttnn.TILE_LAYOUT, )
  ttnn_to_device_434 = ttnn.to_device(ttnn_to_layout_919, device = device)
  ttnn_matmul_192 = ttnn.matmul(ttnn_to_device_434, ttnn_transpose_8, )
  ttnn_add_144 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_192, )
  test_accuracy(addmm_144, ttnn_add_144)
  ttnn_from_device_917 = ttnn.from_device(ttnn_add_144, )
  ttnn_to_layout_920 = ttnn.to_layout(ttnn_from_device_917, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_531 = ttnn.reshape(ttnn_to_layout_920, (1, 9, 1024), )
  test_accuracy(view_505, ttnn_reshape_531)
  ttnn_from_device_918 = ttnn.from_device(ttnn_reshape_531, )
  ttnn_to_layout_921 = ttnn.to_layout(ttnn_from_device_918, ttnn.TILE_LAYOUT, )
  ttnn_to_device_435 = ttnn.to_device(ttnn_to_layout_921, device = device)
  ttnn_add_268 = ttnn.add(ttnn_to_device_435, ttnn_layer_norm_47, )
  test_accuracy(add_121, ttnn_add_268)
  ttnn_layer_norm_48_ = ttnn.layer_norm(ttnn_add_268, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_48_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_268), [1024], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_48 = ttnn.from_torch(ttnn_layer_norm_48_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_48_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_48_), ttnn_layer_norm_48))
  ttnn_from_device_919 = ttnn.from_device(ttnn_layer_norm_48, )
  ttnn_to_layout_922 = ttnn.to_layout(ttnn_from_device_919, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_532 = ttnn.reshape(ttnn_to_layout_922, (9, 1024), )
  test_accuracy(view_506, ttnn_reshape_532)
  ttnn_from_torch_27 = ttnn.from_torch(arg23_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_193 = ttnn.transpose(ttnn_from_torch_27, 0, 1, )
  test_accuracy(t_145, ttnn_transpose_193)
  ttnn_from_device_920 = ttnn.from_device(ttnn_reshape_532, )
  ttnn_to_layout_923 = ttnn.to_layout(ttnn_from_device_920, ttnn.TILE_LAYOUT, )
  ttnn_to_device_436 = ttnn.to_device(ttnn_to_layout_923, device = device)
  ttnn_matmul_193 = ttnn.matmul(ttnn_to_device_436, ttnn_transpose_193, )
  ttnn_from_torch_28 = ttnn.from_torch(arg24_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_145 = ttnn.add(ttnn_from_torch_28, ttnn_matmul_193, )
  test_accuracy(addmm_145, ttnn_add_145)
  ttnn_from_device_921 = ttnn.from_device(ttnn_add_145, )
  ttnn_to_layout_924 = ttnn.to_layout(ttnn_from_device_921, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_533 = ttnn.reshape(ttnn_to_layout_924, (1, 9, 128), )
  test_accuracy(view_507, ttnn_reshape_533)
  ttnn_from_device_922 = ttnn.from_device(ttnn_reshape_533, )
  ttnn_to_layout_925 = ttnn.to_layout(ttnn_from_device_922, ttnn.TILE_LAYOUT, )
  ttnn_to_device_437 = ttnn.to_device(ttnn_to_layout_925, device = device)
  ttnn_multiply_121 = ttnn.multiply(ttnn_to_device_437, 0.5, )
  ttnn_pow_24 = ttnn.pow(ttnn_to_device_437, 3.0, )
  test_accuracy(pow_25, ttnn_pow_24)
  ttnn_multiply_122 = ttnn.multiply(ttnn_pow_24, 0.044715, )
  ttnn_add_269 = ttnn.add(ttnn_to_device_437, ttnn_multiply_122, )
  ttnn_multiply_123 = ttnn.multiply(ttnn_add_269, 0.7978845608028654, )
  ttnn_tanh_24_ = ttnn.tanh(ttnn_multiply_123, )
  ttnn_tanh_24_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_123), )
  ttnn_tanh_24 = ttnn.from_torch(ttnn_tanh_24_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_24_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_24_), ttnn_tanh_24))
  test_accuracy(tanh_24, ttnn_tanh_24)
  test_accuracy(tanh_24, ttnn_tanh_24)
  ttnn_add_270 = ttnn.add(ttnn_tanh_24, 1.0, )
  test_accuracy(add_123, ttnn_add_270)
  ttnn_multiply_124 = ttnn.multiply(ttnn_multiply_121, ttnn_add_270, )
  test_accuracy(mul_100, ttnn_multiply_124)
  ttnn_from_torch_29 = ttnn.from_torch(arg25_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_30 = ttnn.from_torch(arg26_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_49_ = ttnn.layer_norm(ttnn_multiply_124, epsilon = 1e-12, weight = ttnn_from_torch_29, bias = ttnn_from_torch_30)
  ttnn_layer_norm_49_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_multiply_124), [128], ttnn.to_torch(ttnn_from_torch_29), 
                                                                ttnn.to_torch(ttnn_from_torch_30), 1e-12, )[0]
  ttnn_layer_norm_49 = ttnn.from_torch(ttnn_layer_norm_49_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_49_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_49_), ttnn_layer_norm_49))
  ttnn_from_device_923 = ttnn.from_device(ttnn_layer_norm_49, )
  ttnn_to_layout_926 = ttnn.to_layout(ttnn_from_device_923, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_534 = ttnn.reshape(ttnn_to_layout_926, (9, 128), )
  test_accuracy(view_508, ttnn_reshape_534)
  ttnn_from_torch_31 = ttnn.from_torch(arg27_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_194 = ttnn.transpose(ttnn_from_torch_31, 0, 1, )
  test_accuracy(t_146, ttnn_transpose_194)
  ttnn_from_device_924 = ttnn.from_device(ttnn_reshape_534, )
  ttnn_to_layout_927 = ttnn.to_layout(ttnn_from_device_924, ttnn.TILE_LAYOUT, )
  ttnn_to_device_438 = ttnn.to_device(ttnn_to_layout_927, device = device)
  ttnn_matmul_194 = ttnn.matmul(ttnn_to_device_438, ttnn_transpose_194, )
  ttnn_from_torch_32 = ttnn.from_torch(arg28_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_146 = ttnn.add(ttnn_from_torch_32, ttnn_matmul_194, )
  test_accuracy(addmm_146, ttnn_add_146)
  ttnn_from_device_925 = ttnn.from_device(ttnn_add_146, )
  ttnn_to_layout_928 = ttnn.to_layout(ttnn_from_device_925, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_535 = ttnn.reshape(ttnn_to_layout_928, (1, 9, 30000), )
  print("Final accuracy: ", test_accuracy(view_509, ttnn_reshape_535))
  ttnn_to_torch = ttnn.to_torch(ttnn_reshape_535, dtype = torch.bfloat16)
  ttnn.close_device(device)

if __name__ == "__main__":
    filepath = Path(__file__).with_name("albert-large-v2_inputs.pickle")
    file = lzma.open(filepath, "rb")
    inputs = pickle.load(file)
    forward(*inputs)

