import lzma
import numpy as np
import pickle
import torch
import ttnn
from pathlib import Path
import os
import csv
aten = torch.ops.aten
@torch.fx.wrap
def clone(t):
    return ttnn.clone(t, memory_config=t.memory_config(), dtype=t.dtype)


ref = globals()["clone"]
globals()["clone_wrapper"] = ref
del globals()["clone"]

def comp_pcc(golden, calculated, pcc=0.99):
    golden = torch.Tensor(golden)
    calculated = torch.Tensor(calculated)

    if golden.dtype != calculated.dtype:
        calculated = calculated.type(golden.dtype)

    if torch.all(torch.isnan(golden)) and torch.all(torch.isnan(calculated)):
        # logger.warning("Both tensors are 'nan'")
        return True, 1.0

    if torch.all(torch.isnan(golden)) or torch.all(torch.isnan(calculated)):
        # logger.error("One tensor is all nan, the other is not.")
        return False, 0.0

    # Test if either is completely zero
    if torch.any(golden.bool()) != torch.any(calculated.bool()):
        # logger.error("One tensor is all zero")
        return False, 0.0

    # For now, mask all infs and nans so that we check the rest... TODO
    golden = golden.clone()
    golden[
        torch.logical_or(
            torch.isnan(golden),
            torch.logical_or(torch.isinf(golden), torch.isneginf(golden)),
        )
    ] = 0
    calculated = calculated.clone()
    calculated[
        torch.logical_or(
            torch.isnan(calculated),
            torch.logical_or(torch.isinf(calculated), torch.isneginf(calculated)),
        )
    ] = 0

    if torch.equal(golden, calculated):
        return True, 1.0

    if golden.dtype == torch.bfloat16:
        golden = golden.type(torch.float32)
        calculated = calculated.type(torch.float32)
    cal_pcc = np.min(
        np.ma.corrcoef(
            np.ma.masked_invalid(torch.squeeze(golden).detach().numpy()).flatten(),
            np.ma.masked_invalid(torch.squeeze(calculated).detach().numpy()).flatten(),
        )
    )

    if isinstance(cal_pcc, np.ma.core.MaskedConstant):
        return True, 1.0

    return cal_pcc >= pcc, cal_pcc

def construct_pcc_assert_message(message, expected_pytorch_result, actual_pytorch_result):
    messages = []
    messages.append(message)
    # messages.append("Expected")
    # messages.append(str(expected_pytorch_result))
    # messages.append("Actual")
    # messages.append(str(actual_pytorch_result))
    messages = [str(m) for m in messages]
    return "\n".join(messages)

def assert_with_pcc(expected_pytorch_result, actual_pytorch_result, pcc=0.999):
    assert list(expected_pytorch_result.shape) == list(
        actual_pytorch_result.shape
    ), f"list(expected_pytorch_result.shape)={list(expected_pytorch_result.shape)} vs list(actual_pytorch_result.shape)={list(actual_pytorch_result.shape)}"
    pcc_passed, pcc_message = comp_pcc(expected_pytorch_result, actual_pytorch_result, pcc)
    assert pcc_passed, construct_pcc_assert_message(pcc_message, expected_pytorch_result, actual_pytorch_result)
    return pcc_passed, pcc_message


def test_accuracy(expected, actual):
    if isinstance(actual, ttnn.Tensor):
        actual = ttnn.to_torch(actual)
    return assert_with_pcc(expected, actual, pcc = 0.90)


def forward(arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1, arg22_1, arg23_1, arg24_1, arg25_1, arg26_1, arg27_1, arg28_1, arg29_1, arg30_1, arg31_1, arg32_1):
  device = ttnn.open_device(device_id=0, l1_small_size=16384)
  unsqueeze = aten.unsqueeze.default(arg31_1, 1, )
  unsqueeze_1 = aten.unsqueeze.default(unsqueeze, 2, )
  _to_copy = aten._to_copy.default(unsqueeze_1, dtype = torch.bfloat16)
  rsub = aten.rsub.Scalar(_to_copy, 1.0, )
  mul = aten.mul.Tensor(rsub, -3.3895313892515355e+38, )
  slice_1 = aten.slice.Tensor(arg29_1, 0, 0, 9223372036854775807, )
  slice_2 = aten.slice.Tensor(slice_1, 1, 0, 9, )
  embedding = aten.embedding.default(arg0_1, arg30_1, 0, )
  embedding_1 = aten.embedding.default(arg1_1, arg32_1, )
  add = aten.add.Tensor(embedding, embedding_1, )
  embedding_2 = aten.embedding.default(arg2_1, slice_2, )
  add_1 = aten.add.Tensor(add, embedding_2, )
  native_layer_norm = aten.native_layer_norm.default(add_1, [128], arg3_1, arg4_1, 1e-12, )
  getitem = native_layer_norm[0]
  clone_3 = aten.clone.default(getitem, )
  view = aten.view.default(clone_3, [9, 128], )
  t = aten.t.default(arg5_1, )
  addmm = aten.addmm.default(arg6_1, view, t, )
  view_1 = aten.view.default(addmm, [1, 9, 768], )
  view_2 = aten.view.default(view_1, [9, 768], )
  t_1 = aten.t.default(arg7_1, )
  addmm_1 = aten.addmm.default(arg8_1, view_2, t_1, )
  view_3 = aten.view.default(addmm_1, [1, 9, 768], )
  view_4 = aten.view.default(view_1, [9, 768], )
  t_2 = aten.t.default(arg9_1, )
  addmm_2 = aten.addmm.default(arg10_1, view_4, t_2, )
  view_5 = aten.view.default(addmm_2, [1, 9, 768], )
  view_6 = aten.view.default(view_1, [9, 768], )
  t_3 = aten.t.default(arg11_1, )
  addmm_3 = aten.addmm.default(arg12_1, view_6, t_3, )
  view_7 = aten.view.default(addmm_3, [1, 9, 768], )
  view_8 = aten.view.default(view_3, [1, 9, 12, 64], )
  permute = aten.permute.default(view_8, [0, 2, 1, 3], )
  view_9 = aten.view.default(view_5, [1, 9, 12, 64], )
  permute_1 = aten.permute.default(view_9, [0, 2, 1, 3], )
  view_10 = aten.view.default(view_7, [1, 9, 12, 64], )
  permute_2 = aten.permute.default(view_10, [0, 2, 1, 3], )
  transpose = aten.transpose.int(permute_1, -1, -2, )
  expand = aten.expand.default(permute, [1, 12, 9, 64], )
  view_11 = aten.view.default(expand, [12, 9, 64], )
  expand_1 = aten.expand.default(transpose, [1, 12, 64, 9], )
  view_12 = aten.view.default(expand_1, [12, 64, 9], )
  bmm = aten.bmm.default(view_11, view_12, )
  view_13 = aten.view.default(bmm, [1, 12, 9, 9], )
  div = aten.div.Tensor(view_13, 8.0, )
  add_2 = aten.add.Tensor(div, mul, )
  _softmax = aten._softmax.default(add_2, -1, False, )
  clone_4 = aten.clone.default(_softmax, )
  expand_2 = aten.expand.default(clone_4, [1, 12, 9, 9], )
  view_14 = aten.view.default(expand_2, [12, 9, 9], )
  expand_3 = aten.expand.default(permute_2, [1, 12, 9, 64], )
  view_15 = aten.view.default(expand_3, [12, 9, 64], )
  bmm_1 = aten.bmm.default(view_14, view_15, )
  view_16 = aten.view.default(bmm_1, [1, 12, 9, 64], )
  transpose_1 = aten.transpose.int(view_16, 2, 1, )
  clone_5 = aten.clone.default(transpose_1, memory_format = torch.contiguous_format)
  _unsafe_view = aten._unsafe_view.default(clone_5, [1, 9, 768], )
  view_17 = aten.view.default(_unsafe_view, [9, 768], )
  t_4 = aten.t.default(arg13_1, )
  addmm_4 = aten.addmm.default(arg14_1, view_17, t_4, )
  view_18 = aten.view.default(addmm_4, [1, 9, 768], )
  clone_6 = aten.clone.default(view_18, )
  add_3 = aten.add.Tensor(view_1, clone_6, )
  native_layer_norm_1 = aten.native_layer_norm.default(add_3, [768], arg15_1, arg16_1, 1e-12, )
  getitem_3 = native_layer_norm_1[0]
  view_19 = aten.view.default(getitem_3, [9, 768], )
  t_5 = aten.t.default(arg17_1, )
  addmm_5 = aten.addmm.default(arg18_1, view_19, t_5, )
  view_20 = aten.view.default(addmm_5, [1, 9, 3072], )
  mul_1 = aten.mul.Tensor(view_20, 0.5, )
  pow_1 = aten.pow.Tensor_Scalar(view_20, 3.0, )
  mul_2 = aten.mul.Tensor(pow_1, 0.044715, )
  add_4 = aten.add.Tensor(view_20, mul_2, )
  mul_3 = aten.mul.Tensor(add_4, 0.7978845608028654, )
  tanh = aten.tanh.default(mul_3, )
  add_5 = aten.add.Tensor(tanh, 1.0, )
  mul_4 = aten.mul.Tensor(mul_1, add_5, )
  view_21 = aten.view.default(mul_4, [9, 3072], )
  t_6 = aten.t.default(arg19_1, )
  addmm_6 = aten.addmm.default(arg20_1, view_21, t_6, )
  view_22 = aten.view.default(addmm_6, [1, 9, 768], )
  add_6 = aten.add.Tensor(view_22, getitem_3, )
  native_layer_norm_2 = aten.native_layer_norm.default(add_6, [768], arg21_1, arg22_1, 1e-12, )
  getitem_6 = native_layer_norm_2[0]
  view_23 = aten.view.default(getitem_6, [9, 768], )
  t_7 = aten.t.default(arg7_1, )
  addmm_7 = aten.addmm.default(arg8_1, view_23, t_7, )
  view_24 = aten.view.default(addmm_7, [1, 9, 768], )
  view_25 = aten.view.default(getitem_6, [9, 768], )
  t_8 = aten.t.default(arg9_1, )
  addmm_8 = aten.addmm.default(arg10_1, view_25, t_8, )
  view_26 = aten.view.default(addmm_8, [1, 9, 768], )
  view_27 = aten.view.default(getitem_6, [9, 768], )
  t_9 = aten.t.default(arg11_1, )
  addmm_9 = aten.addmm.default(arg12_1, view_27, t_9, )
  view_28 = aten.view.default(addmm_9, [1, 9, 768], )
  view_29 = aten.view.default(view_24, [1, 9, 12, 64], )
  permute_3 = aten.permute.default(view_29, [0, 2, 1, 3], )
  view_30 = aten.view.default(view_26, [1, 9, 12, 64], )
  permute_4 = aten.permute.default(view_30, [0, 2, 1, 3], )
  view_31 = aten.view.default(view_28, [1, 9, 12, 64], )
  permute_5 = aten.permute.default(view_31, [0, 2, 1, 3], )
  transpose_2 = aten.transpose.int(permute_4, -1, -2, )
  expand_4 = aten.expand.default(permute_3, [1, 12, 9, 64], )
  view_32 = aten.view.default(expand_4, [12, 9, 64], )
  expand_5 = aten.expand.default(transpose_2, [1, 12, 64, 9], )
  view_33 = aten.view.default(expand_5, [12, 64, 9], )
  bmm_2 = aten.bmm.default(view_32, view_33, )
  view_34 = aten.view.default(bmm_2, [1, 12, 9, 9], )
  div_1 = aten.div.Tensor(view_34, 8.0, )
  add_7 = aten.add.Tensor(div_1, mul, )
  _softmax_1 = aten._softmax.default(add_7, -1, False, )
  clone_7 = aten.clone.default(_softmax_1, )
  expand_6 = aten.expand.default(clone_7, [1, 12, 9, 9], )
  view_35 = aten.view.default(expand_6, [12, 9, 9], )
  expand_7 = aten.expand.default(permute_5, [1, 12, 9, 64], )
  view_36 = aten.view.default(expand_7, [12, 9, 64], )
  bmm_3 = aten.bmm.default(view_35, view_36, )
  view_37 = aten.view.default(bmm_3, [1, 12, 9, 64], )
  transpose_3 = aten.transpose.int(view_37, 2, 1, )
  clone_8 = aten.clone.default(transpose_3, memory_format = torch.contiguous_format)
  _unsafe_view_1 = aten._unsafe_view.default(clone_8, [1, 9, 768], )
  view_38 = aten.view.default(_unsafe_view_1, [9, 768], )
  t_10 = aten.t.default(arg13_1, )
  addmm_10 = aten.addmm.default(arg14_1, view_38, t_10, )
  view_39 = aten.view.default(addmm_10, [1, 9, 768], )
  clone_9 = aten.clone.default(view_39, )
  add_8 = aten.add.Tensor(getitem_6, clone_9, )
  native_layer_norm_3 = aten.native_layer_norm.default(add_8, [768], arg15_1, arg16_1, 1e-12, )
  getitem_9 = native_layer_norm_3[0]
  view_40 = aten.view.default(getitem_9, [9, 768], )
  t_11 = aten.t.default(arg17_1, )
  addmm_11 = aten.addmm.default(arg18_1, view_40, t_11, )
  view_41 = aten.view.default(addmm_11, [1, 9, 3072], )
  mul_5 = aten.mul.Tensor(view_41, 0.5, )
  pow_2 = aten.pow.Tensor_Scalar(view_41, 3.0, )
  mul_6 = aten.mul.Tensor(pow_2, 0.044715, )
  add_9 = aten.add.Tensor(view_41, mul_6, )
  mul_7 = aten.mul.Tensor(add_9, 0.7978845608028654, )
  tanh_1 = aten.tanh.default(mul_7, )
  add_10 = aten.add.Tensor(tanh_1, 1.0, )
  mul_8 = aten.mul.Tensor(mul_5, add_10, )
  view_42 = aten.view.default(mul_8, [9, 3072], )
  t_12 = aten.t.default(arg19_1, )
  addmm_12 = aten.addmm.default(arg20_1, view_42, t_12, )
  view_43 = aten.view.default(addmm_12, [1, 9, 768], )
  add_11 = aten.add.Tensor(view_43, getitem_9, )
  native_layer_norm_4 = aten.native_layer_norm.default(add_11, [768], arg21_1, arg22_1, 1e-12, )
  getitem_12 = native_layer_norm_4[0]
  view_44 = aten.view.default(getitem_12, [9, 768], )
  t_13 = aten.t.default(arg7_1, )
  addmm_13 = aten.addmm.default(arg8_1, view_44, t_13, )
  view_45 = aten.view.default(addmm_13, [1, 9, 768], )
  view_46 = aten.view.default(getitem_12, [9, 768], )
  t_14 = aten.t.default(arg9_1, )
  addmm_14 = aten.addmm.default(arg10_1, view_46, t_14, )
  view_47 = aten.view.default(addmm_14, [1, 9, 768], )
  view_48 = aten.view.default(getitem_12, [9, 768], )
  t_15 = aten.t.default(arg11_1, )
  addmm_15 = aten.addmm.default(arg12_1, view_48, t_15, )
  view_49 = aten.view.default(addmm_15, [1, 9, 768], )
  view_50 = aten.view.default(view_45, [1, 9, 12, 64], )
  permute_6 = aten.permute.default(view_50, [0, 2, 1, 3], )
  view_51 = aten.view.default(view_47, [1, 9, 12, 64], )
  permute_7 = aten.permute.default(view_51, [0, 2, 1, 3], )
  view_52 = aten.view.default(view_49, [1, 9, 12, 64], )
  permute_8 = aten.permute.default(view_52, [0, 2, 1, 3], )
  transpose_4 = aten.transpose.int(permute_7, -1, -2, )
  expand_8 = aten.expand.default(permute_6, [1, 12, 9, 64], )
  view_53 = aten.view.default(expand_8, [12, 9, 64], )
  expand_9 = aten.expand.default(transpose_4, [1, 12, 64, 9], )
  view_54 = aten.view.default(expand_9, [12, 64, 9], )
  bmm_4 = aten.bmm.default(view_53, view_54, )
  view_55 = aten.view.default(bmm_4, [1, 12, 9, 9], )
  div_2 = aten.div.Tensor(view_55, 8.0, )
  add_12 = aten.add.Tensor(div_2, mul, )
  _softmax_2 = aten._softmax.default(add_12, -1, False, )
  clone_10 = aten.clone.default(_softmax_2, )
  expand_10 = aten.expand.default(clone_10, [1, 12, 9, 9], )
  view_56 = aten.view.default(expand_10, [12, 9, 9], )
  expand_11 = aten.expand.default(permute_8, [1, 12, 9, 64], )
  view_57 = aten.view.default(expand_11, [12, 9, 64], )
  bmm_5 = aten.bmm.default(view_56, view_57, )
  view_58 = aten.view.default(bmm_5, [1, 12, 9, 64], )
  transpose_5 = aten.transpose.int(view_58, 2, 1, )
  clone_11 = aten.clone.default(transpose_5, memory_format = torch.contiguous_format)
  _unsafe_view_2 = aten._unsafe_view.default(clone_11, [1, 9, 768], )
  view_59 = aten.view.default(_unsafe_view_2, [9, 768], )
  t_16 = aten.t.default(arg13_1, )
  addmm_16 = aten.addmm.default(arg14_1, view_59, t_16, )
  view_60 = aten.view.default(addmm_16, [1, 9, 768], )
  clone_12 = aten.clone.default(view_60, )
  add_13 = aten.add.Tensor(getitem_12, clone_12, )
  native_layer_norm_5 = aten.native_layer_norm.default(add_13, [768], arg15_1, arg16_1, 1e-12, )
  getitem_15 = native_layer_norm_5[0]
  view_61 = aten.view.default(getitem_15, [9, 768], )
  t_17 = aten.t.default(arg17_1, )
  addmm_17 = aten.addmm.default(arg18_1, view_61, t_17, )
  view_62 = aten.view.default(addmm_17, [1, 9, 3072], )
  mul_9 = aten.mul.Tensor(view_62, 0.5, )
  pow_3 = aten.pow.Tensor_Scalar(view_62, 3.0, )
  mul_10 = aten.mul.Tensor(pow_3, 0.044715, )
  add_14 = aten.add.Tensor(view_62, mul_10, )
  mul_11 = aten.mul.Tensor(add_14, 0.7978845608028654, )
  tanh_2 = aten.tanh.default(mul_11, )
  add_15 = aten.add.Tensor(tanh_2, 1.0, )
  mul_12 = aten.mul.Tensor(mul_9, add_15, )
  view_63 = aten.view.default(mul_12, [9, 3072], )
  t_18 = aten.t.default(arg19_1, )
  addmm_18 = aten.addmm.default(arg20_1, view_63, t_18, )
  view_64 = aten.view.default(addmm_18, [1, 9, 768], )
  add_16 = aten.add.Tensor(view_64, getitem_15, )
  native_layer_norm_6 = aten.native_layer_norm.default(add_16, [768], arg21_1, arg22_1, 1e-12, )
  getitem_18 = native_layer_norm_6[0]
  view_65 = aten.view.default(getitem_18, [9, 768], )
  t_19 = aten.t.default(arg7_1, )
  addmm_19 = aten.addmm.default(arg8_1, view_65, t_19, )
  view_66 = aten.view.default(addmm_19, [1, 9, 768], )
  view_67 = aten.view.default(getitem_18, [9, 768], )
  t_20 = aten.t.default(arg9_1, )
  addmm_20 = aten.addmm.default(arg10_1, view_67, t_20, )
  view_68 = aten.view.default(addmm_20, [1, 9, 768], )
  view_69 = aten.view.default(getitem_18, [9, 768], )
  t_21 = aten.t.default(arg11_1, )
  addmm_21 = aten.addmm.default(arg12_1, view_69, t_21, )
  view_70 = aten.view.default(addmm_21, [1, 9, 768], )
  view_71 = aten.view.default(view_66, [1, 9, 12, 64], )
  permute_9 = aten.permute.default(view_71, [0, 2, 1, 3], )
  view_72 = aten.view.default(view_68, [1, 9, 12, 64], )
  permute_10 = aten.permute.default(view_72, [0, 2, 1, 3], )
  view_73 = aten.view.default(view_70, [1, 9, 12, 64], )
  permute_11 = aten.permute.default(view_73, [0, 2, 1, 3], )
  transpose_6 = aten.transpose.int(permute_10, -1, -2, )
  expand_12 = aten.expand.default(permute_9, [1, 12, 9, 64], )
  view_74 = aten.view.default(expand_12, [12, 9, 64], )
  expand_13 = aten.expand.default(transpose_6, [1, 12, 64, 9], )
  view_75 = aten.view.default(expand_13, [12, 64, 9], )
  bmm_6 = aten.bmm.default(view_74, view_75, )
  view_76 = aten.view.default(bmm_6, [1, 12, 9, 9], )
  div_3 = aten.div.Tensor(view_76, 8.0, )
  add_17 = aten.add.Tensor(div_3, mul, )
  _softmax_3 = aten._softmax.default(add_17, -1, False, )
  clone_13 = aten.clone.default(_softmax_3, )
  expand_14 = aten.expand.default(clone_13, [1, 12, 9, 9], )
  view_77 = aten.view.default(expand_14, [12, 9, 9], )
  expand_15 = aten.expand.default(permute_11, [1, 12, 9, 64], )
  view_78 = aten.view.default(expand_15, [12, 9, 64], )
  bmm_7 = aten.bmm.default(view_77, view_78, )
  view_79 = aten.view.default(bmm_7, [1, 12, 9, 64], )
  transpose_7 = aten.transpose.int(view_79, 2, 1, )
  clone_14 = aten.clone.default(transpose_7, memory_format = torch.contiguous_format)
  _unsafe_view_3 = aten._unsafe_view.default(clone_14, [1, 9, 768], )
  view_80 = aten.view.default(_unsafe_view_3, [9, 768], )
  t_22 = aten.t.default(arg13_1, )
  addmm_22 = aten.addmm.default(arg14_1, view_80, t_22, )
  view_81 = aten.view.default(addmm_22, [1, 9, 768], )
  clone_15 = aten.clone.default(view_81, )
  add_18 = aten.add.Tensor(getitem_18, clone_15, )
  native_layer_norm_7 = aten.native_layer_norm.default(add_18, [768], arg15_1, arg16_1, 1e-12, )
  getitem_21 = native_layer_norm_7[0]
  view_82 = aten.view.default(getitem_21, [9, 768], )
  t_23 = aten.t.default(arg17_1, )
  addmm_23 = aten.addmm.default(arg18_1, view_82, t_23, )
  view_83 = aten.view.default(addmm_23, [1, 9, 3072], )
  mul_13 = aten.mul.Tensor(view_83, 0.5, )
  pow_4 = aten.pow.Tensor_Scalar(view_83, 3.0, )
  mul_14 = aten.mul.Tensor(pow_4, 0.044715, )
  add_19 = aten.add.Tensor(view_83, mul_14, )
  mul_15 = aten.mul.Tensor(add_19, 0.7978845608028654, )
  tanh_3 = aten.tanh.default(mul_15, )
  add_20 = aten.add.Tensor(tanh_3, 1.0, )
  mul_16 = aten.mul.Tensor(mul_13, add_20, )
  view_84 = aten.view.default(mul_16, [9, 3072], )
  t_24 = aten.t.default(arg19_1, )
  addmm_24 = aten.addmm.default(arg20_1, view_84, t_24, )
  view_85 = aten.view.default(addmm_24, [1, 9, 768], )
  add_21 = aten.add.Tensor(view_85, getitem_21, )
  native_layer_norm_8 = aten.native_layer_norm.default(add_21, [768], arg21_1, arg22_1, 1e-12, )
  getitem_24 = native_layer_norm_8[0]
  view_86 = aten.view.default(getitem_24, [9, 768], )
  t_25 = aten.t.default(arg7_1, )
  addmm_25 = aten.addmm.default(arg8_1, view_86, t_25, )
  view_87 = aten.view.default(addmm_25, [1, 9, 768], )
  view_88 = aten.view.default(getitem_24, [9, 768], )
  t_26 = aten.t.default(arg9_1, )
  addmm_26 = aten.addmm.default(arg10_1, view_88, t_26, )
  view_89 = aten.view.default(addmm_26, [1, 9, 768], )
  view_90 = aten.view.default(getitem_24, [9, 768], )
  t_27 = aten.t.default(arg11_1, )
  addmm_27 = aten.addmm.default(arg12_1, view_90, t_27, )
  view_91 = aten.view.default(addmm_27, [1, 9, 768], )
  view_92 = aten.view.default(view_87, [1, 9, 12, 64], )
  permute_12 = aten.permute.default(view_92, [0, 2, 1, 3], )
  view_93 = aten.view.default(view_89, [1, 9, 12, 64], )
  permute_13 = aten.permute.default(view_93, [0, 2, 1, 3], )
  view_94 = aten.view.default(view_91, [1, 9, 12, 64], )
  permute_14 = aten.permute.default(view_94, [0, 2, 1, 3], )
  transpose_8 = aten.transpose.int(permute_13, -1, -2, )
  expand_16 = aten.expand.default(permute_12, [1, 12, 9, 64], )
  view_95 = aten.view.default(expand_16, [12, 9, 64], )
  expand_17 = aten.expand.default(transpose_8, [1, 12, 64, 9], )
  view_96 = aten.view.default(expand_17, [12, 64, 9], )
  bmm_8 = aten.bmm.default(view_95, view_96, )
  view_97 = aten.view.default(bmm_8, [1, 12, 9, 9], )
  div_4 = aten.div.Tensor(view_97, 8.0, )
  add_22 = aten.add.Tensor(div_4, mul, )
  _softmax_4 = aten._softmax.default(add_22, -1, False, )
  clone_16 = aten.clone.default(_softmax_4, )
  expand_18 = aten.expand.default(clone_16, [1, 12, 9, 9], )
  view_98 = aten.view.default(expand_18, [12, 9, 9], )
  expand_19 = aten.expand.default(permute_14, [1, 12, 9, 64], )
  view_99 = aten.view.default(expand_19, [12, 9, 64], )
  bmm_9 = aten.bmm.default(view_98, view_99, )
  view_100 = aten.view.default(bmm_9, [1, 12, 9, 64], )
  transpose_9 = aten.transpose.int(view_100, 2, 1, )
  clone_17 = aten.clone.default(transpose_9, memory_format = torch.contiguous_format)
  _unsafe_view_4 = aten._unsafe_view.default(clone_17, [1, 9, 768], )
  view_101 = aten.view.default(_unsafe_view_4, [9, 768], )
  t_28 = aten.t.default(arg13_1, )
  addmm_28 = aten.addmm.default(arg14_1, view_101, t_28, )
  view_102 = aten.view.default(addmm_28, [1, 9, 768], )
  clone_18 = aten.clone.default(view_102, )
  add_23 = aten.add.Tensor(getitem_24, clone_18, )
  native_layer_norm_9 = aten.native_layer_norm.default(add_23, [768], arg15_1, arg16_1, 1e-12, )
  getitem_27 = native_layer_norm_9[0]
  view_103 = aten.view.default(getitem_27, [9, 768], )
  t_29 = aten.t.default(arg17_1, )
  addmm_29 = aten.addmm.default(arg18_1, view_103, t_29, )
  view_104 = aten.view.default(addmm_29, [1, 9, 3072], )
  mul_17 = aten.mul.Tensor(view_104, 0.5, )
  pow_5 = aten.pow.Tensor_Scalar(view_104, 3.0, )
  mul_18 = aten.mul.Tensor(pow_5, 0.044715, )
  add_24 = aten.add.Tensor(view_104, mul_18, )
  mul_19 = aten.mul.Tensor(add_24, 0.7978845608028654, )
  tanh_4 = aten.tanh.default(mul_19, )
  add_25 = aten.add.Tensor(tanh_4, 1.0, )
  mul_20 = aten.mul.Tensor(mul_17, add_25, )
  view_105 = aten.view.default(mul_20, [9, 3072], )
  t_30 = aten.t.default(arg19_1, )
  addmm_30 = aten.addmm.default(arg20_1, view_105, t_30, )
  view_106 = aten.view.default(addmm_30, [1, 9, 768], )
  add_26 = aten.add.Tensor(view_106, getitem_27, )
  native_layer_norm_10 = aten.native_layer_norm.default(add_26, [768], arg21_1, arg22_1, 1e-12, )
  getitem_30 = native_layer_norm_10[0]
  view_107 = aten.view.default(getitem_30, [9, 768], )
  t_31 = aten.t.default(arg7_1, )
  addmm_31 = aten.addmm.default(arg8_1, view_107, t_31, )
  view_108 = aten.view.default(addmm_31, [1, 9, 768], )
  view_109 = aten.view.default(getitem_30, [9, 768], )
  t_32 = aten.t.default(arg9_1, )
  addmm_32 = aten.addmm.default(arg10_1, view_109, t_32, )
  view_110 = aten.view.default(addmm_32, [1, 9, 768], )
  view_111 = aten.view.default(getitem_30, [9, 768], )
  t_33 = aten.t.default(arg11_1, )
  addmm_33 = aten.addmm.default(arg12_1, view_111, t_33, )
  view_112 = aten.view.default(addmm_33, [1, 9, 768], )
  view_113 = aten.view.default(view_108, [1, 9, 12, 64], )
  permute_15 = aten.permute.default(view_113, [0, 2, 1, 3], )
  view_114 = aten.view.default(view_110, [1, 9, 12, 64], )
  permute_16 = aten.permute.default(view_114, [0, 2, 1, 3], )
  view_115 = aten.view.default(view_112, [1, 9, 12, 64], )
  permute_17 = aten.permute.default(view_115, [0, 2, 1, 3], )
  transpose_10 = aten.transpose.int(permute_16, -1, -2, )
  expand_20 = aten.expand.default(permute_15, [1, 12, 9, 64], )
  view_116 = aten.view.default(expand_20, [12, 9, 64], )
  expand_21 = aten.expand.default(transpose_10, [1, 12, 64, 9], )
  view_117 = aten.view.default(expand_21, [12, 64, 9], )
  bmm_10 = aten.bmm.default(view_116, view_117, )
  view_118 = aten.view.default(bmm_10, [1, 12, 9, 9], )
  div_5 = aten.div.Tensor(view_118, 8.0, )
  add_27 = aten.add.Tensor(div_5, mul, )
  _softmax_5 = aten._softmax.default(add_27, -1, False, )
  clone_19 = aten.clone.default(_softmax_5, )
  expand_22 = aten.expand.default(clone_19, [1, 12, 9, 9], )
  view_119 = aten.view.default(expand_22, [12, 9, 9], )
  expand_23 = aten.expand.default(permute_17, [1, 12, 9, 64], )
  view_120 = aten.view.default(expand_23, [12, 9, 64], )
  bmm_11 = aten.bmm.default(view_119, view_120, )
  view_121 = aten.view.default(bmm_11, [1, 12, 9, 64], )
  transpose_11 = aten.transpose.int(view_121, 2, 1, )
  clone_20 = aten.clone.default(transpose_11, memory_format = torch.contiguous_format)
  _unsafe_view_5 = aten._unsafe_view.default(clone_20, [1, 9, 768], )
  view_122 = aten.view.default(_unsafe_view_5, [9, 768], )
  t_34 = aten.t.default(arg13_1, )
  addmm_34 = aten.addmm.default(arg14_1, view_122, t_34, )
  view_123 = aten.view.default(addmm_34, [1, 9, 768], )
  clone_21 = aten.clone.default(view_123, )
  add_28 = aten.add.Tensor(getitem_30, clone_21, )
  native_layer_norm_11 = aten.native_layer_norm.default(add_28, [768], arg15_1, arg16_1, 1e-12, )
  getitem_33 = native_layer_norm_11[0]
  view_124 = aten.view.default(getitem_33, [9, 768], )
  t_35 = aten.t.default(arg17_1, )
  addmm_35 = aten.addmm.default(arg18_1, view_124, t_35, )
  view_125 = aten.view.default(addmm_35, [1, 9, 3072], )
  mul_21 = aten.mul.Tensor(view_125, 0.5, )
  pow_6 = aten.pow.Tensor_Scalar(view_125, 3.0, )
  mul_22 = aten.mul.Tensor(pow_6, 0.044715, )
  add_29 = aten.add.Tensor(view_125, mul_22, )
  mul_23 = aten.mul.Tensor(add_29, 0.7978845608028654, )
  tanh_5 = aten.tanh.default(mul_23, )
  add_30 = aten.add.Tensor(tanh_5, 1.0, )
  mul_24 = aten.mul.Tensor(mul_21, add_30, )
  view_126 = aten.view.default(mul_24, [9, 3072], )
  t_36 = aten.t.default(arg19_1, )
  addmm_36 = aten.addmm.default(arg20_1, view_126, t_36, )
  view_127 = aten.view.default(addmm_36, [1, 9, 768], )
  add_31 = aten.add.Tensor(view_127, getitem_33, )
  native_layer_norm_12 = aten.native_layer_norm.default(add_31, [768], arg21_1, arg22_1, 1e-12, )
  getitem_36 = native_layer_norm_12[0]
  view_128 = aten.view.default(getitem_36, [9, 768], )
  t_37 = aten.t.default(arg7_1, )
  addmm_37 = aten.addmm.default(arg8_1, view_128, t_37, )
  view_129 = aten.view.default(addmm_37, [1, 9, 768], )
  view_130 = aten.view.default(getitem_36, [9, 768], )
  t_38 = aten.t.default(arg9_1, )
  addmm_38 = aten.addmm.default(arg10_1, view_130, t_38, )
  view_131 = aten.view.default(addmm_38, [1, 9, 768], )
  view_132 = aten.view.default(getitem_36, [9, 768], )
  t_39 = aten.t.default(arg11_1, )
  addmm_39 = aten.addmm.default(arg12_1, view_132, t_39, )
  view_133 = aten.view.default(addmm_39, [1, 9, 768], )
  view_134 = aten.view.default(view_129, [1, 9, 12, 64], )
  permute_18 = aten.permute.default(view_134, [0, 2, 1, 3], )
  view_135 = aten.view.default(view_131, [1, 9, 12, 64], )
  permute_19 = aten.permute.default(view_135, [0, 2, 1, 3], )
  view_136 = aten.view.default(view_133, [1, 9, 12, 64], )
  permute_20 = aten.permute.default(view_136, [0, 2, 1, 3], )
  transpose_12 = aten.transpose.int(permute_19, -1, -2, )
  expand_24 = aten.expand.default(permute_18, [1, 12, 9, 64], )
  view_137 = aten.view.default(expand_24, [12, 9, 64], )
  expand_25 = aten.expand.default(transpose_12, [1, 12, 64, 9], )
  view_138 = aten.view.default(expand_25, [12, 64, 9], )
  bmm_12 = aten.bmm.default(view_137, view_138, )
  view_139 = aten.view.default(bmm_12, [1, 12, 9, 9], )
  div_6 = aten.div.Tensor(view_139, 8.0, )
  add_32 = aten.add.Tensor(div_6, mul, )
  _softmax_6 = aten._softmax.default(add_32, -1, False, )
  clone_22 = aten.clone.default(_softmax_6, )
  expand_26 = aten.expand.default(clone_22, [1, 12, 9, 9], )
  view_140 = aten.view.default(expand_26, [12, 9, 9], )
  expand_27 = aten.expand.default(permute_20, [1, 12, 9, 64], )
  view_141 = aten.view.default(expand_27, [12, 9, 64], )
  bmm_13 = aten.bmm.default(view_140, view_141, )
  view_142 = aten.view.default(bmm_13, [1, 12, 9, 64], )
  transpose_13 = aten.transpose.int(view_142, 2, 1, )
  clone_23 = aten.clone.default(transpose_13, memory_format = torch.contiguous_format)
  _unsafe_view_6 = aten._unsafe_view.default(clone_23, [1, 9, 768], )
  view_143 = aten.view.default(_unsafe_view_6, [9, 768], )
  t_40 = aten.t.default(arg13_1, )
  addmm_40 = aten.addmm.default(arg14_1, view_143, t_40, )
  view_144 = aten.view.default(addmm_40, [1, 9, 768], )
  clone_24 = aten.clone.default(view_144, )
  add_33 = aten.add.Tensor(getitem_36, clone_24, )
  native_layer_norm_13 = aten.native_layer_norm.default(add_33, [768], arg15_1, arg16_1, 1e-12, )
  getitem_39 = native_layer_norm_13[0]
  view_145 = aten.view.default(getitem_39, [9, 768], )
  t_41 = aten.t.default(arg17_1, )
  addmm_41 = aten.addmm.default(arg18_1, view_145, t_41, )
  view_146 = aten.view.default(addmm_41, [1, 9, 3072], )
  mul_25 = aten.mul.Tensor(view_146, 0.5, )
  pow_7 = aten.pow.Tensor_Scalar(view_146, 3.0, )
  mul_26 = aten.mul.Tensor(pow_7, 0.044715, )
  add_34 = aten.add.Tensor(view_146, mul_26, )
  mul_27 = aten.mul.Tensor(add_34, 0.7978845608028654, )
  tanh_6 = aten.tanh.default(mul_27, )
  add_35 = aten.add.Tensor(tanh_6, 1.0, )
  mul_28 = aten.mul.Tensor(mul_25, add_35, )
  view_147 = aten.view.default(mul_28, [9, 3072], )
  t_42 = aten.t.default(arg19_1, )
  addmm_42 = aten.addmm.default(arg20_1, view_147, t_42, )
  view_148 = aten.view.default(addmm_42, [1, 9, 768], )
  add_36 = aten.add.Tensor(view_148, getitem_39, )
  native_layer_norm_14 = aten.native_layer_norm.default(add_36, [768], arg21_1, arg22_1, 1e-12, )
  getitem_42 = native_layer_norm_14[0]
  view_149 = aten.view.default(getitem_42, [9, 768], )
  t_43 = aten.t.default(arg7_1, )
  addmm_43 = aten.addmm.default(arg8_1, view_149, t_43, )
  view_150 = aten.view.default(addmm_43, [1, 9, 768], )
  view_151 = aten.view.default(getitem_42, [9, 768], )
  t_44 = aten.t.default(arg9_1, )
  addmm_44 = aten.addmm.default(arg10_1, view_151, t_44, )
  view_152 = aten.view.default(addmm_44, [1, 9, 768], )
  view_153 = aten.view.default(getitem_42, [9, 768], )
  t_45 = aten.t.default(arg11_1, )
  addmm_45 = aten.addmm.default(arg12_1, view_153, t_45, )
  view_154 = aten.view.default(addmm_45, [1, 9, 768], )
  view_155 = aten.view.default(view_150, [1, 9, 12, 64], )
  permute_21 = aten.permute.default(view_155, [0, 2, 1, 3], )
  view_156 = aten.view.default(view_152, [1, 9, 12, 64], )
  permute_22 = aten.permute.default(view_156, [0, 2, 1, 3], )
  view_157 = aten.view.default(view_154, [1, 9, 12, 64], )
  permute_23 = aten.permute.default(view_157, [0, 2, 1, 3], )
  transpose_14 = aten.transpose.int(permute_22, -1, -2, )
  expand_28 = aten.expand.default(permute_21, [1, 12, 9, 64], )
  view_158 = aten.view.default(expand_28, [12, 9, 64], )
  expand_29 = aten.expand.default(transpose_14, [1, 12, 64, 9], )
  view_159 = aten.view.default(expand_29, [12, 64, 9], )
  bmm_14 = aten.bmm.default(view_158, view_159, )
  view_160 = aten.view.default(bmm_14, [1, 12, 9, 9], )
  div_7 = aten.div.Tensor(view_160, 8.0, )
  add_37 = aten.add.Tensor(div_7, mul, )
  _softmax_7 = aten._softmax.default(add_37, -1, False, )
  clone_25 = aten.clone.default(_softmax_7, )
  expand_30 = aten.expand.default(clone_25, [1, 12, 9, 9], )
  view_161 = aten.view.default(expand_30, [12, 9, 9], )
  expand_31 = aten.expand.default(permute_23, [1, 12, 9, 64], )
  view_162 = aten.view.default(expand_31, [12, 9, 64], )
  bmm_15 = aten.bmm.default(view_161, view_162, )
  view_163 = aten.view.default(bmm_15, [1, 12, 9, 64], )
  transpose_15 = aten.transpose.int(view_163, 2, 1, )
  clone_26 = aten.clone.default(transpose_15, memory_format = torch.contiguous_format)
  _unsafe_view_7 = aten._unsafe_view.default(clone_26, [1, 9, 768], )
  view_164 = aten.view.default(_unsafe_view_7, [9, 768], )
  t_46 = aten.t.default(arg13_1, )
  addmm_46 = aten.addmm.default(arg14_1, view_164, t_46, )
  view_165 = aten.view.default(addmm_46, [1, 9, 768], )
  clone_27 = aten.clone.default(view_165, )
  add_38 = aten.add.Tensor(getitem_42, clone_27, )
  native_layer_norm_15 = aten.native_layer_norm.default(add_38, [768], arg15_1, arg16_1, 1e-12, )
  getitem_45 = native_layer_norm_15[0]
  view_166 = aten.view.default(getitem_45, [9, 768], )
  t_47 = aten.t.default(arg17_1, )
  addmm_47 = aten.addmm.default(arg18_1, view_166, t_47, )
  view_167 = aten.view.default(addmm_47, [1, 9, 3072], )
  mul_29 = aten.mul.Tensor(view_167, 0.5, )
  pow_8 = aten.pow.Tensor_Scalar(view_167, 3.0, )
  mul_30 = aten.mul.Tensor(pow_8, 0.044715, )
  add_39 = aten.add.Tensor(view_167, mul_30, )
  mul_31 = aten.mul.Tensor(add_39, 0.7978845608028654, )
  tanh_7 = aten.tanh.default(mul_31, )
  add_40 = aten.add.Tensor(tanh_7, 1.0, )
  mul_32 = aten.mul.Tensor(mul_29, add_40, )
  view_168 = aten.view.default(mul_32, [9, 3072], )
  t_48 = aten.t.default(arg19_1, )
  addmm_48 = aten.addmm.default(arg20_1, view_168, t_48, )
  view_169 = aten.view.default(addmm_48, [1, 9, 768], )
  add_41 = aten.add.Tensor(view_169, getitem_45, )
  native_layer_norm_16 = aten.native_layer_norm.default(add_41, [768], arg21_1, arg22_1, 1e-12, )
  getitem_48 = native_layer_norm_16[0]
  view_170 = aten.view.default(getitem_48, [9, 768], )
  t_49 = aten.t.default(arg7_1, )
  addmm_49 = aten.addmm.default(arg8_1, view_170, t_49, )
  view_171 = aten.view.default(addmm_49, [1, 9, 768], )
  view_172 = aten.view.default(getitem_48, [9, 768], )
  t_50 = aten.t.default(arg9_1, )
  addmm_50 = aten.addmm.default(arg10_1, view_172, t_50, )
  view_173 = aten.view.default(addmm_50, [1, 9, 768], )
  view_174 = aten.view.default(getitem_48, [9, 768], )
  t_51 = aten.t.default(arg11_1, )
  addmm_51 = aten.addmm.default(arg12_1, view_174, t_51, )
  view_175 = aten.view.default(addmm_51, [1, 9, 768], )
  view_176 = aten.view.default(view_171, [1, 9, 12, 64], )
  permute_24 = aten.permute.default(view_176, [0, 2, 1, 3], )
  view_177 = aten.view.default(view_173, [1, 9, 12, 64], )
  permute_25 = aten.permute.default(view_177, [0, 2, 1, 3], )
  view_178 = aten.view.default(view_175, [1, 9, 12, 64], )
  permute_26 = aten.permute.default(view_178, [0, 2, 1, 3], )
  transpose_16 = aten.transpose.int(permute_25, -1, -2, )
  expand_32 = aten.expand.default(permute_24, [1, 12, 9, 64], )
  view_179 = aten.view.default(expand_32, [12, 9, 64], )
  expand_33 = aten.expand.default(transpose_16, [1, 12, 64, 9], )
  view_180 = aten.view.default(expand_33, [12, 64, 9], )
  bmm_16 = aten.bmm.default(view_179, view_180, )
  view_181 = aten.view.default(bmm_16, [1, 12, 9, 9], )
  div_8 = aten.div.Tensor(view_181, 8.0, )
  add_42 = aten.add.Tensor(div_8, mul, )
  _softmax_8 = aten._softmax.default(add_42, -1, False, )
  clone_28 = aten.clone.default(_softmax_8, )
  expand_34 = aten.expand.default(clone_28, [1, 12, 9, 9], )
  view_182 = aten.view.default(expand_34, [12, 9, 9], )
  expand_35 = aten.expand.default(permute_26, [1, 12, 9, 64], )
  view_183 = aten.view.default(expand_35, [12, 9, 64], )
  bmm_17 = aten.bmm.default(view_182, view_183, )
  view_184 = aten.view.default(bmm_17, [1, 12, 9, 64], )
  transpose_17 = aten.transpose.int(view_184, 2, 1, )
  clone_29 = aten.clone.default(transpose_17, memory_format = torch.contiguous_format)
  _unsafe_view_8 = aten._unsafe_view.default(clone_29, [1, 9, 768], )
  view_185 = aten.view.default(_unsafe_view_8, [9, 768], )
  t_52 = aten.t.default(arg13_1, )
  addmm_52 = aten.addmm.default(arg14_1, view_185, t_52, )
  view_186 = aten.view.default(addmm_52, [1, 9, 768], )
  clone_30 = aten.clone.default(view_186, )
  add_43 = aten.add.Tensor(getitem_48, clone_30, )
  native_layer_norm_17 = aten.native_layer_norm.default(add_43, [768], arg15_1, arg16_1, 1e-12, )
  getitem_51 = native_layer_norm_17[0]
  view_187 = aten.view.default(getitem_51, [9, 768], )
  t_53 = aten.t.default(arg17_1, )
  addmm_53 = aten.addmm.default(arg18_1, view_187, t_53, )
  view_188 = aten.view.default(addmm_53, [1, 9, 3072], )
  mul_33 = aten.mul.Tensor(view_188, 0.5, )
  pow_9 = aten.pow.Tensor_Scalar(view_188, 3.0, )
  mul_34 = aten.mul.Tensor(pow_9, 0.044715, )
  add_44 = aten.add.Tensor(view_188, mul_34, )
  mul_35 = aten.mul.Tensor(add_44, 0.7978845608028654, )
  tanh_8 = aten.tanh.default(mul_35, )
  add_45 = aten.add.Tensor(tanh_8, 1.0, )
  mul_36 = aten.mul.Tensor(mul_33, add_45, )
  view_189 = aten.view.default(mul_36, [9, 3072], )
  t_54 = aten.t.default(arg19_1, )
  addmm_54 = aten.addmm.default(arg20_1, view_189, t_54, )
  view_190 = aten.view.default(addmm_54, [1, 9, 768], )
  add_46 = aten.add.Tensor(view_190, getitem_51, )
  native_layer_norm_18 = aten.native_layer_norm.default(add_46, [768], arg21_1, arg22_1, 1e-12, )
  getitem_54 = native_layer_norm_18[0]
  view_191 = aten.view.default(getitem_54, [9, 768], )
  t_55 = aten.t.default(arg7_1, )
  addmm_55 = aten.addmm.default(arg8_1, view_191, t_55, )
  view_192 = aten.view.default(addmm_55, [1, 9, 768], )
  view_193 = aten.view.default(getitem_54, [9, 768], )
  t_56 = aten.t.default(arg9_1, )
  addmm_56 = aten.addmm.default(arg10_1, view_193, t_56, )
  view_194 = aten.view.default(addmm_56, [1, 9, 768], )
  view_195 = aten.view.default(getitem_54, [9, 768], )
  t_57 = aten.t.default(arg11_1, )
  addmm_57 = aten.addmm.default(arg12_1, view_195, t_57, )
  view_196 = aten.view.default(addmm_57, [1, 9, 768], )
  view_197 = aten.view.default(view_192, [1, 9, 12, 64], )
  permute_27 = aten.permute.default(view_197, [0, 2, 1, 3], )
  view_198 = aten.view.default(view_194, [1, 9, 12, 64], )
  permute_28 = aten.permute.default(view_198, [0, 2, 1, 3], )
  view_199 = aten.view.default(view_196, [1, 9, 12, 64], )
  permute_29 = aten.permute.default(view_199, [0, 2, 1, 3], )
  transpose_18 = aten.transpose.int(permute_28, -1, -2, )
  expand_36 = aten.expand.default(permute_27, [1, 12, 9, 64], )
  view_200 = aten.view.default(expand_36, [12, 9, 64], )
  expand_37 = aten.expand.default(transpose_18, [1, 12, 64, 9], )
  view_201 = aten.view.default(expand_37, [12, 64, 9], )
  bmm_18 = aten.bmm.default(view_200, view_201, )
  view_202 = aten.view.default(bmm_18, [1, 12, 9, 9], )
  div_9 = aten.div.Tensor(view_202, 8.0, )
  add_47 = aten.add.Tensor(div_9, mul, )
  _softmax_9 = aten._softmax.default(add_47, -1, False, )
  clone_31 = aten.clone.default(_softmax_9, )
  expand_38 = aten.expand.default(clone_31, [1, 12, 9, 9], )
  view_203 = aten.view.default(expand_38, [12, 9, 9], )
  expand_39 = aten.expand.default(permute_29, [1, 12, 9, 64], )
  view_204 = aten.view.default(expand_39, [12, 9, 64], )
  bmm_19 = aten.bmm.default(view_203, view_204, )
  view_205 = aten.view.default(bmm_19, [1, 12, 9, 64], )
  transpose_19 = aten.transpose.int(view_205, 2, 1, )
  clone_32 = aten.clone.default(transpose_19, memory_format = torch.contiguous_format)
  _unsafe_view_9 = aten._unsafe_view.default(clone_32, [1, 9, 768], )
  view_206 = aten.view.default(_unsafe_view_9, [9, 768], )
  t_58 = aten.t.default(arg13_1, )
  addmm_58 = aten.addmm.default(arg14_1, view_206, t_58, )
  view_207 = aten.view.default(addmm_58, [1, 9, 768], )
  clone_33 = aten.clone.default(view_207, )
  add_48 = aten.add.Tensor(getitem_54, clone_33, )
  native_layer_norm_19 = aten.native_layer_norm.default(add_48, [768], arg15_1, arg16_1, 1e-12, )
  getitem_57 = native_layer_norm_19[0]
  view_208 = aten.view.default(getitem_57, [9, 768], )
  t_59 = aten.t.default(arg17_1, )
  addmm_59 = aten.addmm.default(arg18_1, view_208, t_59, )
  view_209 = aten.view.default(addmm_59, [1, 9, 3072], )
  mul_37 = aten.mul.Tensor(view_209, 0.5, )
  pow_10 = aten.pow.Tensor_Scalar(view_209, 3.0, )
  mul_38 = aten.mul.Tensor(pow_10, 0.044715, )
  add_49 = aten.add.Tensor(view_209, mul_38, )
  mul_39 = aten.mul.Tensor(add_49, 0.7978845608028654, )
  tanh_9 = aten.tanh.default(mul_39, )
  add_50 = aten.add.Tensor(tanh_9, 1.0, )
  mul_40 = aten.mul.Tensor(mul_37, add_50, )
  view_210 = aten.view.default(mul_40, [9, 3072], )
  t_60 = aten.t.default(arg19_1, )
  addmm_60 = aten.addmm.default(arg20_1, view_210, t_60, )
  view_211 = aten.view.default(addmm_60, [1, 9, 768], )
  add_51 = aten.add.Tensor(view_211, getitem_57, )
  native_layer_norm_20 = aten.native_layer_norm.default(add_51, [768], arg21_1, arg22_1, 1e-12, )
  getitem_60 = native_layer_norm_20[0]
  view_212 = aten.view.default(getitem_60, [9, 768], )
  t_61 = aten.t.default(arg7_1, )
  addmm_61 = aten.addmm.default(arg8_1, view_212, t_61, )
  view_213 = aten.view.default(addmm_61, [1, 9, 768], )
  view_214 = aten.view.default(getitem_60, [9, 768], )
  t_62 = aten.t.default(arg9_1, )
  addmm_62 = aten.addmm.default(arg10_1, view_214, t_62, )
  view_215 = aten.view.default(addmm_62, [1, 9, 768], )
  view_216 = aten.view.default(getitem_60, [9, 768], )
  t_63 = aten.t.default(arg11_1, )
  addmm_63 = aten.addmm.default(arg12_1, view_216, t_63, )
  view_217 = aten.view.default(addmm_63, [1, 9, 768], )
  view_218 = aten.view.default(view_213, [1, 9, 12, 64], )
  permute_30 = aten.permute.default(view_218, [0, 2, 1, 3], )
  view_219 = aten.view.default(view_215, [1, 9, 12, 64], )
  permute_31 = aten.permute.default(view_219, [0, 2, 1, 3], )
  view_220 = aten.view.default(view_217, [1, 9, 12, 64], )
  permute_32 = aten.permute.default(view_220, [0, 2, 1, 3], )
  transpose_20 = aten.transpose.int(permute_31, -1, -2, )
  expand_40 = aten.expand.default(permute_30, [1, 12, 9, 64], )
  view_221 = aten.view.default(expand_40, [12, 9, 64], )
  expand_41 = aten.expand.default(transpose_20, [1, 12, 64, 9], )
  view_222 = aten.view.default(expand_41, [12, 64, 9], )
  bmm_20 = aten.bmm.default(view_221, view_222, )
  view_223 = aten.view.default(bmm_20, [1, 12, 9, 9], )
  div_10 = aten.div.Tensor(view_223, 8.0, )
  add_52 = aten.add.Tensor(div_10, mul, )
  _softmax_10 = aten._softmax.default(add_52, -1, False, )
  clone_34 = aten.clone.default(_softmax_10, )
  expand_42 = aten.expand.default(clone_34, [1, 12, 9, 9], )
  view_224 = aten.view.default(expand_42, [12, 9, 9], )
  expand_43 = aten.expand.default(permute_32, [1, 12, 9, 64], )
  view_225 = aten.view.default(expand_43, [12, 9, 64], )
  bmm_21 = aten.bmm.default(view_224, view_225, )
  view_226 = aten.view.default(bmm_21, [1, 12, 9, 64], )
  transpose_21 = aten.transpose.int(view_226, 2, 1, )
  clone_35 = aten.clone.default(transpose_21, memory_format = torch.contiguous_format)
  _unsafe_view_10 = aten._unsafe_view.default(clone_35, [1, 9, 768], )
  view_227 = aten.view.default(_unsafe_view_10, [9, 768], )
  t_64 = aten.t.default(arg13_1, )
  addmm_64 = aten.addmm.default(arg14_1, view_227, t_64, )
  view_228 = aten.view.default(addmm_64, [1, 9, 768], )
  clone_36 = aten.clone.default(view_228, )
  add_53 = aten.add.Tensor(getitem_60, clone_36, )
  native_layer_norm_21 = aten.native_layer_norm.default(add_53, [768], arg15_1, arg16_1, 1e-12, )
  getitem_63 = native_layer_norm_21[0]
  view_229 = aten.view.default(getitem_63, [9, 768], )
  t_65 = aten.t.default(arg17_1, )
  addmm_65 = aten.addmm.default(arg18_1, view_229, t_65, )
  view_230 = aten.view.default(addmm_65, [1, 9, 3072], )
  mul_41 = aten.mul.Tensor(view_230, 0.5, )
  pow_11 = aten.pow.Tensor_Scalar(view_230, 3.0, )
  mul_42 = aten.mul.Tensor(pow_11, 0.044715, )
  add_54 = aten.add.Tensor(view_230, mul_42, )
  mul_43 = aten.mul.Tensor(add_54, 0.7978845608028654, )
  tanh_10 = aten.tanh.default(mul_43, )
  add_55 = aten.add.Tensor(tanh_10, 1.0, )
  mul_44 = aten.mul.Tensor(mul_41, add_55, )
  view_231 = aten.view.default(mul_44, [9, 3072], )
  t_66 = aten.t.default(arg19_1, )
  addmm_66 = aten.addmm.default(arg20_1, view_231, t_66, )
  view_232 = aten.view.default(addmm_66, [1, 9, 768], )
  add_56 = aten.add.Tensor(view_232, getitem_63, )
  native_layer_norm_22 = aten.native_layer_norm.default(add_56, [768], arg21_1, arg22_1, 1e-12, )
  getitem_66 = native_layer_norm_22[0]
  view_233 = aten.view.default(getitem_66, [9, 768], )
  t_67 = aten.t.default(arg7_1, )
  addmm_67 = aten.addmm.default(arg8_1, view_233, t_67, )
  view_234 = aten.view.default(addmm_67, [1, 9, 768], )
  view_235 = aten.view.default(getitem_66, [9, 768], )
  t_68 = aten.t.default(arg9_1, )
  addmm_68 = aten.addmm.default(arg10_1, view_235, t_68, )
  view_236 = aten.view.default(addmm_68, [1, 9, 768], )
  view_237 = aten.view.default(getitem_66, [9, 768], )
  t_69 = aten.t.default(arg11_1, )
  addmm_69 = aten.addmm.default(arg12_1, view_237, t_69, )
  view_238 = aten.view.default(addmm_69, [1, 9, 768], )
  view_239 = aten.view.default(view_234, [1, 9, 12, 64], )
  permute_33 = aten.permute.default(view_239, [0, 2, 1, 3], )
  view_240 = aten.view.default(view_236, [1, 9, 12, 64], )
  permute_34 = aten.permute.default(view_240, [0, 2, 1, 3], )
  view_241 = aten.view.default(view_238, [1, 9, 12, 64], )
  permute_35 = aten.permute.default(view_241, [0, 2, 1, 3], )
  transpose_22 = aten.transpose.int(permute_34, -1, -2, )
  expand_44 = aten.expand.default(permute_33, [1, 12, 9, 64], )
  view_242 = aten.view.default(expand_44, [12, 9, 64], )
  expand_45 = aten.expand.default(transpose_22, [1, 12, 64, 9], )
  view_243 = aten.view.default(expand_45, [12, 64, 9], )
  bmm_22 = aten.bmm.default(view_242, view_243, )
  view_244 = aten.view.default(bmm_22, [1, 12, 9, 9], )
  div_11 = aten.div.Tensor(view_244, 8.0, )
  add_57 = aten.add.Tensor(div_11, mul, )
  _softmax_11 = aten._softmax.default(add_57, -1, False, )
  clone_37 = aten.clone.default(_softmax_11, )
  expand_46 = aten.expand.default(clone_37, [1, 12, 9, 9], )
  view_245 = aten.view.default(expand_46, [12, 9, 9], )
  expand_47 = aten.expand.default(permute_35, [1, 12, 9, 64], )
  view_246 = aten.view.default(expand_47, [12, 9, 64], )
  bmm_23 = aten.bmm.default(view_245, view_246, )
  view_247 = aten.view.default(bmm_23, [1, 12, 9, 64], )
  transpose_23 = aten.transpose.int(view_247, 2, 1, )
  clone_38 = aten.clone.default(transpose_23, memory_format = torch.contiguous_format)
  _unsafe_view_11 = aten._unsafe_view.default(clone_38, [1, 9, 768], )
  view_248 = aten.view.default(_unsafe_view_11, [9, 768], )
  t_70 = aten.t.default(arg13_1, )
  addmm_70 = aten.addmm.default(arg14_1, view_248, t_70, )
  view_249 = aten.view.default(addmm_70, [1, 9, 768], )
  clone_39 = aten.clone.default(view_249, )
  add_58 = aten.add.Tensor(getitem_66, clone_39, )
  native_layer_norm_23 = aten.native_layer_norm.default(add_58, [768], arg15_1, arg16_1, 1e-12, )
  getitem_69 = native_layer_norm_23[0]
  view_250 = aten.view.default(getitem_69, [9, 768], )
  t_71 = aten.t.default(arg17_1, )
  addmm_71 = aten.addmm.default(arg18_1, view_250, t_71, )
  view_251 = aten.view.default(addmm_71, [1, 9, 3072], )
  mul_45 = aten.mul.Tensor(view_251, 0.5, )
  pow_12 = aten.pow.Tensor_Scalar(view_251, 3.0, )
  mul_46 = aten.mul.Tensor(pow_12, 0.044715, )
  add_59 = aten.add.Tensor(view_251, mul_46, )
  mul_47 = aten.mul.Tensor(add_59, 0.7978845608028654, )
  tanh_11 = aten.tanh.default(mul_47, )
  add_60 = aten.add.Tensor(tanh_11, 1.0, )
  mul_48 = aten.mul.Tensor(mul_45, add_60, )
  view_252 = aten.view.default(mul_48, [9, 3072], )
  t_72 = aten.t.default(arg19_1, )
  addmm_72 = aten.addmm.default(arg20_1, view_252, t_72, )
  view_253 = aten.view.default(addmm_72, [1, 9, 768], )
  add_61 = aten.add.Tensor(view_253, getitem_69, )
  native_layer_norm_24 = aten.native_layer_norm.default(add_61, [768], arg21_1, arg22_1, 1e-12, )
  getitem_72 = native_layer_norm_24[0]
  view_254 = aten.view.default(getitem_72, [9, 768], )
  t_73 = aten.t.default(arg23_1, )
  addmm_73 = aten.addmm.default(arg24_1, view_254, t_73, )
  view_255 = aten.view.default(addmm_73, [1, 9, 128], )
  mul_49 = aten.mul.Tensor(view_255, 0.5, )
  pow_13 = aten.pow.Tensor_Scalar(view_255, 3.0, )
  mul_50 = aten.mul.Tensor(pow_13, 0.044715, )
  add_62 = aten.add.Tensor(view_255, mul_50, )
  mul_51 = aten.mul.Tensor(add_62, 0.7978845608028654, )
  tanh_12 = aten.tanh.default(mul_51, )
  add_63 = aten.add.Tensor(tanh_12, 1.0, )
  mul_52 = aten.mul.Tensor(mul_49, add_63, )
  native_layer_norm_25 = aten.native_layer_norm.default(mul_52, [128], arg25_1, arg26_1, 1e-12, )
  getitem_75 = native_layer_norm_25[0]
  view_256 = aten.view.default(getitem_75, [9, 128], )
  t_74 = aten.t.default(arg27_1, )
  addmm_74 = aten.addmm.default(arg28_1, view_256, t_74, )
  view_257 = aten.view.default(addmm_74, [1, 9, 30000], )
  # return (view_257,)
  ttnn_from_torch = ttnn.from_torch(arg31_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_reshape = ttnn.reshape(ttnn_from_torch, (1, 1, 9), )
  test_accuracy(unsqueeze, ttnn_reshape)
  ttnn_from_device = ttnn.from_device(ttnn_reshape, )
  ttnn_to_layout_3 = ttnn.to_layout(ttnn_from_device, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_1 = ttnn.reshape(ttnn_to_layout_3, (1, 1, 1, 9), )
  test_accuracy(unsqueeze_1, ttnn_reshape_1)
  ttnn_from_device_1 = ttnn.from_device(ttnn_reshape_1, )
  ttnn_to_layout_4 = ttnn.to_layout(ttnn_from_device_1, ttnn.TILE_LAYOUT, )
  ttnn_to_device = ttnn.to_device(ttnn_to_layout_4, device = device)
  ttnn_typecast = ttnn.typecast(ttnn_to_device, ttnn.bfloat16, )
  test_accuracy(_to_copy, ttnn_typecast)
  ttnn_rsub = ttnn.rsub(ttnn_typecast, 1.0, )
  test_accuracy(rsub, ttnn_rsub)
  ttnn_multiply = ttnn.multiply(ttnn_rsub, -3.3895313892515355e+38, )
  test_accuracy(mul, ttnn_multiply)
  ttnn_from_torch_1 = ttnn.from_torch(arg29_1, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_slice = ttnn.slice(ttnn_from_torch_1, [0, 0], [1, 9], )
  test_accuracy(slice_2, ttnn_slice)
  ttnn_from_torch_2 = ttnn.from_torch(arg30_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_from_torch_3 = ttnn.from_torch(arg0_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_embedding = ttnn.embedding(ttnn_from_torch_2, ttnn_from_torch_3, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout = ttnn.to_layout(ttnn_embedding, ttnn.TILE_LAYOUT, )
  ttnn_from_torch_4 = ttnn.from_torch(arg32_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.uint32)
  ttnn_from_torch_5 = ttnn.from_torch(arg1_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_embedding_1 = ttnn.embedding(ttnn_from_torch_4, ttnn_from_torch_5, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout_1 = ttnn.to_layout(ttnn_embedding_1, ttnn.TILE_LAYOUT, )
  ttnn_add_75 = ttnn.add(ttnn_to_layout, ttnn_to_layout_1, )
  ttnn_from_device_2 = ttnn.from_device(ttnn_slice, )
  ttnn_to_layout_5 = ttnn.to_layout(ttnn_from_device_2, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_to_device_1 = ttnn.to_device(ttnn_to_layout_5, device = device)
  ttnn_from_torch_6 = ttnn.from_torch(arg2_1, device = device, layout = ttnn.ROW_MAJOR_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_embedding_2 = ttnn.embedding(ttnn_to_device_1, ttnn_from_torch_6, layout = ttnn.ROW_MAJOR_LAYOUT)
  ttnn_to_layout_2 = ttnn.to_layout(ttnn_embedding_2, ttnn.TILE_LAYOUT, )
  test_accuracy(embedding_2, ttnn_to_layout_2)
  ttnn_add_76 = ttnn.add(ttnn_add_75, ttnn_to_layout_2, )
  ttnn_from_torch_7 = ttnn.from_torch(arg3_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_8 = ttnn.from_torch(arg4_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_ = ttnn.layer_norm(ttnn_add_76, epsilon = 1e-12, weight = ttnn_from_torch_7, bias = ttnn_from_torch_8)
  ttnn_layer_norm_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_76), [128], ttnn.to_torch(ttnn_from_torch_7), 
                                                                ttnn.to_torch(ttnn_from_torch_8), 1e-12, )[0]
  ttnn_layer_norm = ttnn.from_torch(ttnn_layer_norm_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_), ttnn_layer_norm))
  ttnn_prefix_clone = clone_wrapper(ttnn_layer_norm, )
  test_accuracy(clone_3, ttnn_prefix_clone)
  ttnn_from_device_3 = ttnn.from_device(ttnn_prefix_clone, )
  ttnn_to_layout_6 = ttnn.to_layout(ttnn_from_device_3, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_2 = ttnn.reshape(ttnn_to_layout_6, (9, 128), )
  ttnn_from_torch_9 = ttnn.from_torch(arg5_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose = ttnn.transpose(ttnn_from_torch_9, 0, 1, )
  test_accuracy(t, ttnn_transpose)
  ttnn_from_device_4 = ttnn.from_device(ttnn_reshape_2, )
  ttnn_to_layout_7 = ttnn.to_layout(ttnn_from_device_4, ttnn.TILE_LAYOUT, )
  ttnn_to_device_2 = ttnn.to_device(ttnn_to_layout_7, device = device)
  ttnn_matmul = ttnn.matmul(ttnn_to_device_2, ttnn_transpose, )
  ttnn_from_torch_10 = ttnn.from_torch(arg6_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add = ttnn.add(ttnn_from_torch_10, ttnn_matmul, )
  ttnn_from_device_5 = ttnn.from_device(ttnn_add, )
  ttnn_to_layout_8 = ttnn.to_layout(ttnn_from_device_5, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_3 = ttnn.reshape(ttnn_to_layout_8, (1, 9, 768), )
  ttnn_from_device_6 = ttnn.from_device(ttnn_reshape_3, )
  ttnn_to_layout_9 = ttnn.to_layout(ttnn_from_device_6, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_4 = ttnn.reshape(ttnn_to_layout_9, (9, 768), )
  ttnn_from_torch_11 = ttnn.from_torch(arg7_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_1 = ttnn.transpose(ttnn_from_torch_11, 0, 1, )
  ttnn_from_device_7 = ttnn.from_device(ttnn_reshape_4, )
  ttnn_to_layout_10 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_3 = ttnn.to_device(ttnn_to_layout_10, device = device)
  ttnn_matmul_1 = ttnn.matmul(ttnn_to_device_3, ttnn_transpose_1, )
  ttnn_from_torch_12 = ttnn.from_torch(arg8_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_1 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_1, )
  ttnn_from_device_8 = ttnn.from_device(ttnn_add_1, )
  ttnn_to_layout_11 = ttnn.to_layout(ttnn_from_device_8, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_5 = ttnn.reshape(ttnn_to_layout_11, (1, 9, 768), )
  ttnn_from_torch_13 = ttnn.from_torch(arg9_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_2 = ttnn.transpose(ttnn_from_torch_13, 0, 1, )
  ttnn_to_layout_12 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_4 = ttnn.to_device(ttnn_to_layout_12, device = device)
  ttnn_matmul_2 = ttnn.matmul(ttnn_to_device_4, ttnn_transpose_2, )
  ttnn_from_torch_14 = ttnn.from_torch(arg10_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_2 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_2, )
  ttnn_from_device_10 = ttnn.from_device(ttnn_add_2, )
  ttnn_to_layout_13 = ttnn.to_layout(ttnn_from_device_10, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_7 = ttnn.reshape(ttnn_to_layout_13, (1, 9, 768), )
  ttnn_from_torch_15 = ttnn.from_torch(arg11_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_3 = ttnn.transpose(ttnn_from_torch_15, 0, 1, )
  ttnn_to_layout_14 = ttnn.to_layout(ttnn_from_device_7, ttnn.TILE_LAYOUT, )
  ttnn_to_device_5 = ttnn.to_device(ttnn_to_layout_14, device = device)
  ttnn_matmul_3 = ttnn.matmul(ttnn_to_device_5, ttnn_transpose_3, )
  ttnn_from_torch_16 = ttnn.from_torch(arg12_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_3 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_3, )
  ttnn_from_device_12 = ttnn.from_device(ttnn_add_3, )
  ttnn_to_layout_15 = ttnn.to_layout(ttnn_from_device_12, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_9 = ttnn.reshape(ttnn_to_layout_15, (1, 9, 768), )
  ttnn_from_device_13 = ttnn.from_device(ttnn_reshape_5, )
  ttnn_to_layout_16 = ttnn.to_layout(ttnn_from_device_13, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_10 = ttnn.reshape(ttnn_to_layout_16, (1, 9, 12, 64), )
  ttnn_from_device_14 = ttnn.from_device(ttnn_reshape_10, )
  ttnn_to_layout_17 = ttnn.to_layout(ttnn_from_device_14, ttnn.TILE_LAYOUT, )
  ttnn_to_device_6 = ttnn.to_device(ttnn_to_layout_17, device = device)
  ttnn_permute = ttnn.permute(ttnn_to_device_6, (0, 2, 1, 3), )
  ttnn_from_device_15 = ttnn.from_device(ttnn_reshape_7, )
  ttnn_to_layout_18 = ttnn.to_layout(ttnn_from_device_15, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_11 = ttnn.reshape(ttnn_to_layout_18, (1, 9, 12, 64), )
  ttnn_from_device_16 = ttnn.from_device(ttnn_reshape_11, )
  ttnn_to_layout_19 = ttnn.to_layout(ttnn_from_device_16, ttnn.TILE_LAYOUT, )
  ttnn_to_device_7 = ttnn.to_device(ttnn_to_layout_19, device = device)
  ttnn_permute_1 = ttnn.permute(ttnn_to_device_7, (0, 2, 1, 3), )
  ttnn_from_device_17 = ttnn.from_device(ttnn_reshape_9, )
  ttnn_to_layout_20 = ttnn.to_layout(ttnn_from_device_17, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_12 = ttnn.reshape(ttnn_to_layout_20, (1, 9, 12, 64), )
  ttnn_from_device_18 = ttnn.from_device(ttnn_reshape_12, )
  ttnn_to_layout_21 = ttnn.to_layout(ttnn_from_device_18, ttnn.TILE_LAYOUT, )
  ttnn_to_device_8 = ttnn.to_device(ttnn_to_layout_21, device = device)
  ttnn_permute_2 = ttnn.permute(ttnn_to_device_8, (0, 2, 1, 3), )
  ttnn_transpose_4 = ttnn.transpose(ttnn_permute_1, 3, 2, )
  ttnn_from_device_19 = ttnn.from_device(ttnn_permute, )
  ttnn_to_layout_22 = ttnn.to_layout(ttnn_from_device_19, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_13 = ttnn.reshape(ttnn_to_layout_22, (12, 9, 64), )
  ttnn_from_device_20 = ttnn.from_device(ttnn_transpose_4, )
  ttnn_to_layout_23 = ttnn.to_layout(ttnn_from_device_20, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_14 = ttnn.reshape(ttnn_to_layout_23, (12, 64, 9), )
  ttnn_from_device_21 = ttnn.from_device(ttnn_reshape_13, )
  ttnn_to_layout_24 = ttnn.to_layout(ttnn_from_device_21, ttnn.TILE_LAYOUT, )
  ttnn_to_device_9 = ttnn.to_device(ttnn_to_layout_24, device = device)
  ttnn_from_device_22 = ttnn.from_device(ttnn_reshape_14, )
  ttnn_to_layout_25 = ttnn.to_layout(ttnn_from_device_22, ttnn.TILE_LAYOUT, )
  ttnn_to_device_10 = ttnn.to_device(ttnn_to_layout_25, device = device)
  ttnn_matmul_4 = ttnn.matmul(ttnn_to_device_9, ttnn_to_device_10, )
  ttnn_from_device_23 = ttnn.from_device(ttnn_matmul_4, )
  ttnn_to_layout_26 = ttnn.to_layout(ttnn_from_device_23, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_15 = ttnn.reshape(ttnn_to_layout_26, (1, 12, 9, 9), )
  ttnn_from_device_24 = ttnn.from_device(ttnn_reshape_15, )
  ttnn_to_layout_27 = ttnn.to_layout(ttnn_from_device_24, ttnn.TILE_LAYOUT, )
  ttnn_to_device_11 = ttnn.to_device(ttnn_to_layout_27, device = device)
  ttnn_multiply_1 = ttnn.multiply(ttnn_to_device_11, 0.125, )
  ttnn_add_77 = ttnn.add(ttnn_multiply_1, ttnn_multiply, )
  ttnn_softmax = ttnn.softmax(ttnn_add_77, -1, numeric_stable = True)
  test_accuracy(_softmax, ttnn_softmax)
  ttnn_prefix_clone_1 = clone_wrapper(ttnn_softmax, )
  ttnn_from_device_25 = ttnn.from_device(ttnn_prefix_clone_1, )
  ttnn_to_layout_28 = ttnn.to_layout(ttnn_from_device_25, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_16 = ttnn.reshape(ttnn_to_layout_28, (12, 9, 9), )
  ttnn_from_device_26 = ttnn.from_device(ttnn_permute_2, )
  ttnn_to_layout_29 = ttnn.to_layout(ttnn_from_device_26, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_17 = ttnn.reshape(ttnn_to_layout_29, (12, 9, 64), )
  ttnn_from_device_27 = ttnn.from_device(ttnn_reshape_16, )
  ttnn_to_layout_30 = ttnn.to_layout(ttnn_from_device_27, ttnn.TILE_LAYOUT, )
  ttnn_to_device_12 = ttnn.to_device(ttnn_to_layout_30, device = device)
  ttnn_from_device_28 = ttnn.from_device(ttnn_reshape_17, )
  ttnn_to_layout_31 = ttnn.to_layout(ttnn_from_device_28, ttnn.TILE_LAYOUT, )
  ttnn_to_device_13 = ttnn.to_device(ttnn_to_layout_31, device = device)
  ttnn_matmul_5 = ttnn.matmul(ttnn_to_device_12, ttnn_to_device_13, )
  ttnn_from_device_29 = ttnn.from_device(ttnn_matmul_5, )
  ttnn_to_layout_32 = ttnn.to_layout(ttnn_from_device_29, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_18 = ttnn.reshape(ttnn_to_layout_32, (1, 12, 9, 64), )
  ttnn_from_device_30 = ttnn.from_device(ttnn_reshape_18, )
  ttnn_to_layout_33 = ttnn.to_layout(ttnn_from_device_30, ttnn.TILE_LAYOUT, )
  ttnn_to_device_14 = ttnn.to_device(ttnn_to_layout_33, device = device)
  ttnn_transpose_5 = ttnn.transpose(ttnn_to_device_14, 2, 1, )
  ttnn_prefix_clone_2 = clone_wrapper(ttnn_transpose_5, )
  ttnn_from_device_31 = ttnn.from_device(ttnn_prefix_clone_2, )
  ttnn_to_layout_34 = ttnn.to_layout(ttnn_from_device_31, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_19 = ttnn.reshape(ttnn_to_layout_34, (1, 9, 768), )
  ttnn_from_device_32 = ttnn.from_device(ttnn_reshape_19, )
  ttnn_to_layout_35 = ttnn.to_layout(ttnn_from_device_32, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_20 = ttnn.reshape(ttnn_to_layout_35, (9, 768), )
  ttnn_from_torch_17 = ttnn.from_torch(arg13_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_6 = ttnn.transpose(ttnn_from_torch_17, 0, 1, )
  test_accuracy(t_70, ttnn_transpose_6)
  ttnn_from_device_33 = ttnn.from_device(ttnn_reshape_20, )
  ttnn_to_layout_36 = ttnn.to_layout(ttnn_from_device_33, ttnn.TILE_LAYOUT, )
  ttnn_to_device_15 = ttnn.to_device(ttnn_to_layout_36, device = device)
  ttnn_matmul_6 = ttnn.matmul(ttnn_to_device_15, ttnn_transpose_6, )
  ttnn_from_torch_18 = ttnn.from_torch(arg14_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_4 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_6, )
  ttnn_from_device_34 = ttnn.from_device(ttnn_add_4, )
  ttnn_to_layout_37 = ttnn.to_layout(ttnn_from_device_34, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_21 = ttnn.reshape(ttnn_to_layout_37, (1, 9, 768), )
  ttnn_from_device_35 = ttnn.from_device(ttnn_reshape_21, )
  ttnn_to_layout_38 = ttnn.to_layout(ttnn_from_device_35, ttnn.TILE_LAYOUT, )
  ttnn_to_device_16 = ttnn.to_device(ttnn_to_layout_38, device = device)
  ttnn_prefix_clone_3 = clone_wrapper(ttnn_to_device_16, )
  ttnn_to_layout_39 = ttnn.to_layout(ttnn_from_device_6, ttnn.TILE_LAYOUT, )
  ttnn_to_device_17 = ttnn.to_device(ttnn_to_layout_39, device = device)
  ttnn_add_78 = ttnn.add(ttnn_to_device_17, ttnn_prefix_clone_3, )
  ttnn_from_torch_19 = ttnn.from_torch(arg15_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_20 = ttnn.from_torch(arg16_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_1_ = ttnn.layer_norm(ttnn_add_78, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_1_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_78), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_1 = ttnn.from_torch(ttnn_layer_norm_1_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_1_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_1_), ttnn_layer_norm_1))
  ttnn_from_device_37 = ttnn.from_device(ttnn_layer_norm_1, )
  ttnn_to_layout_40 = ttnn.to_layout(ttnn_from_device_37, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_22 = ttnn.reshape(ttnn_to_layout_40, (9, 768), )
  ttnn_from_torch_21 = ttnn.from_torch(arg17_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_7 = ttnn.transpose(ttnn_from_torch_21, 0, 1, )
  test_accuracy(t_71, ttnn_transpose_7)
  ttnn_from_device_38 = ttnn.from_device(ttnn_reshape_22, )
  ttnn_to_layout_41 = ttnn.to_layout(ttnn_from_device_38, ttnn.TILE_LAYOUT, )
  ttnn_to_device_18 = ttnn.to_device(ttnn_to_layout_41, device = device)
  ttnn_matmul_7 = ttnn.matmul(ttnn_to_device_18, ttnn_transpose_7, )
  ttnn_from_torch_22 = ttnn.from_torch(arg18_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_5 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_7, )
  ttnn_from_device_39 = ttnn.from_device(ttnn_add_5, )
  ttnn_to_layout_42 = ttnn.to_layout(ttnn_from_device_39, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_23 = ttnn.reshape(ttnn_to_layout_42, (1, 9, 3072), )
  ttnn_from_device_40 = ttnn.from_device(ttnn_reshape_23, )
  ttnn_to_layout_43 = ttnn.to_layout(ttnn_from_device_40, ttnn.TILE_LAYOUT, )
  ttnn_to_device_19 = ttnn.to_device(ttnn_to_layout_43, device = device)
  ttnn_multiply_2 = ttnn.multiply(ttnn_to_device_19, 0.5, )
  ttnn_pow = ttnn.pow(ttnn_to_device_19, 3.0, )
  ttnn_multiply_3 = ttnn.multiply(ttnn_pow, 0.044715, )
  ttnn_add_79 = ttnn.add(ttnn_to_device_19, ttnn_multiply_3, )
  ttnn_multiply_4 = ttnn.multiply(ttnn_add_79, 0.7978845608028654, )
  ttnn_tanh_ = ttnn.tanh(ttnn_multiply_4, )
  ttnn_tanh_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_4), )
  ttnn_tanh = ttnn.from_torch(ttnn_tanh_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_), ttnn_tanh))
  test_accuracy(tanh, ttnn_tanh)
  ttnn_add_80 = ttnn.add(ttnn_tanh, 1.0, )
  ttnn_multiply_5 = ttnn.multiply(ttnn_multiply_2, ttnn_add_80, )
  ttnn_from_device_41 = ttnn.from_device(ttnn_multiply_5, )
  ttnn_to_layout_44 = ttnn.to_layout(ttnn_from_device_41, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_24 = ttnn.reshape(ttnn_to_layout_44, (9, 3072), )
  ttnn_from_torch_23 = ttnn.from_torch(arg19_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_8 = ttnn.transpose(ttnn_from_torch_23, 0, 1, )
  test_accuracy(t_72, ttnn_transpose_8)
  ttnn_from_device_42 = ttnn.from_device(ttnn_reshape_24, )
  ttnn_to_layout_45 = ttnn.to_layout(ttnn_from_device_42, ttnn.TILE_LAYOUT, )
  ttnn_to_device_20 = ttnn.to_device(ttnn_to_layout_45, device = device)
  ttnn_matmul_8 = ttnn.matmul(ttnn_to_device_20, ttnn_transpose_8, )
  ttnn_from_torch_24 = ttnn.from_torch(arg20_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_6 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_8, )
  ttnn_from_device_43 = ttnn.from_device(ttnn_add_6, )
  ttnn_to_layout_46 = ttnn.to_layout(ttnn_from_device_43, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_25 = ttnn.reshape(ttnn_to_layout_46, (1, 9, 768), )
  ttnn_from_device_44 = ttnn.from_device(ttnn_reshape_25, )
  ttnn_to_layout_47 = ttnn.to_layout(ttnn_from_device_44, ttnn.TILE_LAYOUT, )
  ttnn_to_device_21 = ttnn.to_device(ttnn_to_layout_47, device = device)
  ttnn_add_81 = ttnn.add(ttnn_to_device_21, ttnn_layer_norm_1, )
  ttnn_from_torch_25 = ttnn.from_torch(arg21_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_26 = ttnn.from_torch(arg22_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_2_ = ttnn.layer_norm(ttnn_add_81, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_2_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_81), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_2 = ttnn.from_torch(ttnn_layer_norm_2_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_2_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_2_), ttnn_layer_norm_2))
  ttnn_from_device_45 = ttnn.from_device(ttnn_layer_norm_2, )
  ttnn_to_layout_48 = ttnn.to_layout(ttnn_from_device_45, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_26 = ttnn.reshape(ttnn_to_layout_48, (9, 768), )
  ttnn_from_device_46 = ttnn.from_device(ttnn_reshape_26, )
  ttnn_to_layout_49 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_22 = ttnn.to_device(ttnn_to_layout_49, device = device)
  ttnn_matmul_9 = ttnn.matmul(ttnn_to_device_22, ttnn_transpose_1, )
  ttnn_add_7 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_9, )
  ttnn_from_device_47 = ttnn.from_device(ttnn_add_7, )
  ttnn_to_layout_50 = ttnn.to_layout(ttnn_from_device_47, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_27 = ttnn.reshape(ttnn_to_layout_50, (1, 9, 768), )
  ttnn_to_layout_51 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_23 = ttnn.to_device(ttnn_to_layout_51, device = device)
  ttnn_matmul_10 = ttnn.matmul(ttnn_to_device_23, ttnn_transpose_2, )
  ttnn_add_8 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_10, )
  ttnn_from_device_49 = ttnn.from_device(ttnn_add_8, )
  ttnn_to_layout_52 = ttnn.to_layout(ttnn_from_device_49, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_29 = ttnn.reshape(ttnn_to_layout_52, (1, 9, 768), )
  ttnn_to_layout_53 = ttnn.to_layout(ttnn_from_device_46, ttnn.TILE_LAYOUT, )
  ttnn_to_device_24 = ttnn.to_device(ttnn_to_layout_53, device = device)
  ttnn_matmul_11 = ttnn.matmul(ttnn_to_device_24, ttnn_transpose_3, )
  ttnn_add_9 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_11, )
  ttnn_from_device_51 = ttnn.from_device(ttnn_add_9, )
  ttnn_to_layout_54 = ttnn.to_layout(ttnn_from_device_51, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_31 = ttnn.reshape(ttnn_to_layout_54, (1, 9, 768), )
  ttnn_from_device_52 = ttnn.from_device(ttnn_reshape_27, )
  ttnn_to_layout_55 = ttnn.to_layout(ttnn_from_device_52, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_32 = ttnn.reshape(ttnn_to_layout_55, (1, 9, 12, 64), )
  ttnn_from_device_53 = ttnn.from_device(ttnn_reshape_32, )
  ttnn_to_layout_56 = ttnn.to_layout(ttnn_from_device_53, ttnn.TILE_LAYOUT, )
  ttnn_to_device_25 = ttnn.to_device(ttnn_to_layout_56, device = device)
  ttnn_permute_3 = ttnn.permute(ttnn_to_device_25, (0, 2, 1, 3), )
  ttnn_from_device_54 = ttnn.from_device(ttnn_reshape_29, )
  ttnn_to_layout_57 = ttnn.to_layout(ttnn_from_device_54, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_33 = ttnn.reshape(ttnn_to_layout_57, (1, 9, 12, 64), )
  ttnn_from_device_55 = ttnn.from_device(ttnn_reshape_33, )
  ttnn_to_layout_58 = ttnn.to_layout(ttnn_from_device_55, ttnn.TILE_LAYOUT, )
  ttnn_to_device_26 = ttnn.to_device(ttnn_to_layout_58, device = device)
  ttnn_permute_4 = ttnn.permute(ttnn_to_device_26, (0, 2, 1, 3), )
  ttnn_from_device_56 = ttnn.from_device(ttnn_reshape_31, )
  ttnn_to_layout_59 = ttnn.to_layout(ttnn_from_device_56, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_34 = ttnn.reshape(ttnn_to_layout_59, (1, 9, 12, 64), )
  ttnn_from_device_57 = ttnn.from_device(ttnn_reshape_34, )
  ttnn_to_layout_60 = ttnn.to_layout(ttnn_from_device_57, ttnn.TILE_LAYOUT, )
  ttnn_to_device_27 = ttnn.to_device(ttnn_to_layout_60, device = device)
  ttnn_permute_5 = ttnn.permute(ttnn_to_device_27, (0, 2, 1, 3), )
  ttnn_transpose_12 = ttnn.transpose(ttnn_permute_4, 3, 2, )
  ttnn_from_device_58 = ttnn.from_device(ttnn_permute_3, )
  ttnn_to_layout_61 = ttnn.to_layout(ttnn_from_device_58, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_35 = ttnn.reshape(ttnn_to_layout_61, (12, 9, 64), )
  ttnn_from_device_59 = ttnn.from_device(ttnn_transpose_12, )
  ttnn_to_layout_62 = ttnn.to_layout(ttnn_from_device_59, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_36 = ttnn.reshape(ttnn_to_layout_62, (12, 64, 9), )
  ttnn_from_device_60 = ttnn.from_device(ttnn_reshape_35, )
  ttnn_to_layout_63 = ttnn.to_layout(ttnn_from_device_60, ttnn.TILE_LAYOUT, )
  ttnn_to_device_28 = ttnn.to_device(ttnn_to_layout_63, device = device)
  ttnn_from_device_61 = ttnn.from_device(ttnn_reshape_36, )
  ttnn_to_layout_64 = ttnn.to_layout(ttnn_from_device_61, ttnn.TILE_LAYOUT, )
  ttnn_to_device_29 = ttnn.to_device(ttnn_to_layout_64, device = device)
  ttnn_matmul_12 = ttnn.matmul(ttnn_to_device_28, ttnn_to_device_29, )
  ttnn_from_device_62 = ttnn.from_device(ttnn_matmul_12, )
  ttnn_to_layout_65 = ttnn.to_layout(ttnn_from_device_62, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_37 = ttnn.reshape(ttnn_to_layout_65, (1, 12, 9, 9), )
  ttnn_from_device_63 = ttnn.from_device(ttnn_reshape_37, )
  ttnn_to_layout_66 = ttnn.to_layout(ttnn_from_device_63, ttnn.TILE_LAYOUT, )
  ttnn_to_device_30 = ttnn.to_device(ttnn_to_layout_66, device = device)
  ttnn_multiply_6 = ttnn.multiply(ttnn_to_device_30, 0.125, )
  ttnn_add_82 = ttnn.add(ttnn_multiply_6, ttnn_multiply, )
  ttnn_softmax_1 = ttnn.softmax(ttnn_add_82, -1, numeric_stable = True)
  test_accuracy(_softmax_1, ttnn_softmax_1)
  ttnn_prefix_clone_4 = clone_wrapper(ttnn_softmax_1, )
  ttnn_from_device_64 = ttnn.from_device(ttnn_prefix_clone_4, )
  ttnn_to_layout_67 = ttnn.to_layout(ttnn_from_device_64, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_38 = ttnn.reshape(ttnn_to_layout_67, (12, 9, 9), )
  ttnn_from_device_65 = ttnn.from_device(ttnn_permute_5, )
  ttnn_to_layout_68 = ttnn.to_layout(ttnn_from_device_65, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_39 = ttnn.reshape(ttnn_to_layout_68, (12, 9, 64), )
  ttnn_from_device_66 = ttnn.from_device(ttnn_reshape_38, )
  ttnn_to_layout_69 = ttnn.to_layout(ttnn_from_device_66, ttnn.TILE_LAYOUT, )
  ttnn_to_device_31 = ttnn.to_device(ttnn_to_layout_69, device = device)
  ttnn_from_device_67 = ttnn.from_device(ttnn_reshape_39, )
  ttnn_to_layout_70 = ttnn.to_layout(ttnn_from_device_67, ttnn.TILE_LAYOUT, )
  ttnn_to_device_32 = ttnn.to_device(ttnn_to_layout_70, device = device)
  ttnn_matmul_13 = ttnn.matmul(ttnn_to_device_31, ttnn_to_device_32, )
  ttnn_from_device_68 = ttnn.from_device(ttnn_matmul_13, )
  ttnn_to_layout_71 = ttnn.to_layout(ttnn_from_device_68, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_40 = ttnn.reshape(ttnn_to_layout_71, (1, 12, 9, 64), )
  ttnn_from_device_69 = ttnn.from_device(ttnn_reshape_40, )
  ttnn_to_layout_72 = ttnn.to_layout(ttnn_from_device_69, ttnn.TILE_LAYOUT, )
  ttnn_to_device_33 = ttnn.to_device(ttnn_to_layout_72, device = device)
  ttnn_transpose_13 = ttnn.transpose(ttnn_to_device_33, 2, 1, )
  ttnn_prefix_clone_5 = clone_wrapper(ttnn_transpose_13, )
  ttnn_from_device_70 = ttnn.from_device(ttnn_prefix_clone_5, )
  ttnn_to_layout_73 = ttnn.to_layout(ttnn_from_device_70, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_41 = ttnn.reshape(ttnn_to_layout_73, (1, 9, 768), )
  ttnn_from_device_71 = ttnn.from_device(ttnn_reshape_41, )
  ttnn_to_layout_74 = ttnn.to_layout(ttnn_from_device_71, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_42 = ttnn.reshape(ttnn_to_layout_74, (9, 768), )
  ttnn_from_device_72 = ttnn.from_device(ttnn_reshape_42, )
  ttnn_to_layout_75 = ttnn.to_layout(ttnn_from_device_72, ttnn.TILE_LAYOUT, )
  ttnn_to_device_34 = ttnn.to_device(ttnn_to_layout_75, device = device)
  ttnn_matmul_14 = ttnn.matmul(ttnn_to_device_34, ttnn_transpose_6, )
  ttnn_add_10 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_14, )
  ttnn_from_device_73 = ttnn.from_device(ttnn_add_10, )
  ttnn_to_layout_76 = ttnn.to_layout(ttnn_from_device_73, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_43 = ttnn.reshape(ttnn_to_layout_76, (1, 9, 768), )
  ttnn_from_device_74 = ttnn.from_device(ttnn_reshape_43, )
  ttnn_to_layout_77 = ttnn.to_layout(ttnn_from_device_74, ttnn.TILE_LAYOUT, )
  ttnn_to_device_35 = ttnn.to_device(ttnn_to_layout_77, device = device)
  ttnn_prefix_clone_6 = clone_wrapper(ttnn_to_device_35, )
  ttnn_add_83 = ttnn.add(ttnn_layer_norm_2, ttnn_prefix_clone_6, )
  ttnn_layer_norm_3_ = ttnn.layer_norm(ttnn_add_83, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_3_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_83), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_3 = ttnn.from_torch(ttnn_layer_norm_3_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_3_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_3_), ttnn_layer_norm_3))
  ttnn_from_device_75 = ttnn.from_device(ttnn_layer_norm_3, )
  ttnn_to_layout_78 = ttnn.to_layout(ttnn_from_device_75, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_44 = ttnn.reshape(ttnn_to_layout_78, (9, 768), )
  ttnn_from_device_76 = ttnn.from_device(ttnn_reshape_44, )
  ttnn_to_layout_79 = ttnn.to_layout(ttnn_from_device_76, ttnn.TILE_LAYOUT, )
  ttnn_to_device_36 = ttnn.to_device(ttnn_to_layout_79, device = device)
  ttnn_matmul_15 = ttnn.matmul(ttnn_to_device_36, ttnn_transpose_7, )
  ttnn_add_11 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_15, )
  ttnn_from_device_77 = ttnn.from_device(ttnn_add_11, )
  ttnn_to_layout_80 = ttnn.to_layout(ttnn_from_device_77, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_45 = ttnn.reshape(ttnn_to_layout_80, (1, 9, 3072), )
  ttnn_from_device_78 = ttnn.from_device(ttnn_reshape_45, )
  ttnn_to_layout_81 = ttnn.to_layout(ttnn_from_device_78, ttnn.TILE_LAYOUT, )
  ttnn_to_device_37 = ttnn.to_device(ttnn_to_layout_81, device = device)
  ttnn_multiply_7 = ttnn.multiply(ttnn_to_device_37, 0.5, )
  ttnn_pow_1 = ttnn.pow(ttnn_to_device_37, 3.0, )
  ttnn_multiply_8 = ttnn.multiply(ttnn_pow_1, 0.044715, )
  ttnn_add_84 = ttnn.add(ttnn_to_device_37, ttnn_multiply_8, )
  ttnn_multiply_9 = ttnn.multiply(ttnn_add_84, 0.7978845608028654, )
  ttnn_tanh_1_ = ttnn.tanh(ttnn_multiply_9, )
  ttnn_tanh_1_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_9), )
  ttnn_tanh_1 = ttnn.from_torch(ttnn_tanh_1_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_1_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_1_), ttnn_tanh_1))
  test_accuracy(tanh_1, ttnn_tanh_1)
  ttnn_add_85 = ttnn.add(ttnn_tanh_1, 1.0, )
  ttnn_multiply_10 = ttnn.multiply(ttnn_multiply_7, ttnn_add_85, )
  ttnn_from_device_79 = ttnn.from_device(ttnn_multiply_10, )
  ttnn_to_layout_82 = ttnn.to_layout(ttnn_from_device_79, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_46 = ttnn.reshape(ttnn_to_layout_82, (9, 3072), )
  ttnn_from_device_80 = ttnn.from_device(ttnn_reshape_46, )
  ttnn_to_layout_83 = ttnn.to_layout(ttnn_from_device_80, ttnn.TILE_LAYOUT, )
  ttnn_to_device_38 = ttnn.to_device(ttnn_to_layout_83, device = device)
  ttnn_matmul_16 = ttnn.matmul(ttnn_to_device_38, ttnn_transpose_8, )
  ttnn_add_12 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_16, )
  ttnn_from_device_81 = ttnn.from_device(ttnn_add_12, )
  ttnn_to_layout_84 = ttnn.to_layout(ttnn_from_device_81, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_47 = ttnn.reshape(ttnn_to_layout_84, (1, 9, 768), )
  ttnn_from_device_82 = ttnn.from_device(ttnn_reshape_47, )
  ttnn_to_layout_85 = ttnn.to_layout(ttnn_from_device_82, ttnn.TILE_LAYOUT, )
  ttnn_to_device_39 = ttnn.to_device(ttnn_to_layout_85, device = device)
  ttnn_add_86 = ttnn.add(ttnn_to_device_39, ttnn_layer_norm_3, )
  ttnn_layer_norm_4_ = ttnn.layer_norm(ttnn_add_86, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_4_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_86), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_4 = ttnn.from_torch(ttnn_layer_norm_4_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_4_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_4_), ttnn_layer_norm_4))
  ttnn_from_device_83 = ttnn.from_device(ttnn_layer_norm_4, )
  ttnn_to_layout_86 = ttnn.to_layout(ttnn_from_device_83, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_48 = ttnn.reshape(ttnn_to_layout_86, (9, 768), )
  ttnn_from_device_84 = ttnn.from_device(ttnn_reshape_48, )
  ttnn_to_layout_87 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_40 = ttnn.to_device(ttnn_to_layout_87, device = device)
  ttnn_matmul_17 = ttnn.matmul(ttnn_to_device_40, ttnn_transpose_1, )
  ttnn_add_13 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_17, )
  ttnn_from_device_85 = ttnn.from_device(ttnn_add_13, )
  ttnn_to_layout_88 = ttnn.to_layout(ttnn_from_device_85, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_49 = ttnn.reshape(ttnn_to_layout_88, (1, 9, 768), )
  ttnn_to_layout_89 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_41 = ttnn.to_device(ttnn_to_layout_89, device = device)
  ttnn_matmul_18 = ttnn.matmul(ttnn_to_device_41, ttnn_transpose_2, )
  ttnn_add_14 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_18, )
  ttnn_from_device_87 = ttnn.from_device(ttnn_add_14, )
  ttnn_to_layout_90 = ttnn.to_layout(ttnn_from_device_87, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_51 = ttnn.reshape(ttnn_to_layout_90, (1, 9, 768), )
  ttnn_to_layout_91 = ttnn.to_layout(ttnn_from_device_84, ttnn.TILE_LAYOUT, )
  ttnn_to_device_42 = ttnn.to_device(ttnn_to_layout_91, device = device)
  ttnn_matmul_19 = ttnn.matmul(ttnn_to_device_42, ttnn_transpose_3, )
  ttnn_add_15 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_19, )
  ttnn_from_device_89 = ttnn.from_device(ttnn_add_15, )
  ttnn_to_layout_92 = ttnn.to_layout(ttnn_from_device_89, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_53 = ttnn.reshape(ttnn_to_layout_92, (1, 9, 768), )
  ttnn_from_device_90 = ttnn.from_device(ttnn_reshape_49, )
  ttnn_to_layout_93 = ttnn.to_layout(ttnn_from_device_90, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_54 = ttnn.reshape(ttnn_to_layout_93, (1, 9, 12, 64), )
  ttnn_from_device_91 = ttnn.from_device(ttnn_reshape_54, )
  ttnn_to_layout_94 = ttnn.to_layout(ttnn_from_device_91, ttnn.TILE_LAYOUT, )
  ttnn_to_device_43 = ttnn.to_device(ttnn_to_layout_94, device = device)
  ttnn_permute_6 = ttnn.permute(ttnn_to_device_43, (0, 2, 1, 3), )
  ttnn_from_device_92 = ttnn.from_device(ttnn_reshape_51, )
  ttnn_to_layout_95 = ttnn.to_layout(ttnn_from_device_92, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_55 = ttnn.reshape(ttnn_to_layout_95, (1, 9, 12, 64), )
  ttnn_from_device_93 = ttnn.from_device(ttnn_reshape_55, )
  ttnn_to_layout_96 = ttnn.to_layout(ttnn_from_device_93, ttnn.TILE_LAYOUT, )
  ttnn_to_device_44 = ttnn.to_device(ttnn_to_layout_96, device = device)
  ttnn_permute_7 = ttnn.permute(ttnn_to_device_44, (0, 2, 1, 3), )
  ttnn_from_device_94 = ttnn.from_device(ttnn_reshape_53, )
  ttnn_to_layout_97 = ttnn.to_layout(ttnn_from_device_94, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_56 = ttnn.reshape(ttnn_to_layout_97, (1, 9, 12, 64), )
  ttnn_from_device_95 = ttnn.from_device(ttnn_reshape_56, )
  ttnn_to_layout_98 = ttnn.to_layout(ttnn_from_device_95, ttnn.TILE_LAYOUT, )
  ttnn_to_device_45 = ttnn.to_device(ttnn_to_layout_98, device = device)
  ttnn_permute_8 = ttnn.permute(ttnn_to_device_45, (0, 2, 1, 3), )
  ttnn_transpose_20 = ttnn.transpose(ttnn_permute_7, 3, 2, )
  ttnn_from_device_96 = ttnn.from_device(ttnn_permute_6, )
  ttnn_to_layout_99 = ttnn.to_layout(ttnn_from_device_96, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_57 = ttnn.reshape(ttnn_to_layout_99, (12, 9, 64), )
  ttnn_from_device_97 = ttnn.from_device(ttnn_transpose_20, )
  ttnn_to_layout_100 = ttnn.to_layout(ttnn_from_device_97, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_58 = ttnn.reshape(ttnn_to_layout_100, (12, 64, 9), )
  ttnn_from_device_98 = ttnn.from_device(ttnn_reshape_57, )
  ttnn_to_layout_101 = ttnn.to_layout(ttnn_from_device_98, ttnn.TILE_LAYOUT, )
  ttnn_to_device_46 = ttnn.to_device(ttnn_to_layout_101, device = device)
  ttnn_from_device_99 = ttnn.from_device(ttnn_reshape_58, )
  ttnn_to_layout_102 = ttnn.to_layout(ttnn_from_device_99, ttnn.TILE_LAYOUT, )
  ttnn_to_device_47 = ttnn.to_device(ttnn_to_layout_102, device = device)
  ttnn_matmul_20 = ttnn.matmul(ttnn_to_device_46, ttnn_to_device_47, )
  ttnn_from_device_100 = ttnn.from_device(ttnn_matmul_20, )
  ttnn_to_layout_103 = ttnn.to_layout(ttnn_from_device_100, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_59 = ttnn.reshape(ttnn_to_layout_103, (1, 12, 9, 9), )
  ttnn_from_device_101 = ttnn.from_device(ttnn_reshape_59, )
  ttnn_to_layout_104 = ttnn.to_layout(ttnn_from_device_101, ttnn.TILE_LAYOUT, )
  ttnn_to_device_48 = ttnn.to_device(ttnn_to_layout_104, device = device)
  ttnn_multiply_11 = ttnn.multiply(ttnn_to_device_48, 0.125, )
  ttnn_add_87 = ttnn.add(ttnn_multiply_11, ttnn_multiply, )
  ttnn_softmax_2 = ttnn.softmax(ttnn_add_87, -1, numeric_stable = True)
  test_accuracy(_softmax_2, ttnn_softmax_2)
  ttnn_prefix_clone_7 = clone_wrapper(ttnn_softmax_2, )
  ttnn_from_device_102 = ttnn.from_device(ttnn_prefix_clone_7, )
  ttnn_to_layout_105 = ttnn.to_layout(ttnn_from_device_102, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_60 = ttnn.reshape(ttnn_to_layout_105, (12, 9, 9), )
  ttnn_from_device_103 = ttnn.from_device(ttnn_permute_8, )
  ttnn_to_layout_106 = ttnn.to_layout(ttnn_from_device_103, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_61 = ttnn.reshape(ttnn_to_layout_106, (12, 9, 64), )
  ttnn_from_device_104 = ttnn.from_device(ttnn_reshape_60, )
  ttnn_to_layout_107 = ttnn.to_layout(ttnn_from_device_104, ttnn.TILE_LAYOUT, )
  ttnn_to_device_49 = ttnn.to_device(ttnn_to_layout_107, device = device)
  ttnn_from_device_105 = ttnn.from_device(ttnn_reshape_61, )
  ttnn_to_layout_108 = ttnn.to_layout(ttnn_from_device_105, ttnn.TILE_LAYOUT, )
  ttnn_to_device_50 = ttnn.to_device(ttnn_to_layout_108, device = device)
  ttnn_matmul_21 = ttnn.matmul(ttnn_to_device_49, ttnn_to_device_50, )
  ttnn_from_device_106 = ttnn.from_device(ttnn_matmul_21, )
  ttnn_to_layout_109 = ttnn.to_layout(ttnn_from_device_106, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_62 = ttnn.reshape(ttnn_to_layout_109, (1, 12, 9, 64), )
  ttnn_from_device_107 = ttnn.from_device(ttnn_reshape_62, )
  ttnn_to_layout_110 = ttnn.to_layout(ttnn_from_device_107, ttnn.TILE_LAYOUT, )
  ttnn_to_device_51 = ttnn.to_device(ttnn_to_layout_110, device = device)
  ttnn_transpose_21 = ttnn.transpose(ttnn_to_device_51, 2, 1, )
  ttnn_prefix_clone_8 = clone_wrapper(ttnn_transpose_21, )
  ttnn_from_device_108 = ttnn.from_device(ttnn_prefix_clone_8, )
  ttnn_to_layout_111 = ttnn.to_layout(ttnn_from_device_108, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_63 = ttnn.reshape(ttnn_to_layout_111, (1, 9, 768), )
  ttnn_from_device_109 = ttnn.from_device(ttnn_reshape_63, )
  ttnn_to_layout_112 = ttnn.to_layout(ttnn_from_device_109, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_64 = ttnn.reshape(ttnn_to_layout_112, (9, 768), )
  ttnn_from_device_110 = ttnn.from_device(ttnn_reshape_64, )
  ttnn_to_layout_113 = ttnn.to_layout(ttnn_from_device_110, ttnn.TILE_LAYOUT, )
  ttnn_to_device_52 = ttnn.to_device(ttnn_to_layout_113, device = device)
  ttnn_matmul_22 = ttnn.matmul(ttnn_to_device_52, ttnn_transpose_6, )
  ttnn_add_16 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_22, )
  ttnn_from_device_111 = ttnn.from_device(ttnn_add_16, )
  ttnn_to_layout_114 = ttnn.to_layout(ttnn_from_device_111, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_65 = ttnn.reshape(ttnn_to_layout_114, (1, 9, 768), )
  ttnn_from_device_112 = ttnn.from_device(ttnn_reshape_65, )
  ttnn_to_layout_115 = ttnn.to_layout(ttnn_from_device_112, ttnn.TILE_LAYOUT, )
  ttnn_to_device_53 = ttnn.to_device(ttnn_to_layout_115, device = device)
  ttnn_prefix_clone_9 = clone_wrapper(ttnn_to_device_53, )
  ttnn_add_88 = ttnn.add(ttnn_layer_norm_4, ttnn_prefix_clone_9, )
  ttnn_layer_norm_5_ = ttnn.layer_norm(ttnn_add_88, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_5_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_88), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_5 = ttnn.from_torch(ttnn_layer_norm_5_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_5_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_5_), ttnn_layer_norm_5))
  ttnn_from_device_113 = ttnn.from_device(ttnn_layer_norm_5, )
  ttnn_to_layout_116 = ttnn.to_layout(ttnn_from_device_113, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_66 = ttnn.reshape(ttnn_to_layout_116, (9, 768), )
  ttnn_from_device_114 = ttnn.from_device(ttnn_reshape_66, )
  ttnn_to_layout_117 = ttnn.to_layout(ttnn_from_device_114, ttnn.TILE_LAYOUT, )
  ttnn_to_device_54 = ttnn.to_device(ttnn_to_layout_117, device = device)
  ttnn_matmul_23 = ttnn.matmul(ttnn_to_device_54, ttnn_transpose_7, )
  ttnn_add_17 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_23, )
  ttnn_from_device_115 = ttnn.from_device(ttnn_add_17, )
  ttnn_to_layout_118 = ttnn.to_layout(ttnn_from_device_115, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_67 = ttnn.reshape(ttnn_to_layout_118, (1, 9, 3072), )
  ttnn_from_device_116 = ttnn.from_device(ttnn_reshape_67, )
  ttnn_to_layout_119 = ttnn.to_layout(ttnn_from_device_116, ttnn.TILE_LAYOUT, )
  ttnn_to_device_55 = ttnn.to_device(ttnn_to_layout_119, device = device)
  ttnn_multiply_12 = ttnn.multiply(ttnn_to_device_55, 0.5, )
  ttnn_pow_2 = ttnn.pow(ttnn_to_device_55, 3.0, )
  ttnn_multiply_13 = ttnn.multiply(ttnn_pow_2, 0.044715, )
  ttnn_add_89 = ttnn.add(ttnn_to_device_55, ttnn_multiply_13, )
  ttnn_multiply_14 = ttnn.multiply(ttnn_add_89, 0.7978845608028654, )
  ttnn_tanh_2_ = ttnn.tanh(ttnn_multiply_14, )
  ttnn_tanh_2_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_14), )
  ttnn_tanh_2 = ttnn.from_torch(ttnn_tanh_2_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_2_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_2_), ttnn_tanh_2))
  test_accuracy(tanh_2, ttnn_tanh_2)
  ttnn_add_90 = ttnn.add(ttnn_tanh_2, 1.0, )
  ttnn_multiply_15 = ttnn.multiply(ttnn_multiply_12, ttnn_add_90, )
  ttnn_from_device_117 = ttnn.from_device(ttnn_multiply_15, )
  ttnn_to_layout_120 = ttnn.to_layout(ttnn_from_device_117, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_68 = ttnn.reshape(ttnn_to_layout_120, (9, 3072), )
  ttnn_from_device_118 = ttnn.from_device(ttnn_reshape_68, )
  ttnn_to_layout_121 = ttnn.to_layout(ttnn_from_device_118, ttnn.TILE_LAYOUT, )
  ttnn_to_device_56 = ttnn.to_device(ttnn_to_layout_121, device = device)
  ttnn_matmul_24 = ttnn.matmul(ttnn_to_device_56, ttnn_transpose_8, )
  ttnn_add_18 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_24, )
  ttnn_from_device_119 = ttnn.from_device(ttnn_add_18, )
  ttnn_to_layout_122 = ttnn.to_layout(ttnn_from_device_119, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_69 = ttnn.reshape(ttnn_to_layout_122, (1, 9, 768), )
  ttnn_from_device_120 = ttnn.from_device(ttnn_reshape_69, )
  ttnn_to_layout_123 = ttnn.to_layout(ttnn_from_device_120, ttnn.TILE_LAYOUT, )
  ttnn_to_device_57 = ttnn.to_device(ttnn_to_layout_123, device = device)
  ttnn_add_91 = ttnn.add(ttnn_to_device_57, ttnn_layer_norm_5, )
  ttnn_layer_norm_6_ = ttnn.layer_norm(ttnn_add_91, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_6_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_91), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_6 = ttnn.from_torch(ttnn_layer_norm_6_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_6_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_6_), ttnn_layer_norm_6))
  ttnn_from_device_121 = ttnn.from_device(ttnn_layer_norm_6, )
  ttnn_to_layout_124 = ttnn.to_layout(ttnn_from_device_121, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_70 = ttnn.reshape(ttnn_to_layout_124, (9, 768), )
  ttnn_from_device_122 = ttnn.from_device(ttnn_reshape_70, )
  ttnn_to_layout_125 = ttnn.to_layout(ttnn_from_device_122, ttnn.TILE_LAYOUT, )
  ttnn_to_device_58 = ttnn.to_device(ttnn_to_layout_125, device = device)
  ttnn_matmul_25 = ttnn.matmul(ttnn_to_device_58, ttnn_transpose_1, )
  ttnn_add_19 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_25, )
  ttnn_from_device_123 = ttnn.from_device(ttnn_add_19, )
  ttnn_to_layout_126 = ttnn.to_layout(ttnn_from_device_123, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_71 = ttnn.reshape(ttnn_to_layout_126, (1, 9, 768), )
  ttnn_to_layout_127 = ttnn.to_layout(ttnn_from_device_122, ttnn.TILE_LAYOUT, )
  ttnn_to_device_59 = ttnn.to_device(ttnn_to_layout_127, device = device)
  ttnn_matmul_26 = ttnn.matmul(ttnn_to_device_59, ttnn_transpose_2, )
  ttnn_add_20 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_26, )
  ttnn_from_device_125 = ttnn.from_device(ttnn_add_20, )
  ttnn_to_layout_128 = ttnn.to_layout(ttnn_from_device_125, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_73 = ttnn.reshape(ttnn_to_layout_128, (1, 9, 768), )
  ttnn_to_layout_129 = ttnn.to_layout(ttnn_from_device_122, ttnn.TILE_LAYOUT, )
  ttnn_to_device_60 = ttnn.to_device(ttnn_to_layout_129, device = device)
  ttnn_matmul_27 = ttnn.matmul(ttnn_to_device_60, ttnn_transpose_3, )
  ttnn_add_21 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_27, )
  ttnn_from_device_127 = ttnn.from_device(ttnn_add_21, )
  ttnn_to_layout_130 = ttnn.to_layout(ttnn_from_device_127, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_75 = ttnn.reshape(ttnn_to_layout_130, (1, 9, 768), )
  ttnn_from_device_128 = ttnn.from_device(ttnn_reshape_71, )
  ttnn_to_layout_131 = ttnn.to_layout(ttnn_from_device_128, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_76 = ttnn.reshape(ttnn_to_layout_131, (1, 9, 12, 64), )
  ttnn_from_device_129 = ttnn.from_device(ttnn_reshape_76, )
  ttnn_to_layout_132 = ttnn.to_layout(ttnn_from_device_129, ttnn.TILE_LAYOUT, )
  ttnn_to_device_61 = ttnn.to_device(ttnn_to_layout_132, device = device)
  ttnn_permute_9 = ttnn.permute(ttnn_to_device_61, (0, 2, 1, 3), )
  ttnn_from_device_130 = ttnn.from_device(ttnn_reshape_73, )
  ttnn_to_layout_133 = ttnn.to_layout(ttnn_from_device_130, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_77 = ttnn.reshape(ttnn_to_layout_133, (1, 9, 12, 64), )
  ttnn_from_device_131 = ttnn.from_device(ttnn_reshape_77, )
  ttnn_to_layout_134 = ttnn.to_layout(ttnn_from_device_131, ttnn.TILE_LAYOUT, )
  ttnn_to_device_62 = ttnn.to_device(ttnn_to_layout_134, device = device)
  ttnn_permute_10 = ttnn.permute(ttnn_to_device_62, (0, 2, 1, 3), )
  ttnn_from_device_132 = ttnn.from_device(ttnn_reshape_75, )
  ttnn_to_layout_135 = ttnn.to_layout(ttnn_from_device_132, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_78 = ttnn.reshape(ttnn_to_layout_135, (1, 9, 12, 64), )
  ttnn_from_device_133 = ttnn.from_device(ttnn_reshape_78, )
  ttnn_to_layout_136 = ttnn.to_layout(ttnn_from_device_133, ttnn.TILE_LAYOUT, )
  ttnn_to_device_63 = ttnn.to_device(ttnn_to_layout_136, device = device)
  ttnn_permute_11 = ttnn.permute(ttnn_to_device_63, (0, 2, 1, 3), )
  ttnn_transpose_28 = ttnn.transpose(ttnn_permute_10, 3, 2, )
  ttnn_from_device_134 = ttnn.from_device(ttnn_permute_9, )
  ttnn_to_layout_137 = ttnn.to_layout(ttnn_from_device_134, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_79 = ttnn.reshape(ttnn_to_layout_137, (12, 9, 64), )
  ttnn_from_device_135 = ttnn.from_device(ttnn_transpose_28, )
  ttnn_to_layout_138 = ttnn.to_layout(ttnn_from_device_135, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_80 = ttnn.reshape(ttnn_to_layout_138, (12, 64, 9), )
  ttnn_from_device_136 = ttnn.from_device(ttnn_reshape_79, )
  ttnn_to_layout_139 = ttnn.to_layout(ttnn_from_device_136, ttnn.TILE_LAYOUT, )
  ttnn_to_device_64 = ttnn.to_device(ttnn_to_layout_139, device = device)
  ttnn_from_device_137 = ttnn.from_device(ttnn_reshape_80, )
  ttnn_to_layout_140 = ttnn.to_layout(ttnn_from_device_137, ttnn.TILE_LAYOUT, )
  ttnn_to_device_65 = ttnn.to_device(ttnn_to_layout_140, device = device)
  ttnn_matmul_28 = ttnn.matmul(ttnn_to_device_64, ttnn_to_device_65, )
  ttnn_from_device_138 = ttnn.from_device(ttnn_matmul_28, )
  ttnn_to_layout_141 = ttnn.to_layout(ttnn_from_device_138, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_81 = ttnn.reshape(ttnn_to_layout_141, (1, 12, 9, 9), )
  ttnn_from_device_139 = ttnn.from_device(ttnn_reshape_81, )
  ttnn_to_layout_142 = ttnn.to_layout(ttnn_from_device_139, ttnn.TILE_LAYOUT, )
  ttnn_to_device_66 = ttnn.to_device(ttnn_to_layout_142, device = device)
  ttnn_multiply_16 = ttnn.multiply(ttnn_to_device_66, 0.125, )
  ttnn_add_92 = ttnn.add(ttnn_multiply_16, ttnn_multiply, )
  ttnn_softmax_3 = ttnn.softmax(ttnn_add_92, -1, numeric_stable = True)
  test_accuracy(_softmax_3, ttnn_softmax_3)
  ttnn_prefix_clone_10 = clone_wrapper(ttnn_softmax_3, )
  ttnn_from_device_140 = ttnn.from_device(ttnn_prefix_clone_10, )
  ttnn_to_layout_143 = ttnn.to_layout(ttnn_from_device_140, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_82 = ttnn.reshape(ttnn_to_layout_143, (12, 9, 9), )
  ttnn_from_device_141 = ttnn.from_device(ttnn_permute_11, )
  ttnn_to_layout_144 = ttnn.to_layout(ttnn_from_device_141, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_83 = ttnn.reshape(ttnn_to_layout_144, (12, 9, 64), )
  ttnn_from_device_142 = ttnn.from_device(ttnn_reshape_82, )
  ttnn_to_layout_145 = ttnn.to_layout(ttnn_from_device_142, ttnn.TILE_LAYOUT, )
  ttnn_to_device_67 = ttnn.to_device(ttnn_to_layout_145, device = device)
  ttnn_from_device_143 = ttnn.from_device(ttnn_reshape_83, )
  ttnn_to_layout_146 = ttnn.to_layout(ttnn_from_device_143, ttnn.TILE_LAYOUT, )
  ttnn_to_device_68 = ttnn.to_device(ttnn_to_layout_146, device = device)
  ttnn_matmul_29 = ttnn.matmul(ttnn_to_device_67, ttnn_to_device_68, )
  ttnn_from_device_144 = ttnn.from_device(ttnn_matmul_29, )
  ttnn_to_layout_147 = ttnn.to_layout(ttnn_from_device_144, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_84 = ttnn.reshape(ttnn_to_layout_147, (1, 12, 9, 64), )
  ttnn_from_device_145 = ttnn.from_device(ttnn_reshape_84, )
  ttnn_to_layout_148 = ttnn.to_layout(ttnn_from_device_145, ttnn.TILE_LAYOUT, )
  ttnn_to_device_69 = ttnn.to_device(ttnn_to_layout_148, device = device)
  ttnn_transpose_29 = ttnn.transpose(ttnn_to_device_69, 2, 1, )
  ttnn_prefix_clone_11 = clone_wrapper(ttnn_transpose_29, )
  ttnn_from_device_146 = ttnn.from_device(ttnn_prefix_clone_11, )
  ttnn_to_layout_149 = ttnn.to_layout(ttnn_from_device_146, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_85 = ttnn.reshape(ttnn_to_layout_149, (1, 9, 768), )
  ttnn_from_device_147 = ttnn.from_device(ttnn_reshape_85, )
  ttnn_to_layout_150 = ttnn.to_layout(ttnn_from_device_147, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_86 = ttnn.reshape(ttnn_to_layout_150, (9, 768), )
  ttnn_from_device_148 = ttnn.from_device(ttnn_reshape_86, )
  ttnn_to_layout_151 = ttnn.to_layout(ttnn_from_device_148, ttnn.TILE_LAYOUT, )
  ttnn_to_device_70 = ttnn.to_device(ttnn_to_layout_151, device = device)
  ttnn_matmul_30 = ttnn.matmul(ttnn_to_device_70, ttnn_transpose_6, )
  ttnn_add_22 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_30, )
  ttnn_from_device_149 = ttnn.from_device(ttnn_add_22, )
  ttnn_to_layout_152 = ttnn.to_layout(ttnn_from_device_149, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_87 = ttnn.reshape(ttnn_to_layout_152, (1, 9, 768), )
  ttnn_from_device_150 = ttnn.from_device(ttnn_reshape_87, )
  ttnn_to_layout_153 = ttnn.to_layout(ttnn_from_device_150, ttnn.TILE_LAYOUT, )
  ttnn_to_device_71 = ttnn.to_device(ttnn_to_layout_153, device = device)
  ttnn_prefix_clone_12 = clone_wrapper(ttnn_to_device_71, )
  ttnn_add_93 = ttnn.add(ttnn_layer_norm_6, ttnn_prefix_clone_12, )
  ttnn_layer_norm_7_ = ttnn.layer_norm(ttnn_add_93, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_7_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_93), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_7 = ttnn.from_torch(ttnn_layer_norm_7_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_7_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_7_), ttnn_layer_norm_7))
  ttnn_from_device_151 = ttnn.from_device(ttnn_layer_norm_7, )
  ttnn_to_layout_154 = ttnn.to_layout(ttnn_from_device_151, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_88 = ttnn.reshape(ttnn_to_layout_154, (9, 768), )
  ttnn_from_device_152 = ttnn.from_device(ttnn_reshape_88, )
  ttnn_to_layout_155 = ttnn.to_layout(ttnn_from_device_152, ttnn.TILE_LAYOUT, )
  ttnn_to_device_72 = ttnn.to_device(ttnn_to_layout_155, device = device)
  ttnn_matmul_31 = ttnn.matmul(ttnn_to_device_72, ttnn_transpose_7, )
  ttnn_add_23 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_31, )
  ttnn_from_device_153 = ttnn.from_device(ttnn_add_23, )
  ttnn_to_layout_156 = ttnn.to_layout(ttnn_from_device_153, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_89 = ttnn.reshape(ttnn_to_layout_156, (1, 9, 3072), )
  ttnn_from_device_154 = ttnn.from_device(ttnn_reshape_89, )
  ttnn_to_layout_157 = ttnn.to_layout(ttnn_from_device_154, ttnn.TILE_LAYOUT, )
  ttnn_to_device_73 = ttnn.to_device(ttnn_to_layout_157, device = device)
  ttnn_multiply_17 = ttnn.multiply(ttnn_to_device_73, 0.5, )
  ttnn_pow_3 = ttnn.pow(ttnn_to_device_73, 3.0, )
  ttnn_multiply_18 = ttnn.multiply(ttnn_pow_3, 0.044715, )
  ttnn_add_94 = ttnn.add(ttnn_to_device_73, ttnn_multiply_18, )
  ttnn_multiply_19 = ttnn.multiply(ttnn_add_94, 0.7978845608028654, )
  ttnn_tanh_3_ = ttnn.tanh(ttnn_multiply_19, )
  ttnn_tanh_3_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_19), )
  ttnn_tanh_3 = ttnn.from_torch(ttnn_tanh_3_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_3_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_3_), ttnn_tanh_3))
  test_accuracy(tanh_3, ttnn_tanh_3)
  ttnn_add_95 = ttnn.add(ttnn_tanh_3, 1.0, )
  ttnn_multiply_20 = ttnn.multiply(ttnn_multiply_17, ttnn_add_95, )
  ttnn_from_device_155 = ttnn.from_device(ttnn_multiply_20, )
  ttnn_to_layout_158 = ttnn.to_layout(ttnn_from_device_155, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_90 = ttnn.reshape(ttnn_to_layout_158, (9, 3072), )
  ttnn_from_device_156 = ttnn.from_device(ttnn_reshape_90, )
  ttnn_to_layout_159 = ttnn.to_layout(ttnn_from_device_156, ttnn.TILE_LAYOUT, )
  ttnn_to_device_74 = ttnn.to_device(ttnn_to_layout_159, device = device)
  ttnn_matmul_32 = ttnn.matmul(ttnn_to_device_74, ttnn_transpose_8, )
  ttnn_add_24 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_32, )
  ttnn_from_device_157 = ttnn.from_device(ttnn_add_24, )
  ttnn_to_layout_160 = ttnn.to_layout(ttnn_from_device_157, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_91 = ttnn.reshape(ttnn_to_layout_160, (1, 9, 768), )
  ttnn_from_device_158 = ttnn.from_device(ttnn_reshape_91, )
  ttnn_to_layout_161 = ttnn.to_layout(ttnn_from_device_158, ttnn.TILE_LAYOUT, )
  ttnn_to_device_75 = ttnn.to_device(ttnn_to_layout_161, device = device)
  ttnn_add_96 = ttnn.add(ttnn_to_device_75, ttnn_layer_norm_7, )
  ttnn_layer_norm_8_ = ttnn.layer_norm(ttnn_add_96, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_8_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_96), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_8 = ttnn.from_torch(ttnn_layer_norm_8_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_8_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_8_), ttnn_layer_norm_8))
  ttnn_from_device_159 = ttnn.from_device(ttnn_layer_norm_8, )
  ttnn_to_layout_162 = ttnn.to_layout(ttnn_from_device_159, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_92 = ttnn.reshape(ttnn_to_layout_162, (9, 768), )
  ttnn_from_device_160 = ttnn.from_device(ttnn_reshape_92, )
  ttnn_to_layout_163 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_76 = ttnn.to_device(ttnn_to_layout_163, device = device)
  ttnn_matmul_33 = ttnn.matmul(ttnn_to_device_76, ttnn_transpose_1, )
  ttnn_add_25 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_33, )
  ttnn_from_device_161 = ttnn.from_device(ttnn_add_25, )
  ttnn_to_layout_164 = ttnn.to_layout(ttnn_from_device_161, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_93 = ttnn.reshape(ttnn_to_layout_164, (1, 9, 768), )
  ttnn_to_layout_165 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_77 = ttnn.to_device(ttnn_to_layout_165, device = device)
  ttnn_matmul_34 = ttnn.matmul(ttnn_to_device_77, ttnn_transpose_2, )
  ttnn_add_26 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_34, )
  ttnn_from_device_163 = ttnn.from_device(ttnn_add_26, )
  ttnn_to_layout_166 = ttnn.to_layout(ttnn_from_device_163, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_95 = ttnn.reshape(ttnn_to_layout_166, (1, 9, 768), )
  ttnn_to_layout_167 = ttnn.to_layout(ttnn_from_device_160, ttnn.TILE_LAYOUT, )
  ttnn_to_device_78 = ttnn.to_device(ttnn_to_layout_167, device = device)
  ttnn_matmul_35 = ttnn.matmul(ttnn_to_device_78, ttnn_transpose_3, )
  ttnn_add_27 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_35, )
  ttnn_from_device_165 = ttnn.from_device(ttnn_add_27, )
  ttnn_to_layout_168 = ttnn.to_layout(ttnn_from_device_165, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_97 = ttnn.reshape(ttnn_to_layout_168, (1, 9, 768), )
  ttnn_from_device_166 = ttnn.from_device(ttnn_reshape_93, )
  ttnn_to_layout_169 = ttnn.to_layout(ttnn_from_device_166, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_98 = ttnn.reshape(ttnn_to_layout_169, (1, 9, 12, 64), )
  ttnn_from_device_167 = ttnn.from_device(ttnn_reshape_98, )
  ttnn_to_layout_170 = ttnn.to_layout(ttnn_from_device_167, ttnn.TILE_LAYOUT, )
  ttnn_to_device_79 = ttnn.to_device(ttnn_to_layout_170, device = device)
  ttnn_permute_12 = ttnn.permute(ttnn_to_device_79, (0, 2, 1, 3), )
  ttnn_from_device_168 = ttnn.from_device(ttnn_reshape_95, )
  ttnn_to_layout_171 = ttnn.to_layout(ttnn_from_device_168, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_99 = ttnn.reshape(ttnn_to_layout_171, (1, 9, 12, 64), )
  ttnn_from_device_169 = ttnn.from_device(ttnn_reshape_99, )
  ttnn_to_layout_172 = ttnn.to_layout(ttnn_from_device_169, ttnn.TILE_LAYOUT, )
  ttnn_to_device_80 = ttnn.to_device(ttnn_to_layout_172, device = device)
  ttnn_permute_13 = ttnn.permute(ttnn_to_device_80, (0, 2, 1, 3), )
  ttnn_from_device_170 = ttnn.from_device(ttnn_reshape_97, )
  ttnn_to_layout_173 = ttnn.to_layout(ttnn_from_device_170, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_100 = ttnn.reshape(ttnn_to_layout_173, (1, 9, 12, 64), )
  ttnn_from_device_171 = ttnn.from_device(ttnn_reshape_100, )
  ttnn_to_layout_174 = ttnn.to_layout(ttnn_from_device_171, ttnn.TILE_LAYOUT, )
  ttnn_to_device_81 = ttnn.to_device(ttnn_to_layout_174, device = device)
  ttnn_permute_14 = ttnn.permute(ttnn_to_device_81, (0, 2, 1, 3), )
  ttnn_transpose_36 = ttnn.transpose(ttnn_permute_13, 3, 2, )
  ttnn_from_device_172 = ttnn.from_device(ttnn_permute_12, )
  ttnn_to_layout_175 = ttnn.to_layout(ttnn_from_device_172, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_101 = ttnn.reshape(ttnn_to_layout_175, (12, 9, 64), )
  ttnn_from_device_173 = ttnn.from_device(ttnn_transpose_36, )
  ttnn_to_layout_176 = ttnn.to_layout(ttnn_from_device_173, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_102 = ttnn.reshape(ttnn_to_layout_176, (12, 64, 9), )
  ttnn_from_device_174 = ttnn.from_device(ttnn_reshape_101, )
  ttnn_to_layout_177 = ttnn.to_layout(ttnn_from_device_174, ttnn.TILE_LAYOUT, )
  ttnn_to_device_82 = ttnn.to_device(ttnn_to_layout_177, device = device)
  ttnn_from_device_175 = ttnn.from_device(ttnn_reshape_102, )
  ttnn_to_layout_178 = ttnn.to_layout(ttnn_from_device_175, ttnn.TILE_LAYOUT, )
  ttnn_to_device_83 = ttnn.to_device(ttnn_to_layout_178, device = device)
  ttnn_matmul_36 = ttnn.matmul(ttnn_to_device_82, ttnn_to_device_83, )
  ttnn_from_device_176 = ttnn.from_device(ttnn_matmul_36, )
  ttnn_to_layout_179 = ttnn.to_layout(ttnn_from_device_176, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_103 = ttnn.reshape(ttnn_to_layout_179, (1, 12, 9, 9), )
  ttnn_from_device_177 = ttnn.from_device(ttnn_reshape_103, )
  ttnn_to_layout_180 = ttnn.to_layout(ttnn_from_device_177, ttnn.TILE_LAYOUT, )
  ttnn_to_device_84 = ttnn.to_device(ttnn_to_layout_180, device = device)
  ttnn_multiply_21 = ttnn.multiply(ttnn_to_device_84, 0.125, )
  ttnn_add_97 = ttnn.add(ttnn_multiply_21, ttnn_multiply, )
  ttnn_softmax_4 = ttnn.softmax(ttnn_add_97, -1, numeric_stable = True)
  test_accuracy(_softmax_4, ttnn_softmax_4)
  ttnn_prefix_clone_13 = clone_wrapper(ttnn_softmax_4, )
  ttnn_from_device_178 = ttnn.from_device(ttnn_prefix_clone_13, )
  ttnn_to_layout_181 = ttnn.to_layout(ttnn_from_device_178, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_104 = ttnn.reshape(ttnn_to_layout_181, (12, 9, 9), )
  ttnn_from_device_179 = ttnn.from_device(ttnn_permute_14, )
  ttnn_to_layout_182 = ttnn.to_layout(ttnn_from_device_179, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_105 = ttnn.reshape(ttnn_to_layout_182, (12, 9, 64), )
  ttnn_from_device_180 = ttnn.from_device(ttnn_reshape_104, )
  ttnn_to_layout_183 = ttnn.to_layout(ttnn_from_device_180, ttnn.TILE_LAYOUT, )
  ttnn_to_device_85 = ttnn.to_device(ttnn_to_layout_183, device = device)
  ttnn_from_device_181 = ttnn.from_device(ttnn_reshape_105, )
  ttnn_to_layout_184 = ttnn.to_layout(ttnn_from_device_181, ttnn.TILE_LAYOUT, )
  ttnn_to_device_86 = ttnn.to_device(ttnn_to_layout_184, device = device)
  ttnn_matmul_37 = ttnn.matmul(ttnn_to_device_85, ttnn_to_device_86, )
  ttnn_from_device_182 = ttnn.from_device(ttnn_matmul_37, )
  ttnn_to_layout_185 = ttnn.to_layout(ttnn_from_device_182, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_106 = ttnn.reshape(ttnn_to_layout_185, (1, 12, 9, 64), )
  ttnn_from_device_183 = ttnn.from_device(ttnn_reshape_106, )
  ttnn_to_layout_186 = ttnn.to_layout(ttnn_from_device_183, ttnn.TILE_LAYOUT, )
  ttnn_to_device_87 = ttnn.to_device(ttnn_to_layout_186, device = device)
  ttnn_transpose_37 = ttnn.transpose(ttnn_to_device_87, 2, 1, )
  ttnn_prefix_clone_14 = clone_wrapper(ttnn_transpose_37, )
  ttnn_from_device_184 = ttnn.from_device(ttnn_prefix_clone_14, )
  ttnn_to_layout_187 = ttnn.to_layout(ttnn_from_device_184, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_107 = ttnn.reshape(ttnn_to_layout_187, (1, 9, 768), )
  ttnn_from_device_185 = ttnn.from_device(ttnn_reshape_107, )
  ttnn_to_layout_188 = ttnn.to_layout(ttnn_from_device_185, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_108 = ttnn.reshape(ttnn_to_layout_188, (9, 768), )
  ttnn_from_device_186 = ttnn.from_device(ttnn_reshape_108, )
  ttnn_to_layout_189 = ttnn.to_layout(ttnn_from_device_186, ttnn.TILE_LAYOUT, )
  ttnn_to_device_88 = ttnn.to_device(ttnn_to_layout_189, device = device)
  ttnn_matmul_38 = ttnn.matmul(ttnn_to_device_88, ttnn_transpose_6, )
  ttnn_add_28 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_38, )
  ttnn_from_device_187 = ttnn.from_device(ttnn_add_28, )
  ttnn_to_layout_190 = ttnn.to_layout(ttnn_from_device_187, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_109 = ttnn.reshape(ttnn_to_layout_190, (1, 9, 768), )
  ttnn_from_device_188 = ttnn.from_device(ttnn_reshape_109, )
  ttnn_to_layout_191 = ttnn.to_layout(ttnn_from_device_188, ttnn.TILE_LAYOUT, )
  ttnn_to_device_89 = ttnn.to_device(ttnn_to_layout_191, device = device)
  ttnn_prefix_clone_15 = clone_wrapper(ttnn_to_device_89, )
  ttnn_add_98 = ttnn.add(ttnn_layer_norm_8, ttnn_prefix_clone_15, )
  ttnn_layer_norm_9_ = ttnn.layer_norm(ttnn_add_98, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_9_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_98), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_9 = ttnn.from_torch(ttnn_layer_norm_9_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_9_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_9_), ttnn_layer_norm_9))
  ttnn_from_device_189 = ttnn.from_device(ttnn_layer_norm_9, )
  ttnn_to_layout_192 = ttnn.to_layout(ttnn_from_device_189, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_110 = ttnn.reshape(ttnn_to_layout_192, (9, 768), )
  ttnn_from_device_190 = ttnn.from_device(ttnn_reshape_110, )
  ttnn_to_layout_193 = ttnn.to_layout(ttnn_from_device_190, ttnn.TILE_LAYOUT, )
  ttnn_to_device_90 = ttnn.to_device(ttnn_to_layout_193, device = device)
  ttnn_matmul_39 = ttnn.matmul(ttnn_to_device_90, ttnn_transpose_7, )
  ttnn_add_29 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_39, )
  ttnn_from_device_191 = ttnn.from_device(ttnn_add_29, )
  ttnn_to_layout_194 = ttnn.to_layout(ttnn_from_device_191, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_111 = ttnn.reshape(ttnn_to_layout_194, (1, 9, 3072), )
  ttnn_from_device_192 = ttnn.from_device(ttnn_reshape_111, )
  ttnn_to_layout_195 = ttnn.to_layout(ttnn_from_device_192, ttnn.TILE_LAYOUT, )
  ttnn_to_device_91 = ttnn.to_device(ttnn_to_layout_195, device = device)
  ttnn_multiply_22 = ttnn.multiply(ttnn_to_device_91, 0.5, )
  ttnn_pow_4 = ttnn.pow(ttnn_to_device_91, 3.0, )
  ttnn_multiply_23 = ttnn.multiply(ttnn_pow_4, 0.044715, )
  ttnn_add_99 = ttnn.add(ttnn_to_device_91, ttnn_multiply_23, )
  ttnn_multiply_24 = ttnn.multiply(ttnn_add_99, 0.7978845608028654, )
  ttnn_tanh_4_ = ttnn.tanh(ttnn_multiply_24, )
  ttnn_tanh_4_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_24), )
  ttnn_tanh_4 = ttnn.from_torch(ttnn_tanh_4_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_4_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_4_), ttnn_tanh_4))
  test_accuracy(tanh_4, ttnn_tanh_4)
  ttnn_add_100 = ttnn.add(ttnn_tanh_4, 1.0, )
  ttnn_multiply_25 = ttnn.multiply(ttnn_multiply_22, ttnn_add_100, )
  ttnn_from_device_193 = ttnn.from_device(ttnn_multiply_25, )
  ttnn_to_layout_196 = ttnn.to_layout(ttnn_from_device_193, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_112 = ttnn.reshape(ttnn_to_layout_196, (9, 3072), )
  ttnn_from_device_194 = ttnn.from_device(ttnn_reshape_112, )
  ttnn_to_layout_197 = ttnn.to_layout(ttnn_from_device_194, ttnn.TILE_LAYOUT, )
  ttnn_to_device_92 = ttnn.to_device(ttnn_to_layout_197, device = device)
  ttnn_matmul_40 = ttnn.matmul(ttnn_to_device_92, ttnn_transpose_8, )
  ttnn_add_30 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_40, )
  ttnn_from_device_195 = ttnn.from_device(ttnn_add_30, )
  ttnn_to_layout_198 = ttnn.to_layout(ttnn_from_device_195, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_113 = ttnn.reshape(ttnn_to_layout_198, (1, 9, 768), )
  ttnn_from_device_196 = ttnn.from_device(ttnn_reshape_113, )
  ttnn_to_layout_199 = ttnn.to_layout(ttnn_from_device_196, ttnn.TILE_LAYOUT, )
  ttnn_to_device_93 = ttnn.to_device(ttnn_to_layout_199, device = device)
  ttnn_add_101 = ttnn.add(ttnn_to_device_93, ttnn_layer_norm_9, )
  ttnn_layer_norm_10_ = ttnn.layer_norm(ttnn_add_101, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_10_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_101), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_10 = ttnn.from_torch(ttnn_layer_norm_10_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_10_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_10_), ttnn_layer_norm_10))
  ttnn_from_device_197 = ttnn.from_device(ttnn_layer_norm_10, )
  ttnn_to_layout_200 = ttnn.to_layout(ttnn_from_device_197, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_114 = ttnn.reshape(ttnn_to_layout_200, (9, 768), )
  ttnn_from_device_198 = ttnn.from_device(ttnn_reshape_114, )
  ttnn_to_layout_201 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_94 = ttnn.to_device(ttnn_to_layout_201, device = device)
  ttnn_matmul_41 = ttnn.matmul(ttnn_to_device_94, ttnn_transpose_1, )
  ttnn_add_31 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_41, )
  ttnn_from_device_199 = ttnn.from_device(ttnn_add_31, )
  ttnn_to_layout_202 = ttnn.to_layout(ttnn_from_device_199, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_115 = ttnn.reshape(ttnn_to_layout_202, (1, 9, 768), )
  ttnn_to_layout_203 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_95 = ttnn.to_device(ttnn_to_layout_203, device = device)
  ttnn_matmul_42 = ttnn.matmul(ttnn_to_device_95, ttnn_transpose_2, )
  ttnn_add_32 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_42, )
  ttnn_from_device_201 = ttnn.from_device(ttnn_add_32, )
  ttnn_to_layout_204 = ttnn.to_layout(ttnn_from_device_201, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_117 = ttnn.reshape(ttnn_to_layout_204, (1, 9, 768), )
  ttnn_to_layout_205 = ttnn.to_layout(ttnn_from_device_198, ttnn.TILE_LAYOUT, )
  ttnn_to_device_96 = ttnn.to_device(ttnn_to_layout_205, device = device)
  ttnn_matmul_43 = ttnn.matmul(ttnn_to_device_96, ttnn_transpose_3, )
  ttnn_add_33 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_43, )
  ttnn_from_device_203 = ttnn.from_device(ttnn_add_33, )
  ttnn_to_layout_206 = ttnn.to_layout(ttnn_from_device_203, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_119 = ttnn.reshape(ttnn_to_layout_206, (1, 9, 768), )
  ttnn_from_device_204 = ttnn.from_device(ttnn_reshape_115, )
  ttnn_to_layout_207 = ttnn.to_layout(ttnn_from_device_204, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_120 = ttnn.reshape(ttnn_to_layout_207, (1, 9, 12, 64), )
  ttnn_from_device_205 = ttnn.from_device(ttnn_reshape_120, )
  ttnn_to_layout_208 = ttnn.to_layout(ttnn_from_device_205, ttnn.TILE_LAYOUT, )
  ttnn_to_device_97 = ttnn.to_device(ttnn_to_layout_208, device = device)
  ttnn_permute_15 = ttnn.permute(ttnn_to_device_97, (0, 2, 1, 3), )
  ttnn_from_device_206 = ttnn.from_device(ttnn_reshape_117, )
  ttnn_to_layout_209 = ttnn.to_layout(ttnn_from_device_206, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_121 = ttnn.reshape(ttnn_to_layout_209, (1, 9, 12, 64), )
  ttnn_from_device_207 = ttnn.from_device(ttnn_reshape_121, )
  ttnn_to_layout_210 = ttnn.to_layout(ttnn_from_device_207, ttnn.TILE_LAYOUT, )
  ttnn_to_device_98 = ttnn.to_device(ttnn_to_layout_210, device = device)
  ttnn_permute_16 = ttnn.permute(ttnn_to_device_98, (0, 2, 1, 3), )
  ttnn_from_device_208 = ttnn.from_device(ttnn_reshape_119, )
  ttnn_to_layout_211 = ttnn.to_layout(ttnn_from_device_208, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_122 = ttnn.reshape(ttnn_to_layout_211, (1, 9, 12, 64), )
  ttnn_from_device_209 = ttnn.from_device(ttnn_reshape_122, )
  ttnn_to_layout_212 = ttnn.to_layout(ttnn_from_device_209, ttnn.TILE_LAYOUT, )
  ttnn_to_device_99 = ttnn.to_device(ttnn_to_layout_212, device = device)
  ttnn_permute_17 = ttnn.permute(ttnn_to_device_99, (0, 2, 1, 3), )
  ttnn_transpose_44 = ttnn.transpose(ttnn_permute_16, 3, 2, )
  ttnn_from_device_210 = ttnn.from_device(ttnn_permute_15, )
  ttnn_to_layout_213 = ttnn.to_layout(ttnn_from_device_210, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_123 = ttnn.reshape(ttnn_to_layout_213, (12, 9, 64), )
  ttnn_from_device_211 = ttnn.from_device(ttnn_transpose_44, )
  ttnn_to_layout_214 = ttnn.to_layout(ttnn_from_device_211, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_124 = ttnn.reshape(ttnn_to_layout_214, (12, 64, 9), )
  ttnn_from_device_212 = ttnn.from_device(ttnn_reshape_123, )
  ttnn_to_layout_215 = ttnn.to_layout(ttnn_from_device_212, ttnn.TILE_LAYOUT, )
  ttnn_to_device_100 = ttnn.to_device(ttnn_to_layout_215, device = device)
  ttnn_from_device_213 = ttnn.from_device(ttnn_reshape_124, )
  ttnn_to_layout_216 = ttnn.to_layout(ttnn_from_device_213, ttnn.TILE_LAYOUT, )
  ttnn_to_device_101 = ttnn.to_device(ttnn_to_layout_216, device = device)
  ttnn_matmul_44 = ttnn.matmul(ttnn_to_device_100, ttnn_to_device_101, )
  ttnn_from_device_214 = ttnn.from_device(ttnn_matmul_44, )
  ttnn_to_layout_217 = ttnn.to_layout(ttnn_from_device_214, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_125 = ttnn.reshape(ttnn_to_layout_217, (1, 12, 9, 9), )
  ttnn_from_device_215 = ttnn.from_device(ttnn_reshape_125, )
  ttnn_to_layout_218 = ttnn.to_layout(ttnn_from_device_215, ttnn.TILE_LAYOUT, )
  ttnn_to_device_102 = ttnn.to_device(ttnn_to_layout_218, device = device)
  ttnn_multiply_26 = ttnn.multiply(ttnn_to_device_102, 0.125, )
  ttnn_add_102 = ttnn.add(ttnn_multiply_26, ttnn_multiply, )
  ttnn_softmax_5 = ttnn.softmax(ttnn_add_102, -1, numeric_stable = True)
  test_accuracy(_softmax_5, ttnn_softmax_5)
  ttnn_prefix_clone_16 = clone_wrapper(ttnn_softmax_5, )
  ttnn_from_device_216 = ttnn.from_device(ttnn_prefix_clone_16, )
  ttnn_to_layout_219 = ttnn.to_layout(ttnn_from_device_216, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_126 = ttnn.reshape(ttnn_to_layout_219, (12, 9, 9), )
  ttnn_from_device_217 = ttnn.from_device(ttnn_permute_17, )
  ttnn_to_layout_220 = ttnn.to_layout(ttnn_from_device_217, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_127 = ttnn.reshape(ttnn_to_layout_220, (12, 9, 64), )
  ttnn_from_device_218 = ttnn.from_device(ttnn_reshape_126, )
  ttnn_to_layout_221 = ttnn.to_layout(ttnn_from_device_218, ttnn.TILE_LAYOUT, )
  ttnn_to_device_103 = ttnn.to_device(ttnn_to_layout_221, device = device)
  ttnn_from_device_219 = ttnn.from_device(ttnn_reshape_127, )
  ttnn_to_layout_222 = ttnn.to_layout(ttnn_from_device_219, ttnn.TILE_LAYOUT, )
  ttnn_to_device_104 = ttnn.to_device(ttnn_to_layout_222, device = device)
  ttnn_matmul_45 = ttnn.matmul(ttnn_to_device_103, ttnn_to_device_104, )
  ttnn_from_device_220 = ttnn.from_device(ttnn_matmul_45, )
  ttnn_to_layout_223 = ttnn.to_layout(ttnn_from_device_220, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_128 = ttnn.reshape(ttnn_to_layout_223, (1, 12, 9, 64), )
  ttnn_from_device_221 = ttnn.from_device(ttnn_reshape_128, )
  ttnn_to_layout_224 = ttnn.to_layout(ttnn_from_device_221, ttnn.TILE_LAYOUT, )
  ttnn_to_device_105 = ttnn.to_device(ttnn_to_layout_224, device = device)
  ttnn_transpose_45 = ttnn.transpose(ttnn_to_device_105, 2, 1, )
  ttnn_prefix_clone_17 = clone_wrapper(ttnn_transpose_45, )
  ttnn_from_device_222 = ttnn.from_device(ttnn_prefix_clone_17, )
  ttnn_to_layout_225 = ttnn.to_layout(ttnn_from_device_222, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_129 = ttnn.reshape(ttnn_to_layout_225, (1, 9, 768), )
  ttnn_from_device_223 = ttnn.from_device(ttnn_reshape_129, )
  ttnn_to_layout_226 = ttnn.to_layout(ttnn_from_device_223, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_130 = ttnn.reshape(ttnn_to_layout_226, (9, 768), )
  ttnn_from_device_224 = ttnn.from_device(ttnn_reshape_130, )
  ttnn_to_layout_227 = ttnn.to_layout(ttnn_from_device_224, ttnn.TILE_LAYOUT, )
  ttnn_to_device_106 = ttnn.to_device(ttnn_to_layout_227, device = device)
  ttnn_matmul_46 = ttnn.matmul(ttnn_to_device_106, ttnn_transpose_6, )
  ttnn_add_34 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_46, )
  ttnn_from_device_225 = ttnn.from_device(ttnn_add_34, )
  ttnn_to_layout_228 = ttnn.to_layout(ttnn_from_device_225, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_131 = ttnn.reshape(ttnn_to_layout_228, (1, 9, 768), )
  ttnn_from_device_226 = ttnn.from_device(ttnn_reshape_131, )
  ttnn_to_layout_229 = ttnn.to_layout(ttnn_from_device_226, ttnn.TILE_LAYOUT, )
  ttnn_to_device_107 = ttnn.to_device(ttnn_to_layout_229, device = device)
  ttnn_prefix_clone_18 = clone_wrapper(ttnn_to_device_107, )
  ttnn_add_103 = ttnn.add(ttnn_layer_norm_10, ttnn_prefix_clone_18, )
  ttnn_layer_norm_11_ = ttnn.layer_norm(ttnn_add_103, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_11_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_103), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_11 = ttnn.from_torch(ttnn_layer_norm_11_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_11_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_11_), ttnn_layer_norm_11))
  ttnn_from_device_227 = ttnn.from_device(ttnn_layer_norm_11, )
  ttnn_to_layout_230 = ttnn.to_layout(ttnn_from_device_227, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_132 = ttnn.reshape(ttnn_to_layout_230, (9, 768), )
  ttnn_from_device_228 = ttnn.from_device(ttnn_reshape_132, )
  ttnn_to_layout_231 = ttnn.to_layout(ttnn_from_device_228, ttnn.TILE_LAYOUT, )
  ttnn_to_device_108 = ttnn.to_device(ttnn_to_layout_231, device = device)
  ttnn_matmul_47 = ttnn.matmul(ttnn_to_device_108, ttnn_transpose_7, )
  ttnn_add_35 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_47, )
  ttnn_from_device_229 = ttnn.from_device(ttnn_add_35, )
  ttnn_to_layout_232 = ttnn.to_layout(ttnn_from_device_229, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_133 = ttnn.reshape(ttnn_to_layout_232, (1, 9, 3072), )
  ttnn_from_device_230 = ttnn.from_device(ttnn_reshape_133, )
  ttnn_to_layout_233 = ttnn.to_layout(ttnn_from_device_230, ttnn.TILE_LAYOUT, )
  ttnn_to_device_109 = ttnn.to_device(ttnn_to_layout_233, device = device)
  ttnn_multiply_27 = ttnn.multiply(ttnn_to_device_109, 0.5, )
  ttnn_pow_5 = ttnn.pow(ttnn_to_device_109, 3.0, )
  ttnn_multiply_28 = ttnn.multiply(ttnn_pow_5, 0.044715, )
  ttnn_add_104 = ttnn.add(ttnn_to_device_109, ttnn_multiply_28, )
  ttnn_multiply_29 = ttnn.multiply(ttnn_add_104, 0.7978845608028654, )
  ttnn_tanh_5_ = ttnn.tanh(ttnn_multiply_29, )
  ttnn_tanh_5_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_29), )
  ttnn_tanh_5 = ttnn.from_torch(ttnn_tanh_5_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_5_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_5_), ttnn_tanh_5))
  test_accuracy(tanh_5, ttnn_tanh_5)
  ttnn_add_105 = ttnn.add(ttnn_tanh_5, 1.0, )
  ttnn_multiply_30 = ttnn.multiply(ttnn_multiply_27, ttnn_add_105, )
  ttnn_from_device_231 = ttnn.from_device(ttnn_multiply_30, )
  ttnn_to_layout_234 = ttnn.to_layout(ttnn_from_device_231, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_134 = ttnn.reshape(ttnn_to_layout_234, (9, 3072), )
  ttnn_from_device_232 = ttnn.from_device(ttnn_reshape_134, )
  ttnn_to_layout_235 = ttnn.to_layout(ttnn_from_device_232, ttnn.TILE_LAYOUT, )
  ttnn_to_device_110 = ttnn.to_device(ttnn_to_layout_235, device = device)
  ttnn_matmul_48 = ttnn.matmul(ttnn_to_device_110, ttnn_transpose_8, )
  ttnn_add_36 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_48, )
  ttnn_from_device_233 = ttnn.from_device(ttnn_add_36, )
  ttnn_to_layout_236 = ttnn.to_layout(ttnn_from_device_233, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_135 = ttnn.reshape(ttnn_to_layout_236, (1, 9, 768), )
  ttnn_from_device_234 = ttnn.from_device(ttnn_reshape_135, )
  ttnn_to_layout_237 = ttnn.to_layout(ttnn_from_device_234, ttnn.TILE_LAYOUT, )
  ttnn_to_device_111 = ttnn.to_device(ttnn_to_layout_237, device = device)
  ttnn_add_106 = ttnn.add(ttnn_to_device_111, ttnn_layer_norm_11, )
  ttnn_layer_norm_12_ = ttnn.layer_norm(ttnn_add_106, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_12_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_106), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_12 = ttnn.from_torch(ttnn_layer_norm_12_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_12_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_12_), ttnn_layer_norm_12))
  ttnn_from_device_235 = ttnn.from_device(ttnn_layer_norm_12, )
  ttnn_to_layout_238 = ttnn.to_layout(ttnn_from_device_235, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_136 = ttnn.reshape(ttnn_to_layout_238, (9, 768), )
  ttnn_from_device_236 = ttnn.from_device(ttnn_reshape_136, )
  ttnn_to_layout_239 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_112 = ttnn.to_device(ttnn_to_layout_239, device = device)
  ttnn_matmul_49 = ttnn.matmul(ttnn_to_device_112, ttnn_transpose_1, )
  ttnn_add_37 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_49, )
  ttnn_from_device_237 = ttnn.from_device(ttnn_add_37, )
  ttnn_to_layout_240 = ttnn.to_layout(ttnn_from_device_237, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_137 = ttnn.reshape(ttnn_to_layout_240, (1, 9, 768), )
  ttnn_to_layout_241 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_113 = ttnn.to_device(ttnn_to_layout_241, device = device)
  ttnn_matmul_50 = ttnn.matmul(ttnn_to_device_113, ttnn_transpose_2, )
  ttnn_add_38 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_50, )
  ttnn_from_device_239 = ttnn.from_device(ttnn_add_38, )
  ttnn_to_layout_242 = ttnn.to_layout(ttnn_from_device_239, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_139 = ttnn.reshape(ttnn_to_layout_242, (1, 9, 768), )
  ttnn_to_layout_243 = ttnn.to_layout(ttnn_from_device_236, ttnn.TILE_LAYOUT, )
  ttnn_to_device_114 = ttnn.to_device(ttnn_to_layout_243, device = device)
  ttnn_matmul_51 = ttnn.matmul(ttnn_to_device_114, ttnn_transpose_3, )
  ttnn_add_39 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_51, )
  ttnn_from_device_241 = ttnn.from_device(ttnn_add_39, )
  ttnn_to_layout_244 = ttnn.to_layout(ttnn_from_device_241, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_141 = ttnn.reshape(ttnn_to_layout_244, (1, 9, 768), )
  ttnn_from_device_242 = ttnn.from_device(ttnn_reshape_137, )
  ttnn_to_layout_245 = ttnn.to_layout(ttnn_from_device_242, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_142 = ttnn.reshape(ttnn_to_layout_245, (1, 9, 12, 64), )
  ttnn_from_device_243 = ttnn.from_device(ttnn_reshape_142, )
  ttnn_to_layout_246 = ttnn.to_layout(ttnn_from_device_243, ttnn.TILE_LAYOUT, )
  ttnn_to_device_115 = ttnn.to_device(ttnn_to_layout_246, device = device)
  ttnn_permute_18 = ttnn.permute(ttnn_to_device_115, (0, 2, 1, 3), )
  ttnn_from_device_244 = ttnn.from_device(ttnn_reshape_139, )
  ttnn_to_layout_247 = ttnn.to_layout(ttnn_from_device_244, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_143 = ttnn.reshape(ttnn_to_layout_247, (1, 9, 12, 64), )
  ttnn_from_device_245 = ttnn.from_device(ttnn_reshape_143, )
  ttnn_to_layout_248 = ttnn.to_layout(ttnn_from_device_245, ttnn.TILE_LAYOUT, )
  ttnn_to_device_116 = ttnn.to_device(ttnn_to_layout_248, device = device)
  ttnn_permute_19 = ttnn.permute(ttnn_to_device_116, (0, 2, 1, 3), )
  ttnn_from_device_246 = ttnn.from_device(ttnn_reshape_141, )
  ttnn_to_layout_249 = ttnn.to_layout(ttnn_from_device_246, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_144 = ttnn.reshape(ttnn_to_layout_249, (1, 9, 12, 64), )
  ttnn_from_device_247 = ttnn.from_device(ttnn_reshape_144, )
  ttnn_to_layout_250 = ttnn.to_layout(ttnn_from_device_247, ttnn.TILE_LAYOUT, )
  ttnn_to_device_117 = ttnn.to_device(ttnn_to_layout_250, device = device)
  ttnn_permute_20 = ttnn.permute(ttnn_to_device_117, (0, 2, 1, 3), )
  ttnn_transpose_52 = ttnn.transpose(ttnn_permute_19, 3, 2, )
  ttnn_from_device_248 = ttnn.from_device(ttnn_permute_18, )
  ttnn_to_layout_251 = ttnn.to_layout(ttnn_from_device_248, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_145 = ttnn.reshape(ttnn_to_layout_251, (12, 9, 64), )
  ttnn_from_device_249 = ttnn.from_device(ttnn_transpose_52, )
  ttnn_to_layout_252 = ttnn.to_layout(ttnn_from_device_249, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_146 = ttnn.reshape(ttnn_to_layout_252, (12, 64, 9), )
  ttnn_from_device_250 = ttnn.from_device(ttnn_reshape_145, )
  ttnn_to_layout_253 = ttnn.to_layout(ttnn_from_device_250, ttnn.TILE_LAYOUT, )
  ttnn_to_device_118 = ttnn.to_device(ttnn_to_layout_253, device = device)
  ttnn_from_device_251 = ttnn.from_device(ttnn_reshape_146, )
  ttnn_to_layout_254 = ttnn.to_layout(ttnn_from_device_251, ttnn.TILE_LAYOUT, )
  ttnn_to_device_119 = ttnn.to_device(ttnn_to_layout_254, device = device)
  ttnn_matmul_52 = ttnn.matmul(ttnn_to_device_118, ttnn_to_device_119, )
  ttnn_from_device_252 = ttnn.from_device(ttnn_matmul_52, )
  ttnn_to_layout_255 = ttnn.to_layout(ttnn_from_device_252, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_147 = ttnn.reshape(ttnn_to_layout_255, (1, 12, 9, 9), )
  ttnn_from_device_253 = ttnn.from_device(ttnn_reshape_147, )
  ttnn_to_layout_256 = ttnn.to_layout(ttnn_from_device_253, ttnn.TILE_LAYOUT, )
  ttnn_to_device_120 = ttnn.to_device(ttnn_to_layout_256, device = device)
  ttnn_multiply_31 = ttnn.multiply(ttnn_to_device_120, 0.125, )
  ttnn_add_107 = ttnn.add(ttnn_multiply_31, ttnn_multiply, )
  ttnn_softmax_6 = ttnn.softmax(ttnn_add_107, -1, numeric_stable = True)
  test_accuracy(_softmax_6, ttnn_softmax_6)
  ttnn_prefix_clone_19 = clone_wrapper(ttnn_softmax_6, )
  ttnn_from_device_254 = ttnn.from_device(ttnn_prefix_clone_19, )
  ttnn_to_layout_257 = ttnn.to_layout(ttnn_from_device_254, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_148 = ttnn.reshape(ttnn_to_layout_257, (12, 9, 9), )
  ttnn_from_device_255 = ttnn.from_device(ttnn_permute_20, )
  ttnn_to_layout_258 = ttnn.to_layout(ttnn_from_device_255, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_149 = ttnn.reshape(ttnn_to_layout_258, (12, 9, 64), )
  ttnn_from_device_256 = ttnn.from_device(ttnn_reshape_148, )
  ttnn_to_layout_259 = ttnn.to_layout(ttnn_from_device_256, ttnn.TILE_LAYOUT, )
  ttnn_to_device_121 = ttnn.to_device(ttnn_to_layout_259, device = device)
  ttnn_from_device_257 = ttnn.from_device(ttnn_reshape_149, )
  ttnn_to_layout_260 = ttnn.to_layout(ttnn_from_device_257, ttnn.TILE_LAYOUT, )
  ttnn_to_device_122 = ttnn.to_device(ttnn_to_layout_260, device = device)
  ttnn_matmul_53 = ttnn.matmul(ttnn_to_device_121, ttnn_to_device_122, )
  ttnn_from_device_258 = ttnn.from_device(ttnn_matmul_53, )
  ttnn_to_layout_261 = ttnn.to_layout(ttnn_from_device_258, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_150 = ttnn.reshape(ttnn_to_layout_261, (1, 12, 9, 64), )
  ttnn_from_device_259 = ttnn.from_device(ttnn_reshape_150, )
  ttnn_to_layout_262 = ttnn.to_layout(ttnn_from_device_259, ttnn.TILE_LAYOUT, )
  ttnn_to_device_123 = ttnn.to_device(ttnn_to_layout_262, device = device)
  ttnn_transpose_53 = ttnn.transpose(ttnn_to_device_123, 2, 1, )
  ttnn_prefix_clone_20 = clone_wrapper(ttnn_transpose_53, )
  ttnn_from_device_260 = ttnn.from_device(ttnn_prefix_clone_20, )
  ttnn_to_layout_263 = ttnn.to_layout(ttnn_from_device_260, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_151 = ttnn.reshape(ttnn_to_layout_263, (1, 9, 768), )
  ttnn_from_device_261 = ttnn.from_device(ttnn_reshape_151, )
  ttnn_to_layout_264 = ttnn.to_layout(ttnn_from_device_261, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_152 = ttnn.reshape(ttnn_to_layout_264, (9, 768), )
  ttnn_from_device_262 = ttnn.from_device(ttnn_reshape_152, )
  ttnn_to_layout_265 = ttnn.to_layout(ttnn_from_device_262, ttnn.TILE_LAYOUT, )
  ttnn_to_device_124 = ttnn.to_device(ttnn_to_layout_265, device = device)
  ttnn_matmul_54 = ttnn.matmul(ttnn_to_device_124, ttnn_transpose_6, )
  ttnn_add_40 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_54, )
  ttnn_from_device_263 = ttnn.from_device(ttnn_add_40, )
  ttnn_to_layout_266 = ttnn.to_layout(ttnn_from_device_263, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_153 = ttnn.reshape(ttnn_to_layout_266, (1, 9, 768), )
  ttnn_from_device_264 = ttnn.from_device(ttnn_reshape_153, )
  ttnn_to_layout_267 = ttnn.to_layout(ttnn_from_device_264, ttnn.TILE_LAYOUT, )
  ttnn_to_device_125 = ttnn.to_device(ttnn_to_layout_267, device = device)
  ttnn_prefix_clone_21 = clone_wrapper(ttnn_to_device_125, )
  ttnn_add_108 = ttnn.add(ttnn_layer_norm_12, ttnn_prefix_clone_21, )
  ttnn_layer_norm_13_ = ttnn.layer_norm(ttnn_add_108, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_13_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_108), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_13 = ttnn.from_torch(ttnn_layer_norm_13_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_13_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_13_), ttnn_layer_norm_13))
  ttnn_from_device_265 = ttnn.from_device(ttnn_layer_norm_13, )
  ttnn_to_layout_268 = ttnn.to_layout(ttnn_from_device_265, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_154 = ttnn.reshape(ttnn_to_layout_268, (9, 768), )
  ttnn_from_device_266 = ttnn.from_device(ttnn_reshape_154, )
  ttnn_to_layout_269 = ttnn.to_layout(ttnn_from_device_266, ttnn.TILE_LAYOUT, )
  ttnn_to_device_126 = ttnn.to_device(ttnn_to_layout_269, device = device)
  ttnn_matmul_55 = ttnn.matmul(ttnn_to_device_126, ttnn_transpose_7, )
  ttnn_add_41 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_55, )
  ttnn_from_device_267 = ttnn.from_device(ttnn_add_41, )
  ttnn_to_layout_270 = ttnn.to_layout(ttnn_from_device_267, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_155 = ttnn.reshape(ttnn_to_layout_270, (1, 9, 3072), )
  ttnn_from_device_268 = ttnn.from_device(ttnn_reshape_155, )
  ttnn_to_layout_271 = ttnn.to_layout(ttnn_from_device_268, ttnn.TILE_LAYOUT, )
  ttnn_to_device_127 = ttnn.to_device(ttnn_to_layout_271, device = device)
  ttnn_multiply_32 = ttnn.multiply(ttnn_to_device_127, 0.5, )
  ttnn_pow_6 = ttnn.pow(ttnn_to_device_127, 3.0, )
  ttnn_multiply_33 = ttnn.multiply(ttnn_pow_6, 0.044715, )
  ttnn_add_109 = ttnn.add(ttnn_to_device_127, ttnn_multiply_33, )
  ttnn_multiply_34 = ttnn.multiply(ttnn_add_109, 0.7978845608028654, )
  ttnn_tanh_6_ = ttnn.tanh(ttnn_multiply_34, )
  ttnn_tanh_6_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_34), )
  ttnn_tanh_6 = ttnn.from_torch(ttnn_tanh_6_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_6_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_6_), ttnn_tanh_6))
  test_accuracy(tanh_6, ttnn_tanh_6)
  ttnn_add_110 = ttnn.add(ttnn_tanh_6, 1.0, )
  ttnn_multiply_35 = ttnn.multiply(ttnn_multiply_32, ttnn_add_110, )
  ttnn_from_device_269 = ttnn.from_device(ttnn_multiply_35, )
  ttnn_to_layout_272 = ttnn.to_layout(ttnn_from_device_269, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_156 = ttnn.reshape(ttnn_to_layout_272, (9, 3072), )
  ttnn_from_device_270 = ttnn.from_device(ttnn_reshape_156, )
  ttnn_to_layout_273 = ttnn.to_layout(ttnn_from_device_270, ttnn.TILE_LAYOUT, )
  ttnn_to_device_128 = ttnn.to_device(ttnn_to_layout_273, device = device)
  ttnn_matmul_56 = ttnn.matmul(ttnn_to_device_128, ttnn_transpose_8, )
  ttnn_add_42 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_56, )
  ttnn_from_device_271 = ttnn.from_device(ttnn_add_42, )
  ttnn_to_layout_274 = ttnn.to_layout(ttnn_from_device_271, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_157 = ttnn.reshape(ttnn_to_layout_274, (1, 9, 768), )
  ttnn_from_device_272 = ttnn.from_device(ttnn_reshape_157, )
  ttnn_to_layout_275 = ttnn.to_layout(ttnn_from_device_272, ttnn.TILE_LAYOUT, )
  ttnn_to_device_129 = ttnn.to_device(ttnn_to_layout_275, device = device)
  ttnn_add_111 = ttnn.add(ttnn_to_device_129, ttnn_layer_norm_13, )
  ttnn_layer_norm_14_ = ttnn.layer_norm(ttnn_add_111, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_14_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_111), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_14 = ttnn.from_torch(ttnn_layer_norm_14_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_14_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_14_), ttnn_layer_norm_14))
  ttnn_from_device_273 = ttnn.from_device(ttnn_layer_norm_14, )
  ttnn_to_layout_276 = ttnn.to_layout(ttnn_from_device_273, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_158 = ttnn.reshape(ttnn_to_layout_276, (9, 768), )
  ttnn_from_device_274 = ttnn.from_device(ttnn_reshape_158, )
  ttnn_to_layout_277 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_130 = ttnn.to_device(ttnn_to_layout_277, device = device)
  ttnn_matmul_57 = ttnn.matmul(ttnn_to_device_130, ttnn_transpose_1, )
  ttnn_add_43 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_57, )
  ttnn_from_device_275 = ttnn.from_device(ttnn_add_43, )
  ttnn_to_layout_278 = ttnn.to_layout(ttnn_from_device_275, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_159 = ttnn.reshape(ttnn_to_layout_278, (1, 9, 768), )
  ttnn_to_layout_279 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_131 = ttnn.to_device(ttnn_to_layout_279, device = device)
  ttnn_matmul_58 = ttnn.matmul(ttnn_to_device_131, ttnn_transpose_2, )
  ttnn_add_44 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_58, )
  ttnn_from_device_277 = ttnn.from_device(ttnn_add_44, )
  ttnn_to_layout_280 = ttnn.to_layout(ttnn_from_device_277, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_161 = ttnn.reshape(ttnn_to_layout_280, (1, 9, 768), )
  ttnn_to_layout_281 = ttnn.to_layout(ttnn_from_device_274, ttnn.TILE_LAYOUT, )
  ttnn_to_device_132 = ttnn.to_device(ttnn_to_layout_281, device = device)
  ttnn_matmul_59 = ttnn.matmul(ttnn_to_device_132, ttnn_transpose_3, )
  ttnn_add_45 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_59, )
  ttnn_from_device_279 = ttnn.from_device(ttnn_add_45, )
  ttnn_to_layout_282 = ttnn.to_layout(ttnn_from_device_279, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_163 = ttnn.reshape(ttnn_to_layout_282, (1, 9, 768), )
  ttnn_from_device_280 = ttnn.from_device(ttnn_reshape_159, )
  ttnn_to_layout_283 = ttnn.to_layout(ttnn_from_device_280, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_164 = ttnn.reshape(ttnn_to_layout_283, (1, 9, 12, 64), )
  ttnn_from_device_281 = ttnn.from_device(ttnn_reshape_164, )
  ttnn_to_layout_284 = ttnn.to_layout(ttnn_from_device_281, ttnn.TILE_LAYOUT, )
  ttnn_to_device_133 = ttnn.to_device(ttnn_to_layout_284, device = device)
  ttnn_permute_21 = ttnn.permute(ttnn_to_device_133, (0, 2, 1, 3), )
  ttnn_from_device_282 = ttnn.from_device(ttnn_reshape_161, )
  ttnn_to_layout_285 = ttnn.to_layout(ttnn_from_device_282, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_165 = ttnn.reshape(ttnn_to_layout_285, (1, 9, 12, 64), )
  ttnn_from_device_283 = ttnn.from_device(ttnn_reshape_165, )
  ttnn_to_layout_286 = ttnn.to_layout(ttnn_from_device_283, ttnn.TILE_LAYOUT, )
  ttnn_to_device_134 = ttnn.to_device(ttnn_to_layout_286, device = device)
  ttnn_permute_22 = ttnn.permute(ttnn_to_device_134, (0, 2, 1, 3), )
  ttnn_from_device_284 = ttnn.from_device(ttnn_reshape_163, )
  ttnn_to_layout_287 = ttnn.to_layout(ttnn_from_device_284, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_166 = ttnn.reshape(ttnn_to_layout_287, (1, 9, 12, 64), )
  ttnn_from_device_285 = ttnn.from_device(ttnn_reshape_166, )
  ttnn_to_layout_288 = ttnn.to_layout(ttnn_from_device_285, ttnn.TILE_LAYOUT, )
  ttnn_to_device_135 = ttnn.to_device(ttnn_to_layout_288, device = device)
  ttnn_permute_23 = ttnn.permute(ttnn_to_device_135, (0, 2, 1, 3), )
  ttnn_transpose_60 = ttnn.transpose(ttnn_permute_22, 3, 2, )
  ttnn_from_device_286 = ttnn.from_device(ttnn_permute_21, )
  ttnn_to_layout_289 = ttnn.to_layout(ttnn_from_device_286, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_167 = ttnn.reshape(ttnn_to_layout_289, (12, 9, 64), )
  ttnn_from_device_287 = ttnn.from_device(ttnn_transpose_60, )
  ttnn_to_layout_290 = ttnn.to_layout(ttnn_from_device_287, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_168 = ttnn.reshape(ttnn_to_layout_290, (12, 64, 9), )
  ttnn_from_device_288 = ttnn.from_device(ttnn_reshape_167, )
  ttnn_to_layout_291 = ttnn.to_layout(ttnn_from_device_288, ttnn.TILE_LAYOUT, )
  ttnn_to_device_136 = ttnn.to_device(ttnn_to_layout_291, device = device)
  ttnn_from_device_289 = ttnn.from_device(ttnn_reshape_168, )
  ttnn_to_layout_292 = ttnn.to_layout(ttnn_from_device_289, ttnn.TILE_LAYOUT, )
  ttnn_to_device_137 = ttnn.to_device(ttnn_to_layout_292, device = device)
  ttnn_matmul_60 = ttnn.matmul(ttnn_to_device_136, ttnn_to_device_137, )
  ttnn_from_device_290 = ttnn.from_device(ttnn_matmul_60, )
  ttnn_to_layout_293 = ttnn.to_layout(ttnn_from_device_290, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_169 = ttnn.reshape(ttnn_to_layout_293, (1, 12, 9, 9), )
  ttnn_from_device_291 = ttnn.from_device(ttnn_reshape_169, )
  ttnn_to_layout_294 = ttnn.to_layout(ttnn_from_device_291, ttnn.TILE_LAYOUT, )
  ttnn_to_device_138 = ttnn.to_device(ttnn_to_layout_294, device = device)
  ttnn_multiply_36 = ttnn.multiply(ttnn_to_device_138, 0.125, )
  ttnn_add_112 = ttnn.add(ttnn_multiply_36, ttnn_multiply, )
  ttnn_softmax_7 = ttnn.softmax(ttnn_add_112, -1, numeric_stable = True)
  test_accuracy(_softmax_7, ttnn_softmax_7)
  ttnn_prefix_clone_22 = clone_wrapper(ttnn_softmax_7, )
  ttnn_from_device_292 = ttnn.from_device(ttnn_prefix_clone_22, )
  ttnn_to_layout_295 = ttnn.to_layout(ttnn_from_device_292, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_170 = ttnn.reshape(ttnn_to_layout_295, (12, 9, 9), )
  ttnn_from_device_293 = ttnn.from_device(ttnn_permute_23, )
  ttnn_to_layout_296 = ttnn.to_layout(ttnn_from_device_293, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_171 = ttnn.reshape(ttnn_to_layout_296, (12, 9, 64), )
  ttnn_from_device_294 = ttnn.from_device(ttnn_reshape_170, )
  ttnn_to_layout_297 = ttnn.to_layout(ttnn_from_device_294, ttnn.TILE_LAYOUT, )
  ttnn_to_device_139 = ttnn.to_device(ttnn_to_layout_297, device = device)
  ttnn_from_device_295 = ttnn.from_device(ttnn_reshape_171, )
  ttnn_to_layout_298 = ttnn.to_layout(ttnn_from_device_295, ttnn.TILE_LAYOUT, )
  ttnn_to_device_140 = ttnn.to_device(ttnn_to_layout_298, device = device)
  ttnn_matmul_61 = ttnn.matmul(ttnn_to_device_139, ttnn_to_device_140, )
  ttnn_from_device_296 = ttnn.from_device(ttnn_matmul_61, )
  ttnn_to_layout_299 = ttnn.to_layout(ttnn_from_device_296, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_172 = ttnn.reshape(ttnn_to_layout_299, (1, 12, 9, 64), )
  ttnn_from_device_297 = ttnn.from_device(ttnn_reshape_172, )
  ttnn_to_layout_300 = ttnn.to_layout(ttnn_from_device_297, ttnn.TILE_LAYOUT, )
  ttnn_to_device_141 = ttnn.to_device(ttnn_to_layout_300, device = device)
  ttnn_transpose_61 = ttnn.transpose(ttnn_to_device_141, 2, 1, )
  ttnn_prefix_clone_23 = clone_wrapper(ttnn_transpose_61, )
  ttnn_from_device_298 = ttnn.from_device(ttnn_prefix_clone_23, )
  ttnn_to_layout_301 = ttnn.to_layout(ttnn_from_device_298, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_173 = ttnn.reshape(ttnn_to_layout_301, (1, 9, 768), )
  ttnn_from_device_299 = ttnn.from_device(ttnn_reshape_173, )
  ttnn_to_layout_302 = ttnn.to_layout(ttnn_from_device_299, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_174 = ttnn.reshape(ttnn_to_layout_302, (9, 768), )
  ttnn_from_device_300 = ttnn.from_device(ttnn_reshape_174, )
  ttnn_to_layout_303 = ttnn.to_layout(ttnn_from_device_300, ttnn.TILE_LAYOUT, )
  ttnn_to_device_142 = ttnn.to_device(ttnn_to_layout_303, device = device)
  ttnn_matmul_62 = ttnn.matmul(ttnn_to_device_142, ttnn_transpose_6, )
  ttnn_add_46 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_62, )
  ttnn_from_device_301 = ttnn.from_device(ttnn_add_46, )
  ttnn_to_layout_304 = ttnn.to_layout(ttnn_from_device_301, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_175 = ttnn.reshape(ttnn_to_layout_304, (1, 9, 768), )
  ttnn_from_device_302 = ttnn.from_device(ttnn_reshape_175, )
  ttnn_to_layout_305 = ttnn.to_layout(ttnn_from_device_302, ttnn.TILE_LAYOUT, )
  ttnn_to_device_143 = ttnn.to_device(ttnn_to_layout_305, device = device)
  ttnn_prefix_clone_24 = clone_wrapper(ttnn_to_device_143, )
  ttnn_add_113 = ttnn.add(ttnn_layer_norm_14, ttnn_prefix_clone_24, )
  ttnn_layer_norm_15_ = ttnn.layer_norm(ttnn_add_113, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_15_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_113), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_15 = ttnn.from_torch(ttnn_layer_norm_15_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_15_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_15_), ttnn_layer_norm_15))
  ttnn_from_device_303 = ttnn.from_device(ttnn_layer_norm_15, )
  ttnn_to_layout_306 = ttnn.to_layout(ttnn_from_device_303, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_176 = ttnn.reshape(ttnn_to_layout_306, (9, 768), )
  ttnn_from_device_304 = ttnn.from_device(ttnn_reshape_176, )
  ttnn_to_layout_307 = ttnn.to_layout(ttnn_from_device_304, ttnn.TILE_LAYOUT, )
  ttnn_to_device_144 = ttnn.to_device(ttnn_to_layout_307, device = device)
  ttnn_matmul_63 = ttnn.matmul(ttnn_to_device_144, ttnn_transpose_7, )
  ttnn_add_47 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_63, )
  ttnn_from_device_305 = ttnn.from_device(ttnn_add_47, )
  ttnn_to_layout_308 = ttnn.to_layout(ttnn_from_device_305, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_177 = ttnn.reshape(ttnn_to_layout_308, (1, 9, 3072), )
  ttnn_from_device_306 = ttnn.from_device(ttnn_reshape_177, )
  ttnn_to_layout_309 = ttnn.to_layout(ttnn_from_device_306, ttnn.TILE_LAYOUT, )
  ttnn_to_device_145 = ttnn.to_device(ttnn_to_layout_309, device = device)
  ttnn_multiply_37 = ttnn.multiply(ttnn_to_device_145, 0.5, )
  ttnn_pow_7 = ttnn.pow(ttnn_to_device_145, 3.0, )
  ttnn_multiply_38 = ttnn.multiply(ttnn_pow_7, 0.044715, )
  ttnn_add_114 = ttnn.add(ttnn_to_device_145, ttnn_multiply_38, )
  ttnn_multiply_39 = ttnn.multiply(ttnn_add_114, 0.7978845608028654, )
  ttnn_tanh_7_ = ttnn.tanh(ttnn_multiply_39, )
  ttnn_tanh_7_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_39), )
  ttnn_tanh_7 = ttnn.from_torch(ttnn_tanh_7_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_7_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_7_), ttnn_tanh_7))
  test_accuracy(tanh_7, ttnn_tanh_7)
  ttnn_add_115 = ttnn.add(ttnn_tanh_7, 1.0, )
  ttnn_multiply_40 = ttnn.multiply(ttnn_multiply_37, ttnn_add_115, )
  ttnn_from_device_307 = ttnn.from_device(ttnn_multiply_40, )
  ttnn_to_layout_310 = ttnn.to_layout(ttnn_from_device_307, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_178 = ttnn.reshape(ttnn_to_layout_310, (9, 3072), )
  ttnn_from_device_308 = ttnn.from_device(ttnn_reshape_178, )
  ttnn_to_layout_311 = ttnn.to_layout(ttnn_from_device_308, ttnn.TILE_LAYOUT, )
  ttnn_to_device_146 = ttnn.to_device(ttnn_to_layout_311, device = device)
  ttnn_matmul_64 = ttnn.matmul(ttnn_to_device_146, ttnn_transpose_8, )
  ttnn_add_48 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_64, )
  ttnn_from_device_309 = ttnn.from_device(ttnn_add_48, )
  ttnn_to_layout_312 = ttnn.to_layout(ttnn_from_device_309, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_179 = ttnn.reshape(ttnn_to_layout_312, (1, 9, 768), )
  ttnn_from_device_310 = ttnn.from_device(ttnn_reshape_179, )
  ttnn_to_layout_313 = ttnn.to_layout(ttnn_from_device_310, ttnn.TILE_LAYOUT, )
  ttnn_to_device_147 = ttnn.to_device(ttnn_to_layout_313, device = device)
  ttnn_add_116 = ttnn.add(ttnn_to_device_147, ttnn_layer_norm_15, )
  ttnn_layer_norm_16_ = ttnn.layer_norm(ttnn_add_116, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_16_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_116), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_16 = ttnn.from_torch(ttnn_layer_norm_16_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_16_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_16_), ttnn_layer_norm_16))
  ttnn_from_device_311 = ttnn.from_device(ttnn_layer_norm_16, )
  ttnn_to_layout_314 = ttnn.to_layout(ttnn_from_device_311, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_180 = ttnn.reshape(ttnn_to_layout_314, (9, 768), )
  ttnn_from_device_312 = ttnn.from_device(ttnn_reshape_180, )
  ttnn_to_layout_315 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_148 = ttnn.to_device(ttnn_to_layout_315, device = device)
  ttnn_matmul_65 = ttnn.matmul(ttnn_to_device_148, ttnn_transpose_1, )
  ttnn_add_49 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_65, )
  ttnn_from_device_313 = ttnn.from_device(ttnn_add_49, )
  ttnn_to_layout_316 = ttnn.to_layout(ttnn_from_device_313, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_181 = ttnn.reshape(ttnn_to_layout_316, (1, 9, 768), )
  ttnn_to_layout_317 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_149 = ttnn.to_device(ttnn_to_layout_317, device = device)
  ttnn_matmul_66 = ttnn.matmul(ttnn_to_device_149, ttnn_transpose_2, )
  ttnn_add_50 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_66, )
  ttnn_from_device_315 = ttnn.from_device(ttnn_add_50, )
  ttnn_to_layout_318 = ttnn.to_layout(ttnn_from_device_315, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_183 = ttnn.reshape(ttnn_to_layout_318, (1, 9, 768), )
  ttnn_to_layout_319 = ttnn.to_layout(ttnn_from_device_312, ttnn.TILE_LAYOUT, )
  ttnn_to_device_150 = ttnn.to_device(ttnn_to_layout_319, device = device)
  ttnn_matmul_67 = ttnn.matmul(ttnn_to_device_150, ttnn_transpose_3, )
  ttnn_add_51 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_67, )
  ttnn_from_device_317 = ttnn.from_device(ttnn_add_51, )
  ttnn_to_layout_320 = ttnn.to_layout(ttnn_from_device_317, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_185 = ttnn.reshape(ttnn_to_layout_320, (1, 9, 768), )
  ttnn_from_device_318 = ttnn.from_device(ttnn_reshape_181, )
  ttnn_to_layout_321 = ttnn.to_layout(ttnn_from_device_318, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_186 = ttnn.reshape(ttnn_to_layout_321, (1, 9, 12, 64), )
  ttnn_from_device_319 = ttnn.from_device(ttnn_reshape_186, )
  ttnn_to_layout_322 = ttnn.to_layout(ttnn_from_device_319, ttnn.TILE_LAYOUT, )
  ttnn_to_device_151 = ttnn.to_device(ttnn_to_layout_322, device = device)
  ttnn_permute_24 = ttnn.permute(ttnn_to_device_151, (0, 2, 1, 3), )
  ttnn_from_device_320 = ttnn.from_device(ttnn_reshape_183, )
  ttnn_to_layout_323 = ttnn.to_layout(ttnn_from_device_320, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_187 = ttnn.reshape(ttnn_to_layout_323, (1, 9, 12, 64), )
  ttnn_from_device_321 = ttnn.from_device(ttnn_reshape_187, )
  ttnn_to_layout_324 = ttnn.to_layout(ttnn_from_device_321, ttnn.TILE_LAYOUT, )
  ttnn_to_device_152 = ttnn.to_device(ttnn_to_layout_324, device = device)
  ttnn_permute_25 = ttnn.permute(ttnn_to_device_152, (0, 2, 1, 3), )
  ttnn_from_device_322 = ttnn.from_device(ttnn_reshape_185, )
  ttnn_to_layout_325 = ttnn.to_layout(ttnn_from_device_322, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_188 = ttnn.reshape(ttnn_to_layout_325, (1, 9, 12, 64), )
  ttnn_from_device_323 = ttnn.from_device(ttnn_reshape_188, )
  ttnn_to_layout_326 = ttnn.to_layout(ttnn_from_device_323, ttnn.TILE_LAYOUT, )
  ttnn_to_device_153 = ttnn.to_device(ttnn_to_layout_326, device = device)
  ttnn_permute_26 = ttnn.permute(ttnn_to_device_153, (0, 2, 1, 3), )
  ttnn_transpose_68 = ttnn.transpose(ttnn_permute_25, 3, 2, )
  ttnn_from_device_324 = ttnn.from_device(ttnn_permute_24, )
  ttnn_to_layout_327 = ttnn.to_layout(ttnn_from_device_324, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_189 = ttnn.reshape(ttnn_to_layout_327, (12, 9, 64), )
  ttnn_from_device_325 = ttnn.from_device(ttnn_transpose_68, )
  ttnn_to_layout_328 = ttnn.to_layout(ttnn_from_device_325, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_190 = ttnn.reshape(ttnn_to_layout_328, (12, 64, 9), )
  ttnn_from_device_326 = ttnn.from_device(ttnn_reshape_189, )
  ttnn_to_layout_329 = ttnn.to_layout(ttnn_from_device_326, ttnn.TILE_LAYOUT, )
  ttnn_to_device_154 = ttnn.to_device(ttnn_to_layout_329, device = device)
  ttnn_from_device_327 = ttnn.from_device(ttnn_reshape_190, )
  ttnn_to_layout_330 = ttnn.to_layout(ttnn_from_device_327, ttnn.TILE_LAYOUT, )
  ttnn_to_device_155 = ttnn.to_device(ttnn_to_layout_330, device = device)
  ttnn_matmul_68 = ttnn.matmul(ttnn_to_device_154, ttnn_to_device_155, )
  ttnn_from_device_328 = ttnn.from_device(ttnn_matmul_68, )
  ttnn_to_layout_331 = ttnn.to_layout(ttnn_from_device_328, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_191 = ttnn.reshape(ttnn_to_layout_331, (1, 12, 9, 9), )
  ttnn_from_device_329 = ttnn.from_device(ttnn_reshape_191, )
  ttnn_to_layout_332 = ttnn.to_layout(ttnn_from_device_329, ttnn.TILE_LAYOUT, )
  ttnn_to_device_156 = ttnn.to_device(ttnn_to_layout_332, device = device)
  ttnn_multiply_41 = ttnn.multiply(ttnn_to_device_156, 0.125, )
  ttnn_add_117 = ttnn.add(ttnn_multiply_41, ttnn_multiply, )
  ttnn_softmax_8 = ttnn.softmax(ttnn_add_117, -1, numeric_stable = True)
  test_accuracy(_softmax_8, ttnn_softmax_8)
  ttnn_prefix_clone_25 = clone_wrapper(ttnn_softmax_8, )
  ttnn_from_device_330 = ttnn.from_device(ttnn_prefix_clone_25, )
  ttnn_to_layout_333 = ttnn.to_layout(ttnn_from_device_330, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_192 = ttnn.reshape(ttnn_to_layout_333, (12, 9, 9), )
  ttnn_from_device_331 = ttnn.from_device(ttnn_permute_26, )
  ttnn_to_layout_334 = ttnn.to_layout(ttnn_from_device_331, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_193 = ttnn.reshape(ttnn_to_layout_334, (12, 9, 64), )
  ttnn_from_device_332 = ttnn.from_device(ttnn_reshape_192, )
  ttnn_to_layout_335 = ttnn.to_layout(ttnn_from_device_332, ttnn.TILE_LAYOUT, )
  ttnn_to_device_157 = ttnn.to_device(ttnn_to_layout_335, device = device)
  ttnn_from_device_333 = ttnn.from_device(ttnn_reshape_193, )
  ttnn_to_layout_336 = ttnn.to_layout(ttnn_from_device_333, ttnn.TILE_LAYOUT, )
  ttnn_to_device_158 = ttnn.to_device(ttnn_to_layout_336, device = device)
  ttnn_matmul_69 = ttnn.matmul(ttnn_to_device_157, ttnn_to_device_158, )
  ttnn_from_device_334 = ttnn.from_device(ttnn_matmul_69, )
  ttnn_to_layout_337 = ttnn.to_layout(ttnn_from_device_334, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_194 = ttnn.reshape(ttnn_to_layout_337, (1, 12, 9, 64), )
  ttnn_from_device_335 = ttnn.from_device(ttnn_reshape_194, )
  ttnn_to_layout_338 = ttnn.to_layout(ttnn_from_device_335, ttnn.TILE_LAYOUT, )
  ttnn_to_device_159 = ttnn.to_device(ttnn_to_layout_338, device = device)
  ttnn_transpose_69 = ttnn.transpose(ttnn_to_device_159, 2, 1, )
  ttnn_prefix_clone_26 = clone_wrapper(ttnn_transpose_69, )
  ttnn_from_device_336 = ttnn.from_device(ttnn_prefix_clone_26, )
  ttnn_to_layout_339 = ttnn.to_layout(ttnn_from_device_336, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_195 = ttnn.reshape(ttnn_to_layout_339, (1, 9, 768), )
  ttnn_from_device_337 = ttnn.from_device(ttnn_reshape_195, )
  ttnn_to_layout_340 = ttnn.to_layout(ttnn_from_device_337, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_196 = ttnn.reshape(ttnn_to_layout_340, (9, 768), )
  ttnn_from_device_338 = ttnn.from_device(ttnn_reshape_196, )
  ttnn_to_layout_341 = ttnn.to_layout(ttnn_from_device_338, ttnn.TILE_LAYOUT, )
  ttnn_to_device_160 = ttnn.to_device(ttnn_to_layout_341, device = device)
  ttnn_matmul_70 = ttnn.matmul(ttnn_to_device_160, ttnn_transpose_6, )
  ttnn_add_52 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_70, )
  ttnn_from_device_339 = ttnn.from_device(ttnn_add_52, )
  ttnn_to_layout_342 = ttnn.to_layout(ttnn_from_device_339, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_197 = ttnn.reshape(ttnn_to_layout_342, (1, 9, 768), )
  ttnn_from_device_340 = ttnn.from_device(ttnn_reshape_197, )
  ttnn_to_layout_343 = ttnn.to_layout(ttnn_from_device_340, ttnn.TILE_LAYOUT, )
  ttnn_to_device_161 = ttnn.to_device(ttnn_to_layout_343, device = device)
  ttnn_prefix_clone_27 = clone_wrapper(ttnn_to_device_161, )
  ttnn_add_118 = ttnn.add(ttnn_layer_norm_16, ttnn_prefix_clone_27, )
  ttnn_layer_norm_17_ = ttnn.layer_norm(ttnn_add_118, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_17_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_118), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_17 = ttnn.from_torch(ttnn_layer_norm_17_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_17_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_17_), ttnn_layer_norm_17))
  ttnn_from_device_341 = ttnn.from_device(ttnn_layer_norm_17, )
  ttnn_to_layout_344 = ttnn.to_layout(ttnn_from_device_341, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_198 = ttnn.reshape(ttnn_to_layout_344, (9, 768), )
  ttnn_from_device_342 = ttnn.from_device(ttnn_reshape_198, )
  ttnn_to_layout_345 = ttnn.to_layout(ttnn_from_device_342, ttnn.TILE_LAYOUT, )
  ttnn_to_device_162 = ttnn.to_device(ttnn_to_layout_345, device = device)
  ttnn_matmul_71 = ttnn.matmul(ttnn_to_device_162, ttnn_transpose_7, )
  ttnn_add_53 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_71, )
  ttnn_from_device_343 = ttnn.from_device(ttnn_add_53, )
  ttnn_to_layout_346 = ttnn.to_layout(ttnn_from_device_343, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_199 = ttnn.reshape(ttnn_to_layout_346, (1, 9, 3072), )
  ttnn_from_device_344 = ttnn.from_device(ttnn_reshape_199, )
  ttnn_to_layout_347 = ttnn.to_layout(ttnn_from_device_344, ttnn.TILE_LAYOUT, )
  ttnn_to_device_163 = ttnn.to_device(ttnn_to_layout_347, device = device)
  ttnn_multiply_42 = ttnn.multiply(ttnn_to_device_163, 0.5, )
  ttnn_pow_8 = ttnn.pow(ttnn_to_device_163, 3.0, )
  ttnn_multiply_43 = ttnn.multiply(ttnn_pow_8, 0.044715, )
  ttnn_add_119 = ttnn.add(ttnn_to_device_163, ttnn_multiply_43, )
  ttnn_multiply_44 = ttnn.multiply(ttnn_add_119, 0.7978845608028654, )
  ttnn_tanh_8_ = ttnn.tanh(ttnn_multiply_44, )
  ttnn_tanh_8_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_44), )
  ttnn_tanh_8 = ttnn.from_torch(ttnn_tanh_8_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_8_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_8_), ttnn_tanh_8))
  test_accuracy(tanh_8, ttnn_tanh_8)
  ttnn_add_120 = ttnn.add(ttnn_tanh_8, 1.0, )
  ttnn_multiply_45 = ttnn.multiply(ttnn_multiply_42, ttnn_add_120, )
  ttnn_from_device_345 = ttnn.from_device(ttnn_multiply_45, )
  ttnn_to_layout_348 = ttnn.to_layout(ttnn_from_device_345, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_200 = ttnn.reshape(ttnn_to_layout_348, (9, 3072), )
  ttnn_from_device_346 = ttnn.from_device(ttnn_reshape_200, )
  ttnn_to_layout_349 = ttnn.to_layout(ttnn_from_device_346, ttnn.TILE_LAYOUT, )
  ttnn_to_device_164 = ttnn.to_device(ttnn_to_layout_349, device = device)
  ttnn_matmul_72 = ttnn.matmul(ttnn_to_device_164, ttnn_transpose_8, )
  ttnn_add_54 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_72, )
  ttnn_from_device_347 = ttnn.from_device(ttnn_add_54, )
  ttnn_to_layout_350 = ttnn.to_layout(ttnn_from_device_347, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_201 = ttnn.reshape(ttnn_to_layout_350, (1, 9, 768), )
  ttnn_from_device_348 = ttnn.from_device(ttnn_reshape_201, )
  ttnn_to_layout_351 = ttnn.to_layout(ttnn_from_device_348, ttnn.TILE_LAYOUT, )
  ttnn_to_device_165 = ttnn.to_device(ttnn_to_layout_351, device = device)
  ttnn_add_121 = ttnn.add(ttnn_to_device_165, ttnn_layer_norm_17, )
  ttnn_layer_norm_18_ = ttnn.layer_norm(ttnn_add_121, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_18_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_121), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_18 = ttnn.from_torch(ttnn_layer_norm_18_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_18_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_18_), ttnn_layer_norm_18))
  ttnn_from_device_349 = ttnn.from_device(ttnn_layer_norm_18, )
  ttnn_to_layout_352 = ttnn.to_layout(ttnn_from_device_349, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_202 = ttnn.reshape(ttnn_to_layout_352, (9, 768), )
  ttnn_from_device_350 = ttnn.from_device(ttnn_reshape_202, )
  ttnn_to_layout_353 = ttnn.to_layout(ttnn_from_device_350, ttnn.TILE_LAYOUT, )
  ttnn_to_device_166 = ttnn.to_device(ttnn_to_layout_353, device = device)
  ttnn_matmul_73 = ttnn.matmul(ttnn_to_device_166, ttnn_transpose_1, )
  ttnn_add_55 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_73, )
  ttnn_from_device_351 = ttnn.from_device(ttnn_add_55, )
  ttnn_to_layout_354 = ttnn.to_layout(ttnn_from_device_351, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_203 = ttnn.reshape(ttnn_to_layout_354, (1, 9, 768), )
  ttnn_to_layout_355 = ttnn.to_layout(ttnn_from_device_350, ttnn.TILE_LAYOUT, )
  ttnn_to_device_167 = ttnn.to_device(ttnn_to_layout_355, device = device)
  ttnn_matmul_74 = ttnn.matmul(ttnn_to_device_167, ttnn_transpose_2, )
  ttnn_add_56 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_74, )
  ttnn_from_device_353 = ttnn.from_device(ttnn_add_56, )
  ttnn_to_layout_356 = ttnn.to_layout(ttnn_from_device_353, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_205 = ttnn.reshape(ttnn_to_layout_356, (1, 9, 768), )
  ttnn_to_layout_357 = ttnn.to_layout(ttnn_from_device_350, ttnn.TILE_LAYOUT, )
  ttnn_to_device_168 = ttnn.to_device(ttnn_to_layout_357, device = device)
  ttnn_matmul_75 = ttnn.matmul(ttnn_to_device_168, ttnn_transpose_3, )
  ttnn_add_57 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_75, )
  ttnn_from_device_355 = ttnn.from_device(ttnn_add_57, )
  ttnn_to_layout_358 = ttnn.to_layout(ttnn_from_device_355, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_207 = ttnn.reshape(ttnn_to_layout_358, (1, 9, 768), )
  ttnn_from_device_356 = ttnn.from_device(ttnn_reshape_203, )
  ttnn_to_layout_359 = ttnn.to_layout(ttnn_from_device_356, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_208 = ttnn.reshape(ttnn_to_layout_359, (1, 9, 12, 64), )
  ttnn_from_device_357 = ttnn.from_device(ttnn_reshape_208, )
  ttnn_to_layout_360 = ttnn.to_layout(ttnn_from_device_357, ttnn.TILE_LAYOUT, )
  ttnn_to_device_169 = ttnn.to_device(ttnn_to_layout_360, device = device)
  ttnn_permute_27 = ttnn.permute(ttnn_to_device_169, (0, 2, 1, 3), )
  ttnn_from_device_358 = ttnn.from_device(ttnn_reshape_205, )
  ttnn_to_layout_361 = ttnn.to_layout(ttnn_from_device_358, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_209 = ttnn.reshape(ttnn_to_layout_361, (1, 9, 12, 64), )
  ttnn_from_device_359 = ttnn.from_device(ttnn_reshape_209, )
  ttnn_to_layout_362 = ttnn.to_layout(ttnn_from_device_359, ttnn.TILE_LAYOUT, )
  ttnn_to_device_170 = ttnn.to_device(ttnn_to_layout_362, device = device)
  ttnn_permute_28 = ttnn.permute(ttnn_to_device_170, (0, 2, 1, 3), )
  ttnn_from_device_360 = ttnn.from_device(ttnn_reshape_207, )
  ttnn_to_layout_363 = ttnn.to_layout(ttnn_from_device_360, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_210 = ttnn.reshape(ttnn_to_layout_363, (1, 9, 12, 64), )
  ttnn_from_device_361 = ttnn.from_device(ttnn_reshape_210, )
  ttnn_to_layout_364 = ttnn.to_layout(ttnn_from_device_361, ttnn.TILE_LAYOUT, )
  ttnn_to_device_171 = ttnn.to_device(ttnn_to_layout_364, device = device)
  ttnn_permute_29 = ttnn.permute(ttnn_to_device_171, (0, 2, 1, 3), )
  ttnn_transpose_76 = ttnn.transpose(ttnn_permute_28, 3, 2, )
  ttnn_from_device_362 = ttnn.from_device(ttnn_permute_27, )
  ttnn_to_layout_365 = ttnn.to_layout(ttnn_from_device_362, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_211 = ttnn.reshape(ttnn_to_layout_365, (12, 9, 64), )
  ttnn_from_device_363 = ttnn.from_device(ttnn_transpose_76, )
  ttnn_to_layout_366 = ttnn.to_layout(ttnn_from_device_363, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_212 = ttnn.reshape(ttnn_to_layout_366, (12, 64, 9), )
  ttnn_from_device_364 = ttnn.from_device(ttnn_reshape_211, )
  ttnn_to_layout_367 = ttnn.to_layout(ttnn_from_device_364, ttnn.TILE_LAYOUT, )
  ttnn_to_device_172 = ttnn.to_device(ttnn_to_layout_367, device = device)
  ttnn_from_device_365 = ttnn.from_device(ttnn_reshape_212, )
  ttnn_to_layout_368 = ttnn.to_layout(ttnn_from_device_365, ttnn.TILE_LAYOUT, )
  ttnn_to_device_173 = ttnn.to_device(ttnn_to_layout_368, device = device)
  ttnn_matmul_76 = ttnn.matmul(ttnn_to_device_172, ttnn_to_device_173, )
  ttnn_from_device_366 = ttnn.from_device(ttnn_matmul_76, )
  ttnn_to_layout_369 = ttnn.to_layout(ttnn_from_device_366, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_213 = ttnn.reshape(ttnn_to_layout_369, (1, 12, 9, 9), )
  ttnn_from_device_367 = ttnn.from_device(ttnn_reshape_213, )
  ttnn_to_layout_370 = ttnn.to_layout(ttnn_from_device_367, ttnn.TILE_LAYOUT, )
  ttnn_to_device_174 = ttnn.to_device(ttnn_to_layout_370, device = device)
  ttnn_multiply_46 = ttnn.multiply(ttnn_to_device_174, 0.125, )
  ttnn_add_122 = ttnn.add(ttnn_multiply_46, ttnn_multiply, )
  ttnn_softmax_9 = ttnn.softmax(ttnn_add_122, -1, numeric_stable = True)
  test_accuracy(_softmax_9, ttnn_softmax_9)
  ttnn_prefix_clone_28 = clone_wrapper(ttnn_softmax_9, )
  ttnn_from_device_368 = ttnn.from_device(ttnn_prefix_clone_28, )
  ttnn_to_layout_371 = ttnn.to_layout(ttnn_from_device_368, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_214 = ttnn.reshape(ttnn_to_layout_371, (12, 9, 9), )
  ttnn_from_device_369 = ttnn.from_device(ttnn_permute_29, )
  ttnn_to_layout_372 = ttnn.to_layout(ttnn_from_device_369, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_215 = ttnn.reshape(ttnn_to_layout_372, (12, 9, 64), )
  ttnn_from_device_370 = ttnn.from_device(ttnn_reshape_214, )
  ttnn_to_layout_373 = ttnn.to_layout(ttnn_from_device_370, ttnn.TILE_LAYOUT, )
  ttnn_to_device_175 = ttnn.to_device(ttnn_to_layout_373, device = device)
  ttnn_from_device_371 = ttnn.from_device(ttnn_reshape_215, )
  ttnn_to_layout_374 = ttnn.to_layout(ttnn_from_device_371, ttnn.TILE_LAYOUT, )
  ttnn_to_device_176 = ttnn.to_device(ttnn_to_layout_374, device = device)
  ttnn_matmul_77 = ttnn.matmul(ttnn_to_device_175, ttnn_to_device_176, )
  ttnn_from_device_372 = ttnn.from_device(ttnn_matmul_77, )
  ttnn_to_layout_375 = ttnn.to_layout(ttnn_from_device_372, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_216 = ttnn.reshape(ttnn_to_layout_375, (1, 12, 9, 64), )
  ttnn_from_device_373 = ttnn.from_device(ttnn_reshape_216, )
  ttnn_to_layout_376 = ttnn.to_layout(ttnn_from_device_373, ttnn.TILE_LAYOUT, )
  ttnn_to_device_177 = ttnn.to_device(ttnn_to_layout_376, device = device)
  ttnn_transpose_77 = ttnn.transpose(ttnn_to_device_177, 2, 1, )
  ttnn_prefix_clone_29 = clone_wrapper(ttnn_transpose_77, )
  ttnn_from_device_374 = ttnn.from_device(ttnn_prefix_clone_29, )
  ttnn_to_layout_377 = ttnn.to_layout(ttnn_from_device_374, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_217 = ttnn.reshape(ttnn_to_layout_377, (1, 9, 768), )
  ttnn_from_device_375 = ttnn.from_device(ttnn_reshape_217, )
  ttnn_to_layout_378 = ttnn.to_layout(ttnn_from_device_375, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_218 = ttnn.reshape(ttnn_to_layout_378, (9, 768), )
  ttnn_from_device_376 = ttnn.from_device(ttnn_reshape_218, )
  ttnn_to_layout_379 = ttnn.to_layout(ttnn_from_device_376, ttnn.TILE_LAYOUT, )
  ttnn_to_device_178 = ttnn.to_device(ttnn_to_layout_379, device = device)
  ttnn_matmul_78 = ttnn.matmul(ttnn_to_device_178, ttnn_transpose_6, )
  ttnn_add_58 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_78, )
  ttnn_from_device_377 = ttnn.from_device(ttnn_add_58, )
  ttnn_to_layout_380 = ttnn.to_layout(ttnn_from_device_377, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_219 = ttnn.reshape(ttnn_to_layout_380, (1, 9, 768), )
  ttnn_from_device_378 = ttnn.from_device(ttnn_reshape_219, )
  ttnn_to_layout_381 = ttnn.to_layout(ttnn_from_device_378, ttnn.TILE_LAYOUT, )
  ttnn_to_device_179 = ttnn.to_device(ttnn_to_layout_381, device = device)
  ttnn_prefix_clone_30 = clone_wrapper(ttnn_to_device_179, )
  ttnn_add_123 = ttnn.add(ttnn_layer_norm_18, ttnn_prefix_clone_30, )
  ttnn_layer_norm_19_ = ttnn.layer_norm(ttnn_add_123, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_19_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_123), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_19 = ttnn.from_torch(ttnn_layer_norm_19_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_19_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_19_), ttnn_layer_norm_19))
  ttnn_from_device_379 = ttnn.from_device(ttnn_layer_norm_19, )
  ttnn_to_layout_382 = ttnn.to_layout(ttnn_from_device_379, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_220 = ttnn.reshape(ttnn_to_layout_382, (9, 768), )
  ttnn_from_device_380 = ttnn.from_device(ttnn_reshape_220, )
  ttnn_to_layout_383 = ttnn.to_layout(ttnn_from_device_380, ttnn.TILE_LAYOUT, )
  ttnn_to_device_180 = ttnn.to_device(ttnn_to_layout_383, device = device)
  ttnn_matmul_79 = ttnn.matmul(ttnn_to_device_180, ttnn_transpose_7, )
  ttnn_add_59 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_79, )
  ttnn_from_device_381 = ttnn.from_device(ttnn_add_59, )
  ttnn_to_layout_384 = ttnn.to_layout(ttnn_from_device_381, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_221 = ttnn.reshape(ttnn_to_layout_384, (1, 9, 3072), )
  ttnn_from_device_382 = ttnn.from_device(ttnn_reshape_221, )
  ttnn_to_layout_385 = ttnn.to_layout(ttnn_from_device_382, ttnn.TILE_LAYOUT, )
  ttnn_to_device_181 = ttnn.to_device(ttnn_to_layout_385, device = device)
  ttnn_multiply_47 = ttnn.multiply(ttnn_to_device_181, 0.5, )
  ttnn_pow_9 = ttnn.pow(ttnn_to_device_181, 3.0, )
  ttnn_multiply_48 = ttnn.multiply(ttnn_pow_9, 0.044715, )
  ttnn_add_124 = ttnn.add(ttnn_to_device_181, ttnn_multiply_48, )
  ttnn_multiply_49 = ttnn.multiply(ttnn_add_124, 0.7978845608028654, )
  ttnn_tanh_9_ = ttnn.tanh(ttnn_multiply_49, )
  ttnn_tanh_9_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_49), )
  ttnn_tanh_9 = ttnn.from_torch(ttnn_tanh_9_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_9_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_9_), ttnn_tanh_9))
  test_accuracy(tanh_9, ttnn_tanh_9)
  ttnn_add_125 = ttnn.add(ttnn_tanh_9, 1.0, )
  ttnn_multiply_50 = ttnn.multiply(ttnn_multiply_47, ttnn_add_125, )
  ttnn_from_device_383 = ttnn.from_device(ttnn_multiply_50, )
  ttnn_to_layout_386 = ttnn.to_layout(ttnn_from_device_383, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_222 = ttnn.reshape(ttnn_to_layout_386, (9, 3072), )
  ttnn_from_device_384 = ttnn.from_device(ttnn_reshape_222, )
  ttnn_to_layout_387 = ttnn.to_layout(ttnn_from_device_384, ttnn.TILE_LAYOUT, )
  ttnn_to_device_182 = ttnn.to_device(ttnn_to_layout_387, device = device)
  ttnn_matmul_80 = ttnn.matmul(ttnn_to_device_182, ttnn_transpose_8, )
  ttnn_add_60 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_80, )
  ttnn_from_device_385 = ttnn.from_device(ttnn_add_60, )
  ttnn_to_layout_388 = ttnn.to_layout(ttnn_from_device_385, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_223 = ttnn.reshape(ttnn_to_layout_388, (1, 9, 768), )
  ttnn_from_device_386 = ttnn.from_device(ttnn_reshape_223, )
  ttnn_to_layout_389 = ttnn.to_layout(ttnn_from_device_386, ttnn.TILE_LAYOUT, )
  ttnn_to_device_183 = ttnn.to_device(ttnn_to_layout_389, device = device)
  ttnn_add_126 = ttnn.add(ttnn_to_device_183, ttnn_layer_norm_19, )
  ttnn_layer_norm_20_ = ttnn.layer_norm(ttnn_add_126, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_20_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_126), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_20 = ttnn.from_torch(ttnn_layer_norm_20_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_20_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_20_), ttnn_layer_norm_20))
  ttnn_from_device_387 = ttnn.from_device(ttnn_layer_norm_20, )
  ttnn_to_layout_390 = ttnn.to_layout(ttnn_from_device_387, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_224 = ttnn.reshape(ttnn_to_layout_390, (9, 768), )
  ttnn_from_device_388 = ttnn.from_device(ttnn_reshape_224, )
  ttnn_to_layout_391 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_184 = ttnn.to_device(ttnn_to_layout_391, device = device)
  ttnn_matmul_81 = ttnn.matmul(ttnn_to_device_184, ttnn_transpose_1, )
  ttnn_add_61 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_81, )
  ttnn_from_device_389 = ttnn.from_device(ttnn_add_61, )
  ttnn_to_layout_392 = ttnn.to_layout(ttnn_from_device_389, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_225 = ttnn.reshape(ttnn_to_layout_392, (1, 9, 768), )
  ttnn_to_layout_393 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_185 = ttnn.to_device(ttnn_to_layout_393, device = device)
  ttnn_matmul_82 = ttnn.matmul(ttnn_to_device_185, ttnn_transpose_2, )
  ttnn_add_62 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_82, )
  ttnn_from_device_391 = ttnn.from_device(ttnn_add_62, )
  ttnn_to_layout_394 = ttnn.to_layout(ttnn_from_device_391, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_227 = ttnn.reshape(ttnn_to_layout_394, (1, 9, 768), )
  ttnn_to_layout_395 = ttnn.to_layout(ttnn_from_device_388, ttnn.TILE_LAYOUT, )
  ttnn_to_device_186 = ttnn.to_device(ttnn_to_layout_395, device = device)
  ttnn_matmul_83 = ttnn.matmul(ttnn_to_device_186, ttnn_transpose_3, )
  ttnn_add_63 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_83, )
  ttnn_from_device_393 = ttnn.from_device(ttnn_add_63, )
  ttnn_to_layout_396 = ttnn.to_layout(ttnn_from_device_393, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_229 = ttnn.reshape(ttnn_to_layout_396, (1, 9, 768), )
  ttnn_from_device_394 = ttnn.from_device(ttnn_reshape_225, )
  ttnn_to_layout_397 = ttnn.to_layout(ttnn_from_device_394, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_230 = ttnn.reshape(ttnn_to_layout_397, (1, 9, 12, 64), )
  ttnn_from_device_395 = ttnn.from_device(ttnn_reshape_230, )
  ttnn_to_layout_398 = ttnn.to_layout(ttnn_from_device_395, ttnn.TILE_LAYOUT, )
  ttnn_to_device_187 = ttnn.to_device(ttnn_to_layout_398, device = device)
  ttnn_permute_30 = ttnn.permute(ttnn_to_device_187, (0, 2, 1, 3), )
  ttnn_from_device_396 = ttnn.from_device(ttnn_reshape_227, )
  ttnn_to_layout_399 = ttnn.to_layout(ttnn_from_device_396, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_231 = ttnn.reshape(ttnn_to_layout_399, (1, 9, 12, 64), )
  ttnn_from_device_397 = ttnn.from_device(ttnn_reshape_231, )
  ttnn_to_layout_400 = ttnn.to_layout(ttnn_from_device_397, ttnn.TILE_LAYOUT, )
  ttnn_to_device_188 = ttnn.to_device(ttnn_to_layout_400, device = device)
  ttnn_permute_31 = ttnn.permute(ttnn_to_device_188, (0, 2, 1, 3), )
  ttnn_from_device_398 = ttnn.from_device(ttnn_reshape_229, )
  ttnn_to_layout_401 = ttnn.to_layout(ttnn_from_device_398, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_232 = ttnn.reshape(ttnn_to_layout_401, (1, 9, 12, 64), )
  ttnn_from_device_399 = ttnn.from_device(ttnn_reshape_232, )
  ttnn_to_layout_402 = ttnn.to_layout(ttnn_from_device_399, ttnn.TILE_LAYOUT, )
  ttnn_to_device_189 = ttnn.to_device(ttnn_to_layout_402, device = device)
  ttnn_permute_32 = ttnn.permute(ttnn_to_device_189, (0, 2, 1, 3), )
  ttnn_transpose_84 = ttnn.transpose(ttnn_permute_31, 3, 2, )
  ttnn_from_device_400 = ttnn.from_device(ttnn_permute_30, )
  ttnn_to_layout_403 = ttnn.to_layout(ttnn_from_device_400, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_233 = ttnn.reshape(ttnn_to_layout_403, (12, 9, 64), )
  ttnn_from_device_401 = ttnn.from_device(ttnn_transpose_84, )
  ttnn_to_layout_404 = ttnn.to_layout(ttnn_from_device_401, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_234 = ttnn.reshape(ttnn_to_layout_404, (12, 64, 9), )
  ttnn_from_device_402 = ttnn.from_device(ttnn_reshape_233, )
  ttnn_to_layout_405 = ttnn.to_layout(ttnn_from_device_402, ttnn.TILE_LAYOUT, )
  ttnn_to_device_190 = ttnn.to_device(ttnn_to_layout_405, device = device)
  ttnn_from_device_403 = ttnn.from_device(ttnn_reshape_234, )
  ttnn_to_layout_406 = ttnn.to_layout(ttnn_from_device_403, ttnn.TILE_LAYOUT, )
  ttnn_to_device_191 = ttnn.to_device(ttnn_to_layout_406, device = device)
  ttnn_matmul_84 = ttnn.matmul(ttnn_to_device_190, ttnn_to_device_191, )
  ttnn_from_device_404 = ttnn.from_device(ttnn_matmul_84, )
  ttnn_to_layout_407 = ttnn.to_layout(ttnn_from_device_404, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_235 = ttnn.reshape(ttnn_to_layout_407, (1, 12, 9, 9), )
  ttnn_from_device_405 = ttnn.from_device(ttnn_reshape_235, )
  ttnn_to_layout_408 = ttnn.to_layout(ttnn_from_device_405, ttnn.TILE_LAYOUT, )
  ttnn_to_device_192 = ttnn.to_device(ttnn_to_layout_408, device = device)
  ttnn_multiply_51 = ttnn.multiply(ttnn_to_device_192, 0.125, )
  ttnn_add_127 = ttnn.add(ttnn_multiply_51, ttnn_multiply, )
  ttnn_softmax_10 = ttnn.softmax(ttnn_add_127, -1, numeric_stable = True)
  test_accuracy(_softmax_10, ttnn_softmax_10)
  ttnn_prefix_clone_31 = clone_wrapper(ttnn_softmax_10, )
  ttnn_from_device_406 = ttnn.from_device(ttnn_prefix_clone_31, )
  ttnn_to_layout_409 = ttnn.to_layout(ttnn_from_device_406, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_236 = ttnn.reshape(ttnn_to_layout_409, (12, 9, 9), )
  ttnn_from_device_407 = ttnn.from_device(ttnn_permute_32, )
  ttnn_to_layout_410 = ttnn.to_layout(ttnn_from_device_407, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_237 = ttnn.reshape(ttnn_to_layout_410, (12, 9, 64), )
  ttnn_from_device_408 = ttnn.from_device(ttnn_reshape_236, )
  ttnn_to_layout_411 = ttnn.to_layout(ttnn_from_device_408, ttnn.TILE_LAYOUT, )
  ttnn_to_device_193 = ttnn.to_device(ttnn_to_layout_411, device = device)
  ttnn_from_device_409 = ttnn.from_device(ttnn_reshape_237, )
  ttnn_to_layout_412 = ttnn.to_layout(ttnn_from_device_409, ttnn.TILE_LAYOUT, )
  ttnn_to_device_194 = ttnn.to_device(ttnn_to_layout_412, device = device)
  ttnn_matmul_85 = ttnn.matmul(ttnn_to_device_193, ttnn_to_device_194, )
  ttnn_from_device_410 = ttnn.from_device(ttnn_matmul_85, )
  ttnn_to_layout_413 = ttnn.to_layout(ttnn_from_device_410, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_238 = ttnn.reshape(ttnn_to_layout_413, (1, 12, 9, 64), )
  ttnn_from_device_411 = ttnn.from_device(ttnn_reshape_238, )
  ttnn_to_layout_414 = ttnn.to_layout(ttnn_from_device_411, ttnn.TILE_LAYOUT, )
  ttnn_to_device_195 = ttnn.to_device(ttnn_to_layout_414, device = device)
  ttnn_transpose_85 = ttnn.transpose(ttnn_to_device_195, 2, 1, )
  ttnn_prefix_clone_32 = clone_wrapper(ttnn_transpose_85, )
  ttnn_from_device_412 = ttnn.from_device(ttnn_prefix_clone_32, )
  ttnn_to_layout_415 = ttnn.to_layout(ttnn_from_device_412, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_239 = ttnn.reshape(ttnn_to_layout_415, (1, 9, 768), )
  ttnn_from_device_413 = ttnn.from_device(ttnn_reshape_239, )
  ttnn_to_layout_416 = ttnn.to_layout(ttnn_from_device_413, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_240 = ttnn.reshape(ttnn_to_layout_416, (9, 768), )
  ttnn_from_device_414 = ttnn.from_device(ttnn_reshape_240, )
  ttnn_to_layout_417 = ttnn.to_layout(ttnn_from_device_414, ttnn.TILE_LAYOUT, )
  ttnn_to_device_196 = ttnn.to_device(ttnn_to_layout_417, device = device)
  ttnn_matmul_86 = ttnn.matmul(ttnn_to_device_196, ttnn_transpose_6, )
  ttnn_add_64 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_86, )
  ttnn_from_device_415 = ttnn.from_device(ttnn_add_64, )
  ttnn_to_layout_418 = ttnn.to_layout(ttnn_from_device_415, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_241 = ttnn.reshape(ttnn_to_layout_418, (1, 9, 768), )
  ttnn_from_device_416 = ttnn.from_device(ttnn_reshape_241, )
  ttnn_to_layout_419 = ttnn.to_layout(ttnn_from_device_416, ttnn.TILE_LAYOUT, )
  ttnn_to_device_197 = ttnn.to_device(ttnn_to_layout_419, device = device)
  ttnn_prefix_clone_33 = clone_wrapper(ttnn_to_device_197, )
  ttnn_add_128 = ttnn.add(ttnn_layer_norm_20, ttnn_prefix_clone_33, )
  ttnn_layer_norm_21_ = ttnn.layer_norm(ttnn_add_128, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_21_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_128), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_21 = ttnn.from_torch(ttnn_layer_norm_21_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_21_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_21_), ttnn_layer_norm_21))
  ttnn_from_device_417 = ttnn.from_device(ttnn_layer_norm_21, )
  ttnn_to_layout_420 = ttnn.to_layout(ttnn_from_device_417, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_242 = ttnn.reshape(ttnn_to_layout_420, (9, 768), )
  ttnn_from_device_418 = ttnn.from_device(ttnn_reshape_242, )
  ttnn_to_layout_421 = ttnn.to_layout(ttnn_from_device_418, ttnn.TILE_LAYOUT, )
  ttnn_to_device_198 = ttnn.to_device(ttnn_to_layout_421, device = device)
  ttnn_matmul_87 = ttnn.matmul(ttnn_to_device_198, ttnn_transpose_7, )
  ttnn_add_65 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_87, )
  ttnn_from_device_419 = ttnn.from_device(ttnn_add_65, )
  ttnn_to_layout_422 = ttnn.to_layout(ttnn_from_device_419, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_243 = ttnn.reshape(ttnn_to_layout_422, (1, 9, 3072), )
  ttnn_from_device_420 = ttnn.from_device(ttnn_reshape_243, )
  ttnn_to_layout_423 = ttnn.to_layout(ttnn_from_device_420, ttnn.TILE_LAYOUT, )
  ttnn_to_device_199 = ttnn.to_device(ttnn_to_layout_423, device = device)
  ttnn_multiply_52 = ttnn.multiply(ttnn_to_device_199, 0.5, )
  ttnn_pow_10 = ttnn.pow(ttnn_to_device_199, 3.0, )
  ttnn_multiply_53 = ttnn.multiply(ttnn_pow_10, 0.044715, )
  ttnn_add_129 = ttnn.add(ttnn_to_device_199, ttnn_multiply_53, )
  ttnn_multiply_54 = ttnn.multiply(ttnn_add_129, 0.7978845608028654, )
  ttnn_tanh_10_ = ttnn.tanh(ttnn_multiply_54, )
  ttnn_tanh_10_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_54), )
  ttnn_tanh_10 = ttnn.from_torch(ttnn_tanh_10_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_10_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_10_), ttnn_tanh_10))
  test_accuracy(tanh_10, ttnn_tanh_10)
  ttnn_add_130 = ttnn.add(ttnn_tanh_10, 1.0, )
  ttnn_multiply_55 = ttnn.multiply(ttnn_multiply_52, ttnn_add_130, )
  ttnn_from_device_421 = ttnn.from_device(ttnn_multiply_55, )
  ttnn_to_layout_424 = ttnn.to_layout(ttnn_from_device_421, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_244 = ttnn.reshape(ttnn_to_layout_424, (9, 3072), )
  ttnn_from_device_422 = ttnn.from_device(ttnn_reshape_244, )
  ttnn_to_layout_425 = ttnn.to_layout(ttnn_from_device_422, ttnn.TILE_LAYOUT, )
  ttnn_to_device_200 = ttnn.to_device(ttnn_to_layout_425, device = device)
  ttnn_matmul_88 = ttnn.matmul(ttnn_to_device_200, ttnn_transpose_8, )
  ttnn_add_66 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_88, )
  ttnn_from_device_423 = ttnn.from_device(ttnn_add_66, )
  ttnn_to_layout_426 = ttnn.to_layout(ttnn_from_device_423, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_245 = ttnn.reshape(ttnn_to_layout_426, (1, 9, 768), )
  ttnn_from_device_424 = ttnn.from_device(ttnn_reshape_245, )
  ttnn_to_layout_427 = ttnn.to_layout(ttnn_from_device_424, ttnn.TILE_LAYOUT, )
  ttnn_to_device_201 = ttnn.to_device(ttnn_to_layout_427, device = device)
  ttnn_add_131 = ttnn.add(ttnn_to_device_201, ttnn_layer_norm_21, )
  ttnn_layer_norm_22_ = ttnn.layer_norm(ttnn_add_131, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_22_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_131), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_22 = ttnn.from_torch(ttnn_layer_norm_22_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_22_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_22_), ttnn_layer_norm_22))
  ttnn_from_device_425 = ttnn.from_device(ttnn_layer_norm_22, )
  ttnn_to_layout_428 = ttnn.to_layout(ttnn_from_device_425, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_246 = ttnn.reshape(ttnn_to_layout_428, (9, 768), )
  ttnn_from_device_426 = ttnn.from_device(ttnn_reshape_246, )
  ttnn_to_layout_429 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_202 = ttnn.to_device(ttnn_to_layout_429, device = device)
  ttnn_matmul_89 = ttnn.matmul(ttnn_to_device_202, ttnn_transpose_1, )
  ttnn_add_67 = ttnn.add(ttnn_from_torch_12, ttnn_matmul_89, )
  ttnn_from_device_427 = ttnn.from_device(ttnn_add_67, )
  ttnn_to_layout_430 = ttnn.to_layout(ttnn_from_device_427, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_247 = ttnn.reshape(ttnn_to_layout_430, (1, 9, 768), )
  ttnn_to_layout_431 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_203 = ttnn.to_device(ttnn_to_layout_431, device = device)
  ttnn_matmul_90 = ttnn.matmul(ttnn_to_device_203, ttnn_transpose_2, )
  ttnn_add_68 = ttnn.add(ttnn_from_torch_14, ttnn_matmul_90, )
  ttnn_from_device_429 = ttnn.from_device(ttnn_add_68, )
  ttnn_to_layout_432 = ttnn.to_layout(ttnn_from_device_429, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_249 = ttnn.reshape(ttnn_to_layout_432, (1, 9, 768), )
  ttnn_to_layout_433 = ttnn.to_layout(ttnn_from_device_426, ttnn.TILE_LAYOUT, )
  ttnn_to_device_204 = ttnn.to_device(ttnn_to_layout_433, device = device)
  ttnn_matmul_91 = ttnn.matmul(ttnn_to_device_204, ttnn_transpose_3, )
  ttnn_add_69 = ttnn.add(ttnn_from_torch_16, ttnn_matmul_91, )
  ttnn_from_device_431 = ttnn.from_device(ttnn_add_69, )
  ttnn_to_layout_434 = ttnn.to_layout(ttnn_from_device_431, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_251 = ttnn.reshape(ttnn_to_layout_434, (1, 9, 768), )
  ttnn_from_device_432 = ttnn.from_device(ttnn_reshape_247, )
  ttnn_to_layout_435 = ttnn.to_layout(ttnn_from_device_432, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_252 = ttnn.reshape(ttnn_to_layout_435, (1, 9, 12, 64), )
  ttnn_from_device_433 = ttnn.from_device(ttnn_reshape_252, )
  ttnn_to_layout_436 = ttnn.to_layout(ttnn_from_device_433, ttnn.TILE_LAYOUT, )
  ttnn_to_device_205 = ttnn.to_device(ttnn_to_layout_436, device = device)
  ttnn_permute_33 = ttnn.permute(ttnn_to_device_205, (0, 2, 1, 3), )
  ttnn_from_device_434 = ttnn.from_device(ttnn_reshape_249, )
  ttnn_to_layout_437 = ttnn.to_layout(ttnn_from_device_434, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_253 = ttnn.reshape(ttnn_to_layout_437, (1, 9, 12, 64), )
  ttnn_from_device_435 = ttnn.from_device(ttnn_reshape_253, )
  ttnn_to_layout_438 = ttnn.to_layout(ttnn_from_device_435, ttnn.TILE_LAYOUT, )
  ttnn_to_device_206 = ttnn.to_device(ttnn_to_layout_438, device = device)
  ttnn_permute_34 = ttnn.permute(ttnn_to_device_206, (0, 2, 1, 3), )
  ttnn_from_device_436 = ttnn.from_device(ttnn_reshape_251, )
  ttnn_to_layout_439 = ttnn.to_layout(ttnn_from_device_436, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_254 = ttnn.reshape(ttnn_to_layout_439, (1, 9, 12, 64), )
  test_accuracy(view_241, ttnn_reshape_254)
  ttnn_from_device_437 = ttnn.from_device(ttnn_reshape_254, )
  ttnn_to_layout_440 = ttnn.to_layout(ttnn_from_device_437, ttnn.TILE_LAYOUT, )
  ttnn_to_device_207 = ttnn.to_device(ttnn_to_layout_440, device = device)
  ttnn_permute_35 = ttnn.permute(ttnn_to_device_207, (0, 2, 1, 3), )
  test_accuracy(permute_35, ttnn_permute_35)
  ttnn_transpose_92 = ttnn.transpose(ttnn_permute_34, 3, 2, )
  test_accuracy(transpose_22, ttnn_transpose_92)
  ttnn_from_device_438 = ttnn.from_device(ttnn_permute_33, )
  ttnn_to_layout_441 = ttnn.to_layout(ttnn_from_device_438, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_255 = ttnn.reshape(ttnn_to_layout_441, (12, 9, 64), )
  ttnn_from_device_439 = ttnn.from_device(ttnn_transpose_92, )
  ttnn_to_layout_442 = ttnn.to_layout(ttnn_from_device_439, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_256 = ttnn.reshape(ttnn_to_layout_442, (12, 64, 9), )
  test_accuracy(view_243, ttnn_reshape_256)
  ttnn_from_device_440 = ttnn.from_device(ttnn_reshape_255, )
  ttnn_to_layout_443 = ttnn.to_layout(ttnn_from_device_440, ttnn.TILE_LAYOUT, )
  ttnn_to_device_208 = ttnn.to_device(ttnn_to_layout_443, device = device)
  ttnn_from_device_441 = ttnn.from_device(ttnn_reshape_256, )
  ttnn_to_layout_444 = ttnn.to_layout(ttnn_from_device_441, ttnn.TILE_LAYOUT, )
  ttnn_to_device_209 = ttnn.to_device(ttnn_to_layout_444, device = device)
  ttnn_matmul_92 = ttnn.matmul(ttnn_to_device_208, ttnn_to_device_209, )
  test_accuracy(bmm_22, ttnn_matmul_92)
  ttnn_from_device_442 = ttnn.from_device(ttnn_matmul_92, )
  ttnn_to_layout_445 = ttnn.to_layout(ttnn_from_device_442, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_257 = ttnn.reshape(ttnn_to_layout_445, (1, 12, 9, 9), )
  test_accuracy(view_244, ttnn_reshape_257)
  ttnn_from_device_443 = ttnn.from_device(ttnn_reshape_257, )
  ttnn_to_layout_446 = ttnn.to_layout(ttnn_from_device_443, ttnn.TILE_LAYOUT, )
  ttnn_to_device_210 = ttnn.to_device(ttnn_to_layout_446, device = device)
  ttnn_multiply_56 = ttnn.multiply(ttnn_to_device_210, 0.125, )
  test_accuracy(div_11, ttnn_multiply_56)
  ttnn_add_132 = ttnn.add(ttnn_multiply_56, ttnn_multiply, )
  test_accuracy(add_57, ttnn_add_132)
  ttnn_softmax_11 = ttnn.softmax(ttnn_add_132, -1, numeric_stable = True)
  test_accuracy(_softmax_11, ttnn_softmax_11)
  ttnn_prefix_clone_34 = clone_wrapper(ttnn_softmax_11, )
  test_accuracy(clone_37, ttnn_prefix_clone_34)
  ttnn_from_device_444 = ttnn.from_device(ttnn_prefix_clone_34, )
  ttnn_to_layout_447 = ttnn.to_layout(ttnn_from_device_444, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_258 = ttnn.reshape(ttnn_to_layout_447, (12, 9, 9), )
  test_accuracy(view_245, ttnn_reshape_258)
  ttnn_from_device_445 = ttnn.from_device(ttnn_permute_35, )
  ttnn_to_layout_448 = ttnn.to_layout(ttnn_from_device_445, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_259 = ttnn.reshape(ttnn_to_layout_448, (12, 9, 64), )
  test_accuracy(view_246, ttnn_reshape_259)
  ttnn_from_device_446 = ttnn.from_device(ttnn_reshape_258, )
  ttnn_to_layout_449 = ttnn.to_layout(ttnn_from_device_446, ttnn.TILE_LAYOUT, )
  ttnn_to_device_211 = ttnn.to_device(ttnn_to_layout_449, device = device)
  ttnn_from_device_447 = ttnn.from_device(ttnn_reshape_259, )
  ttnn_to_layout_450 = ttnn.to_layout(ttnn_from_device_447, ttnn.TILE_LAYOUT, )
  ttnn_to_device_212 = ttnn.to_device(ttnn_to_layout_450, device = device)
  ttnn_matmul_93 = ttnn.matmul(ttnn_to_device_211, ttnn_to_device_212, )
  test_accuracy(bmm_23, ttnn_matmul_93)
  ttnn_from_device_448 = ttnn.from_device(ttnn_matmul_93, )
  ttnn_to_layout_451 = ttnn.to_layout(ttnn_from_device_448, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_260 = ttnn.reshape(ttnn_to_layout_451, (1, 12, 9, 64), )
  test_accuracy(view_247, ttnn_reshape_260)
  ttnn_from_device_449 = ttnn.from_device(ttnn_reshape_260, )
  ttnn_to_layout_452 = ttnn.to_layout(ttnn_from_device_449, ttnn.TILE_LAYOUT, )
  ttnn_to_device_213 = ttnn.to_device(ttnn_to_layout_452, device = device)
  ttnn_transpose_93 = ttnn.transpose(ttnn_to_device_213, 2, 1, )
  test_accuracy(transpose_23, ttnn_transpose_93)
  ttnn_prefix_clone_35 = clone_wrapper(ttnn_transpose_93, )
  test_accuracy(clone_38, ttnn_prefix_clone_35)
  ttnn_from_device_450 = ttnn.from_device(ttnn_prefix_clone_35, )
  ttnn_to_layout_453 = ttnn.to_layout(ttnn_from_device_450, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_261 = ttnn.reshape(ttnn_to_layout_453, (1, 9, 768), )
  test_accuracy(_unsafe_view_11, ttnn_reshape_261)
  ttnn_from_device_451 = ttnn.from_device(ttnn_reshape_261, )
  ttnn_to_layout_454 = ttnn.to_layout(ttnn_from_device_451, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_262 = ttnn.reshape(ttnn_to_layout_454, (9, 768), )
  ttnn_from_device_452 = ttnn.from_device(ttnn_reshape_262, )
  ttnn_to_layout_455 = ttnn.to_layout(ttnn_from_device_452, ttnn.TILE_LAYOUT, )
  ttnn_to_device_214 = ttnn.to_device(ttnn_to_layout_455, device = device)
  ttnn_matmul_94 = ttnn.matmul(ttnn_to_device_214, ttnn_transpose_6, )
  ttnn_add_70 = ttnn.add(ttnn_from_torch_18, ttnn_matmul_94, )
  ttnn_from_device_453 = ttnn.from_device(ttnn_add_70, )
  ttnn_to_layout_456 = ttnn.to_layout(ttnn_from_device_453, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_263 = ttnn.reshape(ttnn_to_layout_456, (1, 9, 768), )
  ttnn_from_device_454 = ttnn.from_device(ttnn_reshape_263, )
  ttnn_to_layout_457 = ttnn.to_layout(ttnn_from_device_454, ttnn.TILE_LAYOUT, )
  ttnn_to_device_215 = ttnn.to_device(ttnn_to_layout_457, device = device)
  ttnn_prefix_clone_36 = clone_wrapper(ttnn_to_device_215, )
  test_accuracy(clone_39, ttnn_prefix_clone_36)
  ttnn_add_133 = ttnn.add(ttnn_layer_norm_22, ttnn_prefix_clone_36, )
  ttnn_layer_norm_23_ = ttnn.layer_norm(ttnn_add_133, epsilon = 1e-12, weight = ttnn_from_torch_19, bias = ttnn_from_torch_20)
  ttnn_layer_norm_23_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_133), [768], ttnn.to_torch(ttnn_from_torch_19), 
                                                                ttnn.to_torch(ttnn_from_torch_20), 1e-12, )[0]
  ttnn_layer_norm_23 = ttnn.from_torch(ttnn_layer_norm_23_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_23_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_23_), ttnn_layer_norm_23))
  ttnn_from_device_455 = ttnn.from_device(ttnn_layer_norm_23, )
  ttnn_to_layout_458 = ttnn.to_layout(ttnn_from_device_455, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_264 = ttnn.reshape(ttnn_to_layout_458, (9, 768), )
  ttnn_from_device_456 = ttnn.from_device(ttnn_reshape_264, )
  ttnn_to_layout_459 = ttnn.to_layout(ttnn_from_device_456, ttnn.TILE_LAYOUT, )
  ttnn_to_device_216 = ttnn.to_device(ttnn_to_layout_459, device = device)
  ttnn_matmul_95 = ttnn.matmul(ttnn_to_device_216, ttnn_transpose_7, )
  ttnn_add_71 = ttnn.add(ttnn_from_torch_22, ttnn_matmul_95, )
  test_accuracy(addmm_71, ttnn_add_71)
  ttnn_from_device_457 = ttnn.from_device(ttnn_add_71, )
  ttnn_to_layout_460 = ttnn.to_layout(ttnn_from_device_457, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_265 = ttnn.reshape(ttnn_to_layout_460, (1, 9, 3072), )
  test_accuracy(view_251, ttnn_reshape_265)
  ttnn_from_device_458 = ttnn.from_device(ttnn_reshape_265, )
  ttnn_to_layout_461 = ttnn.to_layout(ttnn_from_device_458, ttnn.TILE_LAYOUT, )
  ttnn_to_device_217 = ttnn.to_device(ttnn_to_layout_461, device = device)
  ttnn_multiply_57 = ttnn.multiply(ttnn_to_device_217, 0.5, )
  ttnn_pow_11 = ttnn.pow(ttnn_to_device_217, 3.0, )
  test_accuracy(pow_12, ttnn_pow_11)
  ttnn_multiply_58 = ttnn.multiply(ttnn_pow_11, 0.044715, )
  ttnn_add_134 = ttnn.add(ttnn_to_device_217, ttnn_multiply_58, )
  ttnn_multiply_59 = ttnn.multiply(ttnn_add_134, 0.7978845608028654, )
  ttnn_tanh_11_ = ttnn.tanh(ttnn_multiply_59, )
  ttnn_tanh_11_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_59), )
  ttnn_tanh_11 = ttnn.from_torch(ttnn_tanh_11_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_11_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_11_), ttnn_tanh_11))
  test_accuracy(tanh_11, ttnn_tanh_11)
  test_accuracy(tanh_11, ttnn_tanh_11)
  ttnn_add_135 = ttnn.add(ttnn_tanh_11, 1.0, )
  test_accuracy(add_60, ttnn_add_135)
  ttnn_multiply_60 = ttnn.multiply(ttnn_multiply_57, ttnn_add_135, )
  test_accuracy(mul_48, ttnn_multiply_60)
  ttnn_from_device_459 = ttnn.from_device(ttnn_multiply_60, )
  ttnn_to_layout_462 = ttnn.to_layout(ttnn_from_device_459, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_266 = ttnn.reshape(ttnn_to_layout_462, (9, 3072), )
  test_accuracy(view_252, ttnn_reshape_266)
  ttnn_from_device_460 = ttnn.from_device(ttnn_reshape_266, )
  ttnn_to_layout_463 = ttnn.to_layout(ttnn_from_device_460, ttnn.TILE_LAYOUT, )
  ttnn_to_device_218 = ttnn.to_device(ttnn_to_layout_463, device = device)
  ttnn_matmul_96 = ttnn.matmul(ttnn_to_device_218, ttnn_transpose_8, )
  ttnn_add_72 = ttnn.add(ttnn_from_torch_24, ttnn_matmul_96, )
  test_accuracy(addmm_72, ttnn_add_72)
  ttnn_from_device_461 = ttnn.from_device(ttnn_add_72, )
  ttnn_to_layout_464 = ttnn.to_layout(ttnn_from_device_461, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_267 = ttnn.reshape(ttnn_to_layout_464, (1, 9, 768), )
  test_accuracy(view_253, ttnn_reshape_267)
  ttnn_from_device_462 = ttnn.from_device(ttnn_reshape_267, )
  ttnn_to_layout_465 = ttnn.to_layout(ttnn_from_device_462, ttnn.TILE_LAYOUT, )
  ttnn_to_device_219 = ttnn.to_device(ttnn_to_layout_465, device = device)
  ttnn_add_136 = ttnn.add(ttnn_to_device_219, ttnn_layer_norm_23, )
  test_accuracy(add_61, ttnn_add_136)
  ttnn_layer_norm_24_ = ttnn.layer_norm(ttnn_add_136, epsilon = 1e-12, weight = ttnn_from_torch_25, bias = ttnn_from_torch_26)
  ttnn_layer_norm_24_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_add_136), [768], ttnn.to_torch(ttnn_from_torch_25), 
                                                                ttnn.to_torch(ttnn_from_torch_26), 1e-12, )[0]
  ttnn_layer_norm_24 = ttnn.from_torch(ttnn_layer_norm_24_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_24_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_24_), ttnn_layer_norm_24))
  ttnn_from_device_463 = ttnn.from_device(ttnn_layer_norm_24, )
  ttnn_to_layout_466 = ttnn.to_layout(ttnn_from_device_463, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_268 = ttnn.reshape(ttnn_to_layout_466, (9, 768), )
  test_accuracy(view_254, ttnn_reshape_268)
  ttnn_from_torch_27 = ttnn.from_torch(arg23_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_97 = ttnn.transpose(ttnn_from_torch_27, 0, 1, )
  test_accuracy(t_73, ttnn_transpose_97)
  ttnn_from_device_464 = ttnn.from_device(ttnn_reshape_268, )
  ttnn_to_layout_467 = ttnn.to_layout(ttnn_from_device_464, ttnn.TILE_LAYOUT, )
  ttnn_to_device_220 = ttnn.to_device(ttnn_to_layout_467, device = device)
  ttnn_matmul_97 = ttnn.matmul(ttnn_to_device_220, ttnn_transpose_97, )
  ttnn_from_torch_28 = ttnn.from_torch(arg24_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_73 = ttnn.add(ttnn_from_torch_28, ttnn_matmul_97, )
  test_accuracy(addmm_73, ttnn_add_73)
  ttnn_from_device_465 = ttnn.from_device(ttnn_add_73, )
  ttnn_to_layout_468 = ttnn.to_layout(ttnn_from_device_465, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_269 = ttnn.reshape(ttnn_to_layout_468, (1, 9, 128), )
  test_accuracy(view_255, ttnn_reshape_269)
  ttnn_from_device_466 = ttnn.from_device(ttnn_reshape_269, )
  ttnn_to_layout_469 = ttnn.to_layout(ttnn_from_device_466, ttnn.TILE_LAYOUT, )
  ttnn_to_device_221 = ttnn.to_device(ttnn_to_layout_469, device = device)
  ttnn_multiply_61 = ttnn.multiply(ttnn_to_device_221, 0.5, )
  ttnn_pow_12 = ttnn.pow(ttnn_to_device_221, 3.0, )
  test_accuracy(pow_13, ttnn_pow_12)
  ttnn_multiply_62 = ttnn.multiply(ttnn_pow_12, 0.044715, )
  ttnn_add_137 = ttnn.add(ttnn_to_device_221, ttnn_multiply_62, )
  ttnn_multiply_63 = ttnn.multiply(ttnn_add_137, 0.7978845608028654, )
  ttnn_tanh_12_ = ttnn.tanh(ttnn_multiply_63, )
  ttnn_tanh_12_intermediate = aten.tanh.default(ttnn.to_torch(ttnn_multiply_63), )
  ttnn_tanh_12 = ttnn.from_torch(ttnn_tanh_12_intermediate, device=device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_tanh_12_: ", test_accuracy(ttnn.to_torch(ttnn_tanh_12_), ttnn_tanh_12))
  test_accuracy(tanh_12, ttnn_tanh_12)
  test_accuracy(tanh_12, ttnn_tanh_12)
  ttnn_add_138 = ttnn.add(ttnn_tanh_12, 1.0, )
  test_accuracy(add_63, ttnn_add_138)
  ttnn_multiply_64 = ttnn.multiply(ttnn_multiply_61, ttnn_add_138, )
  test_accuracy(mul_52, ttnn_multiply_64)
  ttnn_from_torch_29 = ttnn.from_torch(arg25_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_from_torch_30 = ttnn.from_torch(arg26_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_layer_norm_25_ = ttnn.layer_norm(ttnn_multiply_64, epsilon = 1e-12, weight = ttnn_from_torch_29, bias = ttnn_from_torch_30)
  ttnn_layer_norm_25_intermediate = aten.native_layer_norm.default(ttnn.to_torch(ttnn_multiply_64), [128], ttnn.to_torch(ttnn_from_torch_29), 
                                                                ttnn.to_torch(ttnn_from_torch_30), 1e-12, )[0]
  ttnn_layer_norm_25 = ttnn.from_torch(ttnn_layer_norm_25_intermediate, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  print("ttnn_layer_norm_25_: ", test_accuracy(ttnn.to_torch(ttnn_layer_norm_25_), ttnn_layer_norm_25))
  ttnn_from_device_467 = ttnn.from_device(ttnn_layer_norm_25, )
  ttnn_to_layout_470 = ttnn.to_layout(ttnn_from_device_467, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_270 = ttnn.reshape(ttnn_to_layout_470, (9, 128), )
  test_accuracy(view_256, ttnn_reshape_270)
  ttnn_from_torch_31 = ttnn.from_torch(arg27_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_transpose_98 = ttnn.transpose(ttnn_from_torch_31, 0, 1, )
  test_accuracy(t_74, ttnn_transpose_98)
  ttnn_from_device_468 = ttnn.from_device(ttnn_reshape_270, )
  ttnn_to_layout_471 = ttnn.to_layout(ttnn_from_device_468, ttnn.TILE_LAYOUT, )
  ttnn_to_device_222 = ttnn.to_device(ttnn_to_layout_471, device = device)
  ttnn_matmul_98 = ttnn.matmul(ttnn_to_device_222, ttnn_transpose_98, )
  ttnn_from_torch_32 = ttnn.from_torch(arg28_1, device = device, layout = ttnn.TILE_LAYOUT, dtype = ttnn.bfloat16)
  ttnn_add_74 = ttnn.add(ttnn_from_torch_32, ttnn_matmul_98, )
  test_accuracy(addmm_74, ttnn_add_74)
  ttnn_from_device_469 = ttnn.from_device(ttnn_add_74, )
  ttnn_to_layout_472 = ttnn.to_layout(ttnn_from_device_469, ttnn.ROW_MAJOR_LAYOUT, )
  ttnn_reshape_271 = ttnn.reshape(ttnn_to_layout_472, (1, 9, 30000), )
  print(test_accuracy(view_257, ttnn_reshape_271))
  ttnn_to_torch = ttnn.to_torch(ttnn_reshape_271, dtype = torch.bfloat16)
  ttnn.close_device(device)

if __name__ == "__main__":
    filepath = Path(__file__).with_name("albert-base-v2_inputs.pickle")
    file = lzma.open(filepath, "rb")
    inputs = pickle.load(file)
    forward(*inputs)

